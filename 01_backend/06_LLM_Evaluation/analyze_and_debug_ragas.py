#!/usr/bin/env python3
"""
RAGAS í‰ê°€ ê²°ê³¼ ë¶„ì„ ë° Data Leakage ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- ì§ˆë¬¸ ìœ í˜•ë³„(í‚¤ì›Œë“œ/ë¬¸ë§¥/ì¶”ìƒ) RAGAS ì ìˆ˜ ë¶„ì„
- Answer Relevancy ë©”íŠ¸ë¦­ ë””ë²„ê¹…
"""

import pandas as pd
import os
import time
import traceback
from pathlib import Path
from dotenv import load_dotenv
from ragas import evaluate
from ragas.metrics import answer_relevancy
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from datasets import Dataset

# HuggingFaceEmbeddings ë˜í¼ (embed_query ë©”ì„œë“œ ì¶”ê°€)
class HuggingFaceEmbeddingsWrapper:
    """HuggingFaceEmbeddingsë¥¼ RAGASê°€ ê¸°ëŒ€í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘"""
    def __init__(self, model_name):
        self.base_embeddings = HuggingFaceEmbeddings(model=model_name)
    
    def embed_query(self, text: str):
        """embed_textë¥¼ embed_queryë¡œ ë§¤í•‘"""
        return self.base_embeddings.embed_text(text)
    
    def embed_documents(self, texts: list):
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        return [self.base_embeddings.embed_text(text) for text in texts]

# ============================================================================
# 1. ì§ˆë¬¸ ìœ í˜•ë³„ RAGAS ì ìˆ˜ ë¶„ì„ (Data Leakage ê²€ì¦)
# ============================================================================
print("=" * 70)
print("ğŸ“Š 1. ì§ˆë¬¸ ìœ í˜•ë³„ RAGAS ì ìˆ˜ ë¶„ì„ (Data Leakage ê²€ì¦)")
print("=" * 70)

base_dir = Path("/home/pencilfoxs/00_new/History_Docent/06_LLM_Evaluation/results")
ragas_file = base_dir / "ragas_evaluation_results.csv"
meta_file = base_dir / "llm_selected_model_full_test.csv"

if not ragas_file.exists():
    print(f"âŒ Error: {ragas_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

if not meta_file.exists():
    print(f"âŒ Error: {meta_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

# ë°ì´í„° ë¡œë“œ
print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
ragas_df = pd.read_csv(ragas_file)
meta_df = pd.read_csv(meta_file)

print(f"  - RAGAS ê²°ê³¼: {len(ragas_df)}í–‰")
print(f"  - ë©”íƒ€ë°ì´í„°: {len(meta_df)}í–‰")

# ë°ì´í„° ë³‘í•©
if 'query_id' in ragas_df.columns and 'query_id' in meta_df.columns:
    merged_df = pd.merge(
        ragas_df[['query_id', 'context_recall', 'context_precision', 'faithfulness', 'answer_relevancy']],
        meta_df[['query_id', 'type', 'chunk_id']],
        on='query_id',
        how='left'
    )
    
    print(f"  - ë³‘í•© í›„: {len(merged_df)}í–‰")
    print(f"\n  ğŸ“Š ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬:")
    type_counts = merged_df['type'].value_counts()
    print(type_counts.to_string())
    
    # ìœ í˜•ë³„ í†µê³„ ê³„ì‚°
    metrics = ['context_recall', 'context_precision', 'faithfulness']
    
    print("\n" + "-" * 70)
    print("ğŸ“ˆ ìœ í˜•ë³„ í‰ê·  ì ìˆ˜ ë¹„êµ (Data Leakage ê²€ì¦)")
    print("-" * 70)
    
    results_summary = []
    
    for metric in metrics:
        if metric in merged_df.columns:
            print(f"\n[{metric.upper()}]")
            print("-" * 70)
            
            # ì „ì²´ í†µê³„
            valid_all = merged_df[merged_df[metric].notna()]
            if len(valid_all) > 0:
                overall_mean = valid_all[metric].mean()
                overall_std = valid_all[metric].std()
                overall_count = len(valid_all)
                print(f"ì „ì²´ í‰ê· : {overall_mean:.4f} (std: {overall_std:.4f}, n={overall_count})")
            
            # ìœ í˜•ë³„ í†µê³„
            valid_df = merged_df[merged_df[metric].notna()]
            if len(valid_df) > 0:
                type_stats = valid_df.groupby('type')[metric].agg(['count', 'mean', 'std', 'min', 'max'])
                print(type_stats.to_string())
                
                # ìœ í˜•ë³„ í‰ê·  ì ìˆ˜ ì¶”ì¶œ
                for q_type in ['keyword', 'context', 'abstract']:
                    if q_type in type_stats.index:
                        mean_score = type_stats.loc[q_type, 'mean']
                        count = int(type_stats.loc[q_type, 'count'])
                        results_summary.append({
                            'metric': metric,
                            'type': q_type,
                            'mean': mean_score,
                            'count': count
                        })
    
    # ìœ í˜•ë³„ ì ìˆ˜ ë¹„êµ ìš”ì•½ (í•µì‹¬ ê²€ì¦)
    print("\n" + "=" * 70)
    print("ğŸ” í•µì‹¬ ê²€ì¦: Abstract vs Keyword ì ìˆ˜ ì°¨ì´ (Data Leakage ê²€ì¦)")
    print("=" * 70)
    
    summary_df = pd.DataFrame(results_summary)
    if len(summary_df) > 0:
        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        pivot_df = summary_df.pivot(index='metric', columns='type', values='mean')
        print("\nìœ í˜•ë³„ í‰ê·  ì ìˆ˜:")
        print(pivot_df.to_string())
        
        # Abstract vs Keyword ë¹„êµ (í•µì‹¬ ê²€ì¦)
        print("\n" + "-" * 70)
        print("ğŸ’¡ Data Leakage ê²€ì¦ ê²°ê³¼:")
        print("-" * 70)
        
        for metric in metrics:
            if metric in pivot_df.index:
                keyword_mean = pivot_df.loc[metric, 'keyword'] if 'keyword' in pivot_df.columns else None
                abstract_mean = pivot_df.loc[metric, 'abstract'] if 'abstract' in pivot_df.columns else None
                
                if keyword_mean is not None and abstract_mean is not None:
                    diff = keyword_mean - abstract_mean
                    diff_pct = (diff / keyword_mean) * 100 if keyword_mean > 0 else 0
                    
                    print(f"\n[{metric}]")
                    print(f"  Keyword í‰ê· :  {keyword_mean:.4f}")
                    print(f"  Abstract í‰ê· : {abstract_mean:.4f}")
                    print(f"  ì°¨ì´: {diff:.4f} ({diff_pct:+.1f}%)")
                    
                    if diff > 0.15:  # 15% ì´ìƒ ì°¨ì´
                        print(f"  âš ï¸  ê²½ê³ : Abstractê°€ Keywordë³´ë‹¤ í˜„ì €íˆ ë‚®ìŠµë‹ˆë‹¤!")
                        print(f"      â†’ Data Leakage ê°€ëŠ¥ì„± ë†’ìŒ (ê²€ìƒ‰ DBì— ì •ë‹µì´ ì§ì ‘ í¬í•¨ë¨)")
                        print(f"      â†’ ìƒˆë¡œìš´ ì§ˆë¬¸(Unseen Query)ì— ì•½í•  ê°€ëŠ¥ì„±")
                    elif diff > 0.05:  # 5-15% ì°¨ì´
                        print(f"  âš ï¸  ì£¼ì˜: Abstractê°€ Keywordë³´ë‹¤ ì•½ê°„ ë‚®ìŠµë‹ˆë‹¤.")
                        print(f"      â†’ ì¼ë¶€ Data Leakage ê°€ëŠ¥ì„±, Unseen Query í…ŒìŠ¤íŠ¸ ê¶Œì¥")
                    else:  # ì°¨ì´ < 5%
                        print(f"  âœ… ì–‘í˜¸: Abstractì™€ Keyword ì ìˆ˜ê°€ ë¹„ìŠ·í•©ë‹ˆë‹¤.")
                        print(f"      â†’ ì‹œìŠ¤í…œì´ ìƒˆë¡œìš´ ì§ˆë¬¸ì—ë„ ì˜ ì‘ë™í•  ê°€ëŠ¥ì„±")
    
    # Data Leakage í•´ì„
    print("\n" + "=" * 70)
    print("ğŸ“‹ Data Leakage ê²€ì¦ í•´ì„ ê°€ì´ë“œ")
    print("=" * 70)
    print("""
    ğŸ’¡ í•´ì„ ë°©ë²•:
    
    1. Keyword (30%): ê³ ìœ ëª…ì‚¬/ì‚¬ì‹¤ ê¸°ë°˜ ì§ˆë¬¸
       - ê²€ìƒ‰ DBì— ì •í™•íˆ ë§¤ì¹­ë  ê°€ëŠ¥ì„± ë†’ìŒ
       - ì˜ˆ: "ì†ê¸°ì •ì˜ ì œìëŠ” ëˆ„êµ¬ì¸ê°€?"
    
    2. Abstract (30%): í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìƒëµí•œ ì¶”ìƒì  ì§ˆë¬¸
       - ê²€ìƒ‰ ë‚œì´ë„ ë†’ìŒ (Unseen Queryì™€ ìœ ì‚¬)
       - ì˜ˆ: "ê·¸ ì˜¬ë¦¼í”½ì—ì„œ 1ë“± í•œ ìœ ëª…í•œ ì‚¬ëŒì´ ì œì¼ ì•„ë¼ë˜ ì œìëŠ”..."
    
    3. Context (40%): ë¬¸ë§¥/ì¸ê³¼ê´€ê³„ ì§ˆë¬¸ (ì¤‘ê°„ ë‚œì´ë„)
    
    ğŸ” ê²€ì¦ ê¸°ì¤€:
    - Abstract ì ìˆ˜ê°€ Keywordë³´ë‹¤ í˜„ì €íˆ ë‚®ë‹¤ë©´ â†’ Data Leakage ì¡´ì¬
    - Abstract ì ìˆ˜ê°€ Keywordì™€ ë¹„ìŠ·í•˜ë‹¤ë©´ â†’ ì‹œìŠ¤í…œì´ ì˜ í•™ìŠµë¨
    
    ğŸ“Œ ë©´ì ‘ê´€ ë‹µë³€ ì „ëµ:
    "ë„¤, ë§ìŠµë‹ˆë‹¤. Synthetic Datasetì˜ í•œê³„ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ Abstract ìœ í˜•ì€ 
    í‚¤ì›Œë“œë¥¼ ì œê±°í•˜ì—¬ 'Unseen Query'ì™€ ìœ ì‚¬í•œ í™˜ê²½ì„ ì‹œë®¬ë ˆì´ì…˜í–ˆìŠµë‹ˆë‹¤. 
    ì‹¤ì œ Unseen Query í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì™¸ë¶€ ì§ˆë¬¸ì…‹(ìˆ˜ëŠ¥ ê¸°ì¶œ ë“±)ì„ ì¶”ê°€í•  ê³„íšì…ë‹ˆë‹¤."
    """)
    
    # chunk_id ê¸°ë°˜ ë¶„ì„ (ê°™ì€ ì²­í¬ì—ì„œ ìƒì„±ëœ ì§ˆë¬¸ë“¤ì˜ ì ìˆ˜)
    print("\n" + "-" * 70)
    print("ğŸ” ì²­í¬ë³„ ì§ˆë¬¸ ìƒì„± ë¶„ì„")
    print("-" * 70)
    
    chunk_question_count = merged_df.groupby('chunk_id').size()
    print(f"  - ì´ ì²­í¬ ìˆ˜: {chunk_question_count.shape[0]}")
    print(f"  - ì²­í¬ë‹¹ í‰ê·  ì§ˆë¬¸ ìˆ˜: {chunk_question_count.mean():.2f}")
    print(f"  - ìµœëŒ€ ì§ˆë¬¸ ìˆ˜: {chunk_question_count.max()}")
    print(f"  - ìµœì†Œ ì§ˆë¬¸ ìˆ˜: {chunk_question_count.min()}")
    
    # ê°™ì€ ì²­í¬ì—ì„œ ìƒì„±ëœ ì§ˆë¬¸ë“¤ì˜ ì ìˆ˜ ë¶„í¬
    multi_question_chunks = chunk_question_count[chunk_question_count > 1].index
    if len(multi_question_chunks) > 0:
        print(f"\n  - ì—¬ëŸ¬ ì§ˆë¬¸ì´ ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(multi_question_chunks)}")
        for metric in metrics:
            if metric in merged_df.columns:
                multi_chunk_scores = merged_df[merged_df['chunk_id'].isin(multi_question_chunks) & merged_df[metric].notna()][metric]
                single_chunk_scores = merged_df[~merged_df['chunk_id'].isin(multi_question_chunks) & merged_df[metric].notna()][metric]
                if len(multi_chunk_scores) > 0 and len(single_chunk_scores) > 0:
                    print(f"\n  [{metric}]")
                    print(f"    - ì—¬ëŸ¬ ì§ˆë¬¸ ì²­í¬ í‰ê· : {multi_chunk_scores.mean():.4f} (n={len(multi_chunk_scores)})")
                    print(f"    - ë‹¨ì¼ ì§ˆë¬¸ ì²­í¬ í‰ê· : {single_chunk_scores.mean():.4f} (n={len(single_chunk_scores)})")
    
else:
    print("âŒ Error: query_id ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

# ============================================================================
# 2. Answer Relevancy ë””ë²„ê¹… (Minimal Test)
# ============================================================================
print("\n" + "=" * 70)
print("ğŸ› 2. Answer Relevancy ë””ë²„ê¹… (Minimal Test)")
print("=" * 70)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
env_path = Path("/home/pencilfoxs/00_new/.env2")
load_dotenv(env_path)
google_api_key = os.getenv("GOOGLE_AI_STUDIO_API_KEY")
if not google_api_key:
    google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    print("âŒ Error: GOOGLE_AI_STUDIO_API_KEY ë˜ëŠ” GOOGLE_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

os.environ["GOOGLE_API_KEY"] = google_api_key

try:
    # ëª¨ë¸ ì„¤ì • (evaluate_ragas_full.pyì™€ ë™ì¼)
    print("\nğŸ”§ ëª¨ë¸ ì„¤ì • ì¤‘ (evaluate_ragas_full.pyì™€ ë™ì¼í•œ ì„¤ì •)...")
    print("  - LLM: Gemini-2.0-flash (temperature=0, top_p=1)")
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0, top_p=1)
    ragas_llm = LangchainLLMWrapper(llm)
    
    print("  - Embeddings: BAAI/bge-m3 (ë˜í¼ ì ìš©)")
    # RAGAS í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ ì‚¬ìš©
    base_embeddings = HuggingFaceEmbeddings(model="BAAI/bge-m3")
    ragas_embeddings = HuggingFaceEmbeddingsWrapper("BAAI/bge-m3")
    
    # AnswerRelevancy ì„¤ì • (ëª…ì‹œì ìœ¼ë¡œ embeddings ì„¤ì •)
    print("  - AnswerRelevancy ë©”íŠ¸ë¦­ ì„¤ì •...")
    answer_relevancy.embeddings = ragas_embeddings
    answer_relevancy.llm = ragas_llm
    
    print("  âœ… ì„¤ì • ì™„ë£Œ")
    
    # ìƒ˜í”Œ ë°ì´í„° (í•œêµ­ì–´)
    print("\nğŸ“ ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„...")
    sample_data = {
        'question': ["ì´ìˆœì‹  ì¥êµ°ì€ ì–´ëŠ ì‹œëŒ€ ì‚¬ëŒì¸ê°€ìš”?"],
        'answer': ["ì´ìˆœì‹  ì¥êµ°ì€ ì¡°ì„  ì‹œëŒ€ì˜ ì¥êµ°ì…ë‹ˆë‹¤."],
        'contexts': [["ì´ìˆœì‹ ì€ ì¡°ì„  ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤."]],
        'ground_truth': ["ì´ìˆœì‹  ì¥êµ°ì€ ì¡°ì„  ì‹œëŒ€ ì‚¬ëŒì…ë‹ˆë‹¤."]
    }
    dataset = Dataset.from_dict(sample_data)
    
    print("  - ì§ˆë¬¸: ì´ìˆœì‹  ì¥êµ°ì€ ì–´ëŠ ì‹œëŒ€ ì‚¬ëŒì¸ê°€ìš”?")
    print("  - ë‹µë³€: ì´ìˆœì‹  ì¥êµ°ì€ ì¡°ì„  ì‹œëŒ€ì˜ ì¥êµ°ì…ë‹ˆë‹¤.")
    
    print("\nâ³ í‰ê°€ ì‹¤í–‰ ì¤‘... (ì•½ 10-30ì´ˆ ì†Œìš”)")
    print("  ğŸ’¡ Answer Relevancy ì‘ë™ ì›ë¦¬:")
    print("     1. LLMì´ ë‹µë³€(Answer)ì„ ë³´ê³  ê°€ìƒ ì§ˆë¬¸(Generated Question)ì„ ìƒì„±")
    print("     2. ì›ë˜ ì§ˆë¬¸(User Question)ê³¼ ê°€ìƒ ì§ˆë¬¸ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°")
    print("     3. ì´ë•Œ ì„ë² ë”© ëª¨ë¸(BAAI/bge-m3)ì´ í•„ìš”")
    
    start_time = time.time()
    
    result = evaluate(
        dataset=dataset,
        metrics=[answer_relevancy],
        llm=ragas_llm,
        embeddings=ragas_embeddings,
        raise_exceptions=True
    )
    
    elapsed = time.time() - start_time
    print(f"\nâœ… ì„±ê³µ! (ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ)")
    
    result_df = result.to_pandas()
    print("\nğŸ“Š ê²°ê³¼:")
    print(result_df.to_string())
    
    relevancy_score = result_df['answer_relevancy'].iloc[0] if 'answer_relevancy' in result_df.columns else None
    if relevancy_score is not None and not pd.isna(relevancy_score):
        print(f"\nâœ… Answer Relevancy ì ìˆ˜: {relevancy_score:.4f}")
        print("\nğŸ’¡ í•´ê²°ì±…:")
        print("  - ì„ë² ë”© ëª¨ë¸ ì„¤ì •ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
        print("  - ì „ì²´ í‰ê°€ì—ì„œ Answer Relevancyê°€ NaNì¸ ì´ìœ ëŠ”:")
        print("    1. evaluate_ragas_full.pyì—ì„œ metric.embeddingsë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ì§€ ì•ŠìŒ")
        print("    2. ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì¤‘ íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì˜¤ë¥˜")
        print("    3. í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¬¸ì œ")
        print("\n  ğŸ“Œ ê¶Œì¥ ìˆ˜ì •ì‚¬í•­:")
        print("    evaluate_ragas_full.pyì˜ line 81-83ì— ë‹¤ìŒ ì¶”ê°€:")
        print("    for metric in metrics:")
        print("        metric.llm = ragas_llm")
        print("        metric.embeddings = ragas_embeddings  # â† ì¶”ê°€ í•„ìš”")
    else:
        print("\nâŒ Answer Relevancy ì ìˆ˜ê°€ ì—¬ì „íˆ NaNì…ë‹ˆë‹¤.")
        print("  - ì¶”ê°€ ë””ë²„ê¹…ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("  - ì„ë² ë”© ëª¨ë¸ ë¡œë”© í™•ì¸ í•„ìš”")
        
except Exception as e:
    print("\nâŒ ì˜¤ë¥˜ ë°œìƒ:")
    print(str(e))
    traceback.print_exc()
    print("\nğŸ’¡ ë””ë²„ê¹… ì •ë³´:")
    print("  - API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸")
    print("  - ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸")
    print("  - RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸")

print("\n" + "=" * 70)
print("âœ… ë¶„ì„ ì™„ë£Œ")
print("=" * 70)

