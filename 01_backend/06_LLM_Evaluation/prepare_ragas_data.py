#!/usr/bin/env python3
"""
RAGAS í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸
- ëª©ì : ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ CSVì— ì¶”ê°€
- ì…ë ¥: llm_selected_model_full_test.csv
- ì¶œë ¥: llm_selected_model_full_test_with_contexts.csv
- íŠ¹ì§•: 50ê°œì”© ë°°ì¹˜ ì²˜ë¦¬, ì¬ê°œ ê¸°ëŠ¥, ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì§€ì›
"""

import json
import pandas as pd
import torch
import gc
import os
import sys
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Set
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
from konlpy.tag import Okt
import chromadb
from datetime import datetime

# -----------------------------------------------------------------------------
# ì„¤ì •
# -----------------------------------------------------------------------------
BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
INPUT_CSV = BASE_DIR / "06_LLM_Evaluation/results/llm_selected_model_full_test.csv"
OUTPUT_CSV = BASE_DIR / "06_LLM_Evaluation/results/llm_selected_model_full_test_with_contexts.csv"
CHUNK_FILE = BASE_DIR / "02_Chunking/output/all_chunks.json"
VECTORDB_DIR = BASE_DIR / "04_VectorDB/chroma_db"
COLLECTION_NAME = "korean_history_chunks"
RESULTS_DIR = BASE_DIR / "06_LLM_Evaluation/results"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# ì„ ì •ëœ ëª¨ë¸
EMBEDDING_MODEL = "BAAI/bge-m3"
RERANKER_MODEL = "Dongjin-kr/ko-reranker"

# ê²€ìƒ‰ ì„¤ì •
TOP_K_RETRIEVE = 3  # RAGAS í‰ê°€ì— í•„ìš”í•œ ìµœì¢… ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
CANDIDATE_K = 50  # 1ì°¨ ê²€ìƒ‰ í›„ë³´êµ° ê°œìˆ˜
BATCH_SIZE = 50  # ë°°ì¹˜ í¬ê¸°

# -----------------------------------------------------------------------------
# ê²€ìƒ‰ ì‹œìŠ¤í…œ í´ë˜ìŠ¤ (test_selected_model.pyì—ì„œ ê°€ì ¸ì˜´)
# -----------------------------------------------------------------------------
class BM25Retriever:
    def __init__(self, chunk_dict):
        self.chunk_dict = chunk_dict
        self.chunk_ids = list(chunk_dict.keys())
        self.tokenizer = Okt()
        print("   ğŸ§® BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        texts = []
        for chunk_id in tqdm(self.chunk_ids, desc="   BM25 í† í¬ë‚˜ì´ì§•", leave=False):
            text = chunk_dict[chunk_id]['text']
            tokens = [t for t in self.tokenizer.morphs(text, stem=True) if t.strip()]
            texts.append(tokens)
        self.bm25 = BM25Okapi(texts)
    
    def search(self, query: str, top_k: int) -> list:
        query_tokens = [t for t in self.tokenizer.morphs(query, stem=True) if t.strip()]
        scores = self.bm25.get_scores(query_tokens)
        top_indices = np.argsort(scores)[::-1][:top_k]
        return [{'chunk_id': self.chunk_ids[i], 'text': self.chunk_dict[self.chunk_ids[i]]['text'], 'score': float(scores[i])} for i in top_indices]

class VectorRetriever:
    def __init__(self, collection, model_name):
        self.collection = collection
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = SentenceTransformer(model_name, device=self.device)
        
    def search(self, query: str, top_k: int) -> list:
        query_emb = self.model.encode(query, normalize_embeddings=True, show_progress_bar=False).tolist()
        results = self.collection.query(query_embeddings=[query_emb], n_results=top_k)
        ret = []
        if results['ids'] and len(results['ids'][0]) > 0:
            for i, chunk_id in enumerate(results['ids'][0]):
                ret.append({
                    'chunk_id': chunk_id,
                    'text': results['documents'][0][i],
                    'score': 1.0 - results['distances'][0][i]
                })
        return ret

class HybridRetriever:
    def __init__(self, bm25, vector):
        self.bm25 = bm25
        self.vector = vector
        self.chunk_dict = bm25.chunk_dict
    
    def search_weighted(self, query: str, top_k: int, v_weight: float = 0.6, b_weight: float = 0.4) -> list:
        bm25_res = self.bm25.search(query, top_k * 2)
        vector_res = self.vector.search(query, top_k * 2)
        
        scores = {}
        for res, weight in [(bm25_res, b_weight), (vector_res, v_weight)]:
            if not res: continue
            max_s = max(r['score'] for r in res) if res else 1.0
            min_s = min(r['score'] for r in res) if res else 0.0
            denom = max_s - min_s if max_s != min_s else 1.0
            for r in res:
                norm = (r['score'] - min_s) / denom if denom > 0 else 0.5
                if r['chunk_id'] not in scores:
                    scores[r['chunk_id']] = {'chunk_id': r['chunk_id'], 'text': r['text'], 'score': 0.0}
                scores[r['chunk_id']]['score'] += weight * norm
                
        sorted_items = sorted(scores.values(), key=lambda x: x['score'], reverse=True)
        return sorted_items[:top_k]

class Reranker:
    def __init__(self, model_name):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = CrossEncoder(
            model_name, 
            device=self.device,
            automodel_args={"torch_dtype": torch.float16}
        )
    
    def rerank(self, query: str, candidates: list, top_k: int) -> list:
        if not candidates:
            return []
        
        pairs = [[query, item['text']] for item in candidates]
        scores = self.model.predict(pairs)
        
        reranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
        return [item for item, _ in reranked[:top_k]]

# -----------------------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# -----------------------------------------------------------------------------
def main():
    print("=" * 60)
    print("RAGAS í‰ê°€ìš© ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ CSV íŒŒì¼ ë¡œë“œ: {INPUT_CSV}")
    if not INPUT_CSV.exists():
        raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_CSV}")
    
    df = pd.read_csv(INPUT_CSV)
    print(f"   ì´ {len(df)}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")
    
    # ì¬ê°œ ê¸°ëŠ¥: ì´ë¯¸ contextsê°€ ìˆë‹¤ë©´ ê±´ë„ˆë›°ê¸°
    if OUTPUT_CSV.exists():
        print(f"\nğŸ”„ ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ ë°œê²¬: {OUTPUT_CSV}")
        existing_df = pd.read_csv(OUTPUT_CSV)
        if 'contexts' in existing_df.columns:
            processed_ids = set(existing_df['query_id'].tolist())
            df_to_process = df[~df['query_id'].isin(processed_ids)].copy()
            print(f"   ì´ë¯¸ ì²˜ë¦¬ë¨: {len(processed_ids)}ê°œ")
            print(f"   ë‚¨ì€ ì‘ì—…: {len(df_to_process)}ê°œ")
            
            if len(df_to_process) == 0:
                print("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                return
            
            # ê¸°ì¡´ ê²°ê³¼ì— ìƒˆ ê²°ê³¼ë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´ ê¸°ì¡´ DataFrame ì¤€ë¹„
            base_df = existing_df.copy()
        else:
            df_to_process = df.copy()
            base_df = None
    else:
        df_to_process = df.copy()
        base_df = None
    
    # 2. ì²­í¬ ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“š ì²­í¬ ë°ì´í„° ë¡œë“œ: {CHUNK_FILE}")
    with open(CHUNK_FILE, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    chunk_dict = {chunk['chunk_id']: chunk for chunk in chunks}
    print(f"   ì´ {len(chunk_dict)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    
    # 3. ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print(f"\nğŸ› ï¸  ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    
    # BM25
    bm25_retriever = BM25Retriever(chunk_dict)
    
    # Vector DB
    print("   ğŸ“ Vector DB ì—°ê²° ì¤‘...")
    client = chromadb.PersistentClient(path=str(VECTORDB_DIR))
    collection = client.get_collection(COLLECTION_NAME)
    vector_retriever = VectorRetriever(collection, EMBEDDING_MODEL)
    
    # Hybrid
    hybrid_retriever = HybridRetriever(bm25_retriever, vector_retriever)
    
    # Reranker
    print("   ğŸ”„ Reranker ë¡œë“œ ì¤‘...")
    reranker = Reranker(RERANKER_MODEL)
    
    print("   âœ… ê²€ìƒ‰ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    # 4. ê° ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰ ë° contexts ì¶”ê°€
    print(f"\nğŸ” ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘ (Top-{TOP_K_RETRIEVE} ë¬¸ì„œ)...")
    
    contexts_list = []
    results_list = []
    
    for idx, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc="   ê²€ìƒ‰ ì§„í–‰"):
        query = row['query']
        
        try:
            # Hybrid ê²€ìƒ‰ (í›„ë³´êµ° ë§ì´ ê°€ì ¸ì˜¤ê¸°)
            candidates = hybrid_retriever.search_weighted(query, top_k=CANDIDATE_K, v_weight=0.6, b_weight=0.4)
            
            # Reranking
            final_results = reranker.rerank(query, candidates, top_k=TOP_K_RETRIEVE)
            
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            contexts = [item['text'] for item in final_results]
            contexts_list.append(contexts)
            
            # ê²°ê³¼ ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„
            result_row = row.to_dict()
            result_row['contexts'] = contexts
            results_list.append(result_row)
            
        except Exception as e:
            print(f"\n   âš ï¸  ì§ˆë¬¸ {row['query_id']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ë¹ˆ contextsë¡œ ëŒ€ì²´
            contexts_list.append([])
            result_row = row.to_dict()
            result_row['contexts'] = []
            results_list.append(result_row)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë°°ì¹˜ë§ˆë‹¤)
        if (idx + 1) % BATCH_SIZE == 0:
            gc.collect()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    # 5. DataFrameì— contexts ì»¬ëŸ¼ ì¶”ê°€
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    new_df = pd.DataFrame(results_list)
    
    # ê¸°ì¡´ ê²°ê³¼ì™€ ë³‘í•©
    if base_df is not None:
        final_df = pd.concat([base_df, new_df], ignore_index=True)
        # ì¤‘ë³µ ì œê±° (query_id ê¸°ì¤€)
        final_df = final_df.drop_duplicates(subset=['query_id'], keep='last')
    else:
        final_df = new_df
    
    # contextsë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (CSV ì €ì¥ì„ ìœ„í•´)
    final_df['contexts'] = final_df['contexts'].apply(lambda x: json.dumps(x, ensure_ascii=False))
    
    # ì €ì¥
    final_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')
    
    print(f"   âœ… ì™„ë£Œ! ì´ {len(final_df)}ê°œ ì§ˆë¬¸ì— contexts ì¶”ê°€ë¨")
    print(f"   ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {OUTPUT_CSV}")
    
    print("\n" + "=" * 60)
    print("ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 60)

if __name__ == "__main__":
    main()

