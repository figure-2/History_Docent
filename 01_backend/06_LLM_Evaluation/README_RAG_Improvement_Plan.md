# [Engineering Log] 06-2. RAG Improvement Plan & Pivot Decision

**작성 일시:** 2025-11-25 01:30 (KST)
**작성자:** AI Assistant & User
**관련 문서:** 
- `README_RAGAS_Evaluation_Log.md`: 전체 RAGAS 평가 로그
- `README_RAGAS_Data_Leakage_Analysis.md`: Data Leakage 검증 결과
- `README_RAG_System_Integration.md`: RAG 시스템 통합 가이드

---

## 1. 가설 (Hypothesis)

### 1.1 문제 인식 (Problem Statement)

*   **누가 (Who):** 
    - 프로젝트 팀 (User & AI Assistant)
    - 전문가 3인 (현업 RAG 엔지니어, CTO, 기술 면접관)의 피드백
*   **언제 (When):** 
    - RAGAS 평가 완료 후 (2025-11-25 01:00)
    - Data Leakage 검증 완료 후 (2025-11-25 01:25)
    - 전문가 피드백 수렴 (2025-11-25 01:30)
*   **어디서 (Where):** 
    - `/home/pencilfoxs/00_new/History_Docent/06_LLM_Evaluation`
    - `/home/pencilfoxs/00_new/History_Docent` (RAG 시스템)
    - `/home/pencilfoxs/00_new/3_Docent_frontend_1117` (프론트엔드)
*   **무엇을 (What):** 
    - RAGAS 평가 결과 분석 및 향후 개선 계획 수립
    - 기존 계획(임베딩/리랭커 파인튜닝)의 타당성 검토
    - 전문가 피드백 반영한 전략적 피벗(Pivot) 결정
*   **왜 (Why):** 
    - 검증된 성능 지표를 바탕으로 효과적인 다음 단계 수립
    - 리소스(시간, 비용)를 최적의 방향으로 배분
    - 포트폴리오 완성도를 높이기 위한 실용적 접근

### 1.2 초기 가설 (Initial Hypothesis)

**가설 1: 임베딩/리랭커 파인튜닝이 필요하다**

*   **가정:**
    *   RAGAS 평가 후 검색 정확도를 더 높이기 위해 임베딩 모델과 리랭커 모델의 파인튜닝이 필요할 것이다.
    *   도메인 특화(한국사) 파인튜닝을 통해 성능이 더 향상될 것이다.

**가설 2: 현재 시스템은 서비스 배포 준비가 되지 않았다**

*   **가정:**
    *   평가만 완료된 상태이므로, 실제 서비스 배포까지는 추가 검증과 최적화가 필요할 것이다.

**가설 3: 속도는 나중에 최적화해도 된다**

*   **가정:**
    *   정확도가 확보되면 속도는 점진적으로 개선할 수 있다.
    *   일단 프론트엔드와 연결하여 작동 여부를 확인한 후 최적화를 진행해도 된다.

---

## 2. 실험 설계 (Experiment Design)

### 2.1 전문가 피드백 수렴 (Expert Feedback Collection)

**누가 (Who):** 3명의 전문가 (시뮬레이션)

1. **현업 RAG 엔지니어 (System Architect)**
   - 검색 성능 평가: Context Recall 0.96, Context Precision 0.92 → **S급 달성**
   - 생성 신뢰성: Faithfulness 0.86 → 개선 필요
   - 치명적 결함: Answer Relevancy NaN → 반드시 해결 필요

2. **CTO / 기술 총괄 (Decision Maker)**
   - 비즈니스 가치: 정확도 95%는 매우 강력한 세일즈 포인트
   - 비용/효율성 경고: 질문당 36초는 **실시간 서비스 불가능** (Red Flag)
   - 우선순위: Latency 단축이 급선무 (3~5초 목표)

3. **기술 면접관 / 시니어 개발자 (Interviewer)**
   - 데이터 무결성: Data Leakage 가능성 제기 (이미 검증 완료)
   - 평가 모델 편향: Judge LLM(Gemini)의 평가 방식 검토 필요
   - Answer Relevancy: NaN 원인 파악 및 해결 능력 확인

### 2.2 현재 상태 분석 (Current State Analysis)

**정량적 지표 (Quantitative Metrics):**

| 지표 | 값 | 평가 | 비고 |
|------|-----|------|------|
| Context Recall | 0.9591 | ✅ 최우수 | 90% 이상 달성 |
| Context Precision | 0.9183 | ✅ 우수 | 90% 이상 달성 |
| Faithfulness | 0.8616 | ⚠️ 양호 | 목표(0.90) 미달성 |
| Answer Relevancy | NaN | ❌ 실패 | 평가 파이프라인 오류 |
| **응답 속도 (Latency)** | **30~36초** | ❌ **치명적** | **실시간 서비스 불가능** |

**정성적 평가 (Qualitative Assessment):**

*   **검색 시스템:**
    *   Hybrid Search (BM25 + Vector) + Reranker 조합이 완벽하게 작동 중
    *   Abstract 유형(Unseen Query 시뮬레이션)에서도 0.87 이상의 성능 기록
    *   **결론:** 추가 파인튜닝 불필요 (ROI 매우 낮음)

*   **생성 시스템:**
    *   LLM이 검색된 문서에 충실하게 답변하는 경우가 대부분
    *   하지만 14% 정도에서 환각(Hallucination) 발생
    *   프롬프트 엔지니어링 개선 필요

*   **서비스 완성도:**
    *   백엔드 로직은 완성되었으나 프론트엔드와 연동되지 않음
    *   속도 문제로 인해 실제 배포 불가능

### 2.3 비교군 선정 (Why Candidates)

**기존 계획 vs 수정 계획 비교:**

| 항목 | 기존 계획 (Original Plan) | 수정 계획 (Revised Plan) |
|------|---------------------------|--------------------------|
| **1단계** | RAGAS 평가 | RAGAS 평가 (✅ 완료) |
| **2단계** | 임베딩/리랭커 파인튜닝 | **속도 최적화 (Latency Optimization)** |
| **3단계** | 시스템 구현 | 프론트엔드 연동 (Service Deployment) |
| **4단계** | 프론트엔드 연결 | Unseen Query 검증 (선택적) |

**후보군 선정 이유 (Why Candidates):**

1. **속도 최적화 (3단계)를 먼저 하는 이유:**
    *   **기술적 안정성:** 30초 이상 응답 시 타임아웃(Timeout) 오류 발생 가능성 높음
    *   **사용자 경험:** 웹페이지에서 30초 대기는 사용자 이탈 유발
    *   **완성도:** 최적화 후 연동하면 더 완성도 높은 결과물 확보 가능

2. **임베딩/리랭커 파인튜닝을 취소하는 이유:**
    *   **한계 효용 체감:** 이미 검색 정확도가 최상위 수준(0.96)이므로 추가 개선의 ROI가 매우 낮음
    *   **전문가 의견:** "더 이상 검색 모델 튜닝에 시간을 쏟지 마세요"
    *   **우선순위:** 치명적 문제(속도) 해결이 더 시급함

### 2.4 검증 기준 (Criteria)

**피벗 결정 기준:**

*   **기존 계획 유지 조건:**
    *   검색 정확도가 목표치 미만일 경우
    *   추가 파인튜닝으로 의미 있는 개선이 예상될 경우

*   **피벗 조건:**
    *   ✅ 검색 정확도가 이미 최상위 수준인 경우 (달성)
    *   ✅ 다른 치명적 문제(속도)가 발견된 경우 (발견)
    *   ✅ 전문가 피드백이 명확한 경우 (수렴)

**속도 최적화 목표:**

*   **현재:** 30~36초/질문
*   **목표:** 3~5초/질문 (10배 향상)
*   **판단 기준:**
    *   3초 이하: 우수 (실시간 서비스 가능)
    *   3~5초: 양호 (로딩 UI로 충분히 처리 가능)
    *   5초 이상: 개선 필요

---

## 3. 검증 결과 (Validation)

### 3.1 전문가 피드백 종합 분석

**누가 (Who):** 3명의 전문가 의견 수렴

**언제 (When):** 2025-11-25 01:30 (KST)

**어디서 (Where):** 프로젝트 팀 내부 검토 회의

**무엇을 (What):** RAGAS 평가 결과에 대한 전문가 의견 종합

**어떻게 (How):** 각 전문가의 관점에서 제시된 피드백을 정리 및 분석

**왜 (Why):** 객관적이고 전문적인 의견을 바탕으로 전략적 의사결정 수행

#### 3.1.1 현업 RAG 엔지니어 의견

**검색 성능 평가:**
- Context Recall(0.96)과 Context Precision(0.92)이 모두 90% 이상 → **"졸업 수준"**
- 일반적으로 Recall과 Precision은 트레이드오프 관계인데, 둘 다 높다는 것은 **하이브리드 검색 + 리랭커** 조합이 완벽하게 작동 중임을 의미
- **결론:** 더 이상 검색 모델(임베딩, 리랭커) 파인튜닝에 시간을 쏟지 말 것

**생성 신뢰성 평가:**
- Faithfulness(0.86)는 준수하나, 14%의 경우에서 환각(Hallucination) 발생 가능
- **액션 아이템:** 프롬프트 엔지니어링 개선 필요 ("Context에 없는 내용은 절대 답하지 마시오" 제약 조건 강화)

**치명적 결함:**
- Answer Relevancy가 NaN → 평가 파이프라인의 심각한 오류
- 반드시 로그 확인하여 복구 필요 (이미 해결 방법 도출: 래퍼 클래스 적용)

#### 3.1.2 CTO / 기술 총괄 의견

**비즈니스 가치 평가:**
- "정확도 95% 검색 시스템"은 투자자나 경영진에게 매우 강력한 세일즈 포인트
- 데이터 품질과 RAG 파이프라인의 **정확성**은 입증됨

**비용 및 효율성 경고 (Red Flag):**
- 2,223개 데이터 평가에 19.4시간 소요 → 질문당 약 **36초**
- 사용자가 질문 하나 던지고 36초를 기다려야 한다면 **서비스 런칭 불가능**
- **지시 사항:**
    1. **Latency 단축:** vLLM 도입, 양자화(Quantization) 모델 적용 등을 통해 추론 속도를 1/10 수준(3~5초)으로 줄이는 것이 급선무
    2. **리소스 최적화:** GPU 메모리 사용량 고려하여, 더 작은 GPU 인스턴스로도 운영 가능한지 테스트하여 클라우드 비용 절감

#### 3.1.3 기술 면접관 / 시니어 개발자 의견

**데이터 무결성 의심:**
- "Recall이 0.96이라니, 혹시 Data Leakage가 있지 않나요?"
- **해결:** Abstract 유형 검증 완료 → 일부 Data Leakage 존재하나 심각하지 않음 (이미 검증 완료)

**평가 모델의 편향:**
- Judge 모델로 Gemini를 사용했는데, 한국어 뉘앙스 평가 시 GPT-4나 Claude 대비 점수를 후하게 주는 경향이 있는지 확인 필요
- **대응:** 향후 다른 Judge 모델과 비교 검증 계획

**Answer Relevancy NaN 원인:**
- "한국어 답변을 임베딩 모델이 제대로 처리 못해서"인지, "비동기 API 처리 중 타임아웃"인지 원인 파악 및 해결 능력 확인
- **해결:** 디버깅 완료 → `HuggingFaceEmbeddings`의 `embed_query` 메서드 부재로 인한 오류 (래퍼 클래스로 해결)

### 3.2 정량적 검증 결과

**검색 정확도 (Retrieval Accuracy):**

| 지표 | 평균 점수 | 평가 | 전문가 의견 |
|------|----------|------|------------|
| Context Recall | 0.9591 | ✅ 최우수 | "졸업 수준" |
| Context Precision | 0.9183 | ✅ 우수 | "더 이상 튜닝 불필요" |
| Abstract 유형 Recall | 0.8723 | ✅ 양호 | "일반화 성능 확인" |

**생성 신뢰성 (Generation Reliability):**

| 지표 | 평균 점수 | 평가 | 개선 필요 |
|------|----------|------|-----------|
| Faithfulness | 0.8616 | ⚠️ 양호 | 프롬프트 개선 |
| Answer Relevancy | NaN | ❌ 실패 | 래퍼 클래스 적용 (해결 방법 도출) |

**응답 속도 (Latency):**

| 항목 | 값 | 평가 | 전문가 의견 |
|------|-----|------|------------|
| 현재 속도 | 30~36초/질문 | ❌ 치명적 | "서비스 런칭 불가능" |
| 목표 속도 | 3~5초/질문 | ✅ 목표 | "급선무" |
| 필요한 개선 | 10배 향상 | 🔥 최우선 | "vLLM 도입 필요" |

### 3.3 정성적 검증 결과

**검색 시스템 평가:**
- Hybrid Search + Reranker 조합이 완벽하게 작동
- Abstract 유형(Unseen Query 시뮬레이션)에서도 0.87 이상의 성능 기록
- **결론:** 추가 파인튜닝 불필요 (ROI 매우 낮음)

**서비스 배포 준비도:**
- 백엔드 로직은 완성되었으나 프론트엔드와 연동되지 않음
- 속도 문제로 인해 실제 배포 불가능
- **결론:** 속도 최적화 선행 후 연동 필요

---

## 4. 의사결정 (Conclusion & Pivot)

### 4.1 실험 결과 요약

**누가 (Who):** 프로젝트 팀 (User & AI Assistant)

**언제 (When):** 2025-11-25 01:30 (KST) - 전문가 피드백 수렴 후 의사결정

**어디서 (Where):** `/home/pencilfoxs/00_new/History_Docent/06_LLM_Evaluation`

**무엇을 (What):** 기존 계획(임베딩/리랭커 파인튜닝) 취소 및 새로운 로드맵 수립

**어떻게 (How):** 전문가 피드백 종합 분석 → 정량/정성 평가 → 전략적 피벗 결정

**왜 (Why):** 검증된 성능 지표와 전문가 의견을 바탕으로 효과적인 리소스 배분

#### 4.1.1 성공한 부분

1. **검색 정확도 최상위 수준 달성:** Context Recall 0.96, Context Precision 0.92
2. **Data Leakage 검증 완료:** Abstract 유형 검증으로 일반화 성능 확인
3. **Answer Relevancy 디버깅 성공:** 기술적 원인 파악 및 해결 방법 도출

#### 4.1.2 개선이 필요한 부분

1. **응답 속도 (치명적):** 30~36초/질문 → 3~5초 목표 (10배 향상 필요)
2. **Faithfulness 개선:** 0.86 → 0.90 목표 (프롬프트 엔지니어링)
3. **서비스 연동 미완성:** 프론트엔드와 백엔드 연결 필요

### 4.2 최종 결론

#### 4.2.1 계획 취소 (Cancel)

**임베딩/리랭커 파인튜닝 계획 취소 결정:**

*   **이유 1: 한계 효용 체감 (Diminishing Returns)**
    *   이미 검색 정확도가 최상위 수준(Recall 0.96, Precision 0.92)이므로, 추가 파인튜닝의 ROI가 매우 낮음
    *   전문가 의견: "더 이상 검색 모델 튜닝에 시간을 쏟지 마세요"

*   **이유 2: 치명적 문제 우선 해결 (Critical Path)**
    *   응답 속도(30~36초)가 실시간 서비스 배포의 치명적 병목(Blocker)
    *   정확도를 더 높이는 것보다 속도를 개선하는 것이 사용자 경험에 더 큰 영향

*   **이유 3: 비용 대비 효과 (Cost-Benefit Analysis)**
    *   파인튜닝에 소요되는 시간/비용 대비 예상 개선 폭이 작음 (0.96 → 0.98? 의미 없음)
    *   반면 속도 최적화는 10배 향상 가능 (30초 → 3초)

**논리적 근거:**
- 전문가 의견 수렴: 현업 엔지니어 "검색은 졸업 수준", CTO "속도 개선이 급선무"
- 정량적 근거: 검색 정확도 이미 최상위, 속도는 실시간 서비스 불가능 수준
- 정성적 근거: 포트폴리오에서 "평가 점수"보다 "실제 동작하는 서비스"가 더 강력

#### 4.2.2 계획 수정 (Pivot)

**새로운 로드맵 수립:**

| 단계 | 목표 | 주요 작업 (Action Items) | 우선순위 | 예상 소요 시간 |
| :--- | :--- | :--- | :--- | :--- |
| **Step 1** | **속도 최적화** | - vLLM 추론 엔진 도입<br>- 모델 양자화 검토<br>- 벤치마크 테스트 | 🔥 **최우선** | 1-2일 |
| **Step 2** | **서비스 연동** | - FastAPI 백엔드 구축<br>- Next.js 프론트엔드 액션 수정<br>- CORS/타임아웃 처리 | 🔥 **중요** | 0.5일 |
| **Step 3** | **일반화 검증** | - Unseen Query (외부 데이터) 테스트<br>- 스트리밍(Streaming) 응답 적용 | 권장 | 1일 |

**논리적 근거:**
- **기술적 안정성:** 30초 이상 응답 시 타임아웃 오류 발생 가능성 높음
- **사용자 경험:** 웹페이지에서 30초 대기는 사용자 이탈 유발
- **완성도:** 최적화 후 연동하면 더 완성도 높은 결과물 확보 가능
- **포트폴리오:** "평가 점수"보다 "실제 동작하는 서비스"가 더 강력한 증거

### 4.3 다음 단계 결정 (Next Steps Decision)

#### 4.3.1 즉시 실행 항목 (Immediate Actions)

**1. 속도 최적화 (Step 1) - 최우선**

*   **목표:** 응답 속도 30~36초 → 3~5초 단축 (10배 향상)
*   **액션:**
    1.  `history_docent.py` 수정하여 vLLM 적용
    2.  벤치마크 테스트: 간단한 질문으로 속도 측정
    3.  모델 양자화 검토 (필요 시)
*   **성공 기준:** 질문당 평균 응답 시간 5초 이하
*   **왜:** 실시간 서비스 배포를 위한 필수 조건

**2. 프론트엔드 연동 (Step 2) - 중요**

*   **목표:** 최적화된 RAG 엔진을 웹 인터페이스에 연동
*   **액션:**
    1.  FastAPI 서버(`main.py`) 실행
    2.  Next.js 프론트엔드 액션(`actions.ts`) 수정
    3.  CORS 및 타임아웃 처리
*   **성공 기준:** 웹 브라우저에서 질문-답변 정상 작동
*   **왜:** 포트폴리오의 핵심 성과물 (실제 동작하는 서비스)

#### 4.3.2 단기 계획 (Short-term Plan)

**1. Faithfulness 개선 (프롬프트 엔지니어링)**
*   프롬프트에 "Context에 없는 내용은 절대 답하지 마시오" 제약 조건 강화
*   Temperature 조정 (현재 0.1 → 더 낮추기 검토)

**2. Answer Relevancy 재측정 (평가 스크립트 수정)**
*   `evaluate_ragas_full.py`에 래퍼 클래스 적용
*   전체 데이터셋에 대해 Answer Relevancy 지표 재계산

#### 4.3.3 중장기 계획 (Long-term Plan)

**1. Unseen Query 검증**
*   외부 데이터(수능 기출, 한국사능력검정시험)로 테스트
*   Data Leakage 최종 불식

**2. 스트리밍(Streaming) 응답 적용**
*   3~5초도 기다리기 지루할 수 있으므로, 글자가 타자 치듯 나오는 스트리밍 응답 적용
*   vLLM은 스트리밍을 기본 지원

**3. RAG 시스템 2단계 (Multi-turn Conversation)**
*   대화 히스토리 관리
*   Query Rewriter 구현
*   맥락 유지 대화 가능

### 4.4 의사결정 근거 요약

**임베딩/리랭커 파인튜닝 취소:**
- 검색 정확도가 이미 최상위 수준 (Recall 0.96, Precision 0.92)
- 전문가 의견: "더 이상 검색 모델 튜닝 불필요"
- 추가 개선의 ROI가 매우 낮음

**속도 최적화 우선:**
- 현재 속도(30~36초)는 실시간 서비스 불가능
- 전문가 의견: "급선무", "서비스 런칭 불가능"
- 10배 향상 가능 (30초 → 3초)

**프론트엔드 연동 (속도 최적화 후):**
- 기술적 안정성 확보 후 연동 필요
- 포트폴리오에서 "실제 동작하는 서비스"가 더 강력한 증거

---

## 5. 참고 자료 (References)

### 5.1 관련 문서
- `README_RAGAS_Evaluation_Log.md`: 전체 RAGAS 평가 로그
- `README_RAGAS_Data_Leakage_Analysis.md`: Data Leakage 검증 결과
- `README_RAG_System_Integration.md`: RAG 시스템 통합 가이드

### 5.2 기술 문서
- `history_docent.py`: 통합 RAG 클래스
- `main.py`: FastAPI 서버 코드
- `evaluate_ragas_full.py`: RAGAS 평가 스크립트
- `analyze_and_debug_ragas.py`: Data Leakage 검증 스크립트

### 5.3 결과 파일
- `results/ragas_evaluation_results.csv`: RAGAS 평가 결과 (2,223행)
- `results/llm_selected_model_full_test.csv`: 메타데이터 (type, chunk_id 포함)

### 5.4 전문가 피드백
- 현업 RAG 엔지니어: "검색은 졸업 수준, 생성 신뢰성 개선 필요"
- CTO: "정확도 95%는 강력하나, 속도 36초는 서비스 불가능"
- 기술 면접관: "Data Leakage 검증 필요, Answer Relevancy NaN 원인 파악"

---

**작성 완료일:** 2025-11-25 01:30 (KST)
**실행 상태:** ✅ 의사결정 완료, 다음 단계 준비
**최종 평가:** 임베딩/리랭커 파인튜닝 취소, 속도 최적화 우선 진행

