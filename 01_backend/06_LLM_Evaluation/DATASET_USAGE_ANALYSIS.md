# 11,140개 데이터셋 활용 가능성 분석

## 현재 데이터셋 구조
```json
{
  "query": "질문",
  "gold_text": "정답 문서",
  "chunk_id": "chk_000000",
  "type": "keyword|context|abstract",
  "metadata": {...}
}
```

---

## 1. 임베딩 모델 선정 ✅ **가능 (현재 구조 그대로 사용)**

### 필요한 데이터
- ✅ query (질문)
- ✅ gold_text (정답 문서)
- ✅ chunk_id (ground truth)

### 평가 방법
- 질문과 문서를 임베딩하여 유사도 계산
- Recall@K, MRR 계산
- 질문 유형별 성능 분석 가능

### 상태
**✅ 바로 사용 가능**

---

## 2. 임베딩 모델 파인튜닝 ⚠️ **가능하지만 추가 작업 필요**

### 필요한 데이터
- ✅ query (질문)
- ✅ gold_text (positive 문서)
- ❌ negative 문서 (부정 샘플) - **추가 생성 필요**

### 추가 작업
1. **Negative 샘플 생성**
   - 각 질문에 대해 정답이 아닌 다른 청크들을 negative로 사용
   - Hard negative mining (유사하지만 정답이 아닌 문서)
   - Random negative (무작위 문서)

2. **학습 데이터 형식 변환**
   ```json
   {
     "query": "질문",
     "positive": "정답 문서",
     "negatives": ["부정 문서1", "부정 문서2", ...]
   }
   ```

### 상태
**⚠️ Negative 샘플 생성 후 사용 가능**

---

## 3. 리트리버 개선 ✅ **가능 (현재 구조 그대로 사용)**

### 필요한 데이터
- ✅ query (질문)
- ✅ gold_text (정답 문서)
- ✅ 전체 문서 corpus (3,719개 청크)

### 평가 방법
- BM25, Vector Search, Hybrid Search 비교
- Recall@K, MRR 계산
- 질문 유형별 성능 분석

### 상태
**✅ 바로 사용 가능**

---

## 4. 리랭커 파인튜닝 ⚠️ **가능하지만 추가 작업 필요**

### 필요한 데이터
- ✅ query (질문)
- ✅ gold_text (positive 문서)
- ❌ negative 문서들 (부정 샘플) - **추가 생성 필요**
- ❌ relevance score (관련도 점수) - **추가 생성 필요**

### 추가 작업
1. **Negative 샘플 생성**
   - 정답이 아닌 다른 청크들을 negative로 사용
   - Hard negative (유사하지만 정답이 아닌 문서)

2. **Relevance Score 생성**
   - Positive: 1 (정답)
   - Hard Negative: 0.3-0.5 (유사하지만 정답 아님)
   - Random Negative: 0 (무관련)

3. **학습 데이터 형식 변환**
   ```json
   {
     "query": "질문",
     "documents": [
       {"text": "정답 문서", "score": 1.0},
       {"text": "부정 문서1", "score": 0.3},
       {"text": "부정 문서2", "score": 0.0}
     ]
   }
   ```

### 상태
**⚠️ Negative 샘플 및 Score 생성 후 사용 가능**

---

## 5. LLM 모델 선정 ⚠️ **가능하지만 추가 작업 필요**

### 필요한 데이터 (RAGAS 평가용)
- ✅ question (질문)
- ✅ contexts (검색된 문서들) - **RAG 실행 필요**
- ❌ answer (LLM이 생성한 답변) - **추가 생성 필요**
- ❌ ground_truth (정답) - **추가 생성 필요**

### 추가 작업
1. **답변 생성**
   - 각 질문에 대해 선택한 LLM으로 답변 생성
   - RAG 파이프라인 실행 (Retrieval → Generation)

2. **Ground Truth 생성**
   - gold_text를 기반으로 정답 요약 생성
   - 또는 gold_text 자체를 ground_truth로 사용

3. **RAGAS 평가 형식 변환**
   ```json
   {
     "question": "질문",
     "contexts": ["검색된 문서1", "검색된 문서2", ...],
     "answer": "LLM이 생성한 답변",
     "ground_truth": "정답"
   }
   ```

### 상태
**⚠️ RAG 실행 및 답변 생성 후 사용 가능**

---

## 종합 분석

| 용도 | 현재 구조 사용 가능 | 추가 작업 필요 | 난이도 |
|------|-------------------|--------------|--------|
| 1. 임베딩 모델 선정 | ✅ 가능 | 없음 | 쉬움 |
| 2. 임베딩 모델 파인튜닝 | ⚠️ 가능 | Negative 샘플 생성 | 중간 |
| 3. 리트리버 개선 | ✅ 가능 | 없음 | 쉬움 |
| 4. 리랭커 파인튜닝 | ⚠️ 가능 | Negative 샘플 + Score 생성 | 중간 |
| 5. LLM 모델 선정 | ⚠️ 가능 | RAG 실행 + 답변 생성 | 중간 |

---

## 권장 순서

1. **1단계: 임베딩 모델 선정** (바로 가능)
   - 현재 데이터셋으로 7개 모델 평가
   - 최적 모델 선정

2. **2단계: 리트리버 개선** (바로 가능)
   - 선정된 임베딩 모델로 리트리버 성능 평가
   - BM25 vs Vector vs Hybrid 비교

3. **3단계: 리랭커 파인튜닝** (추가 작업 필요)
   - Negative 샘플 생성
   - 리랭커 파인튜닝

4. **4단계: 임베딩 모델 파인튜닝** (추가 작업 필요)
   - Negative 샘플 생성
   - 선정된 모델 파인튜닝

5. **5단계: LLM 모델 선정** (추가 작업 필요)
   - RAG 파이프라인 실행
   - 답변 생성 및 RAGAS 평가

---

## 결론

**현재 데이터셋으로 바로 사용 가능:**
- ✅ 임베딩 모델 선정
- ✅ 리트리버 개선

**추가 작업 후 사용 가능:**
- ⚠️ 임베딩 모델 파인튜닝 (Negative 샘플 생성)
- ⚠️ 리랭커 파인튜닝 (Negative 샘플 + Score 생성)
- ⚠️ LLM 모델 선정 (RAG 실행 + 답변 생성)

**데이터셋 형태 변경 필요 여부:**
- 기본 구조는 유지 가능
- 각 용도에 맞게 변환 스크립트 작성 필요


