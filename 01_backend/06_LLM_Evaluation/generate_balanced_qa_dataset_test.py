#!/usr/bin/env python3
"""
ê· í˜• ì¡íŒ QA ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Robust Version)
- ëª¨ë“  ì²­í¬ì— ëŒ€í•´ Keyword, Context, Abstract 3ê°€ì§€ ìœ í˜• ì§ˆë¬¸ ìƒì„±
- Rate Limit (429) ë°©ì–´: ì§€ìˆ˜ ë°±ì˜¤í”„(Exponential Backoff)
- 10ê°œ ë‹¨ìœ„ ìë™ ì €ì¥ (ë°ì´í„° ì†ì‹¤ ë°©ì§€)
- ì´ì–´í•˜ê¸° ê¸°ëŠ¥ (ì¤‘ë‹¨ í›„ ì¬ì‹¤í–‰ ì‹œ ì´ì–´ì„œ ì§„í–‰)
- ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì§€ì› (nohup)
"""

import json
import time
import os
import random
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import requests
from datetime import datetime

# -----------------------------------------------------------------------------
# ì„¤ì • (Configuration)
# -----------------------------------------------------------------------------
ENV_FILE = Path("/home/pencilfoxs/00_new/.env2")
API_KEY = None

# API Key ë¡œë“œ (ê³µë°± ì²˜ë¦¬ í¬í•¨)
if ENV_FILE.exists():
    with open(ENV_FILE, 'r') as f:
        for line in f:
            if "GOOGLE_API_KEY_2" in line and "=" in line:
                API_KEY = line.split("=", 1)[1].strip()
                break

if not API_KEY:
    print("âš ï¸ Error: GOOGLE_API_KEY_2 not found in .env2")
    exit(1)

GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent"

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
CHUNK_FILE = BASE_DIR / "02_Chunking/output/all_chunks.json"
OUTPUT_DIR = BASE_DIR / "06_LLM_Evaluation"
OUTPUT_DIR.mkdir(exist_ok=True)

OUTPUT_FILE = OUTPUT_DIR / "balanced_qa_dataset_test.jsonl"  # í…ŒìŠ¤íŠ¸ìš© JSONL í¬ë§·
LOG_FILE = OUTPUT_DIR / "generation_progress_test.log"
STATS_FILE = OUTPUT_DIR / "generation_stats_test.json"

# ì‹¤í–‰ ì„¤ì •
MAX_WORKERS = 5  # 429 ë°©ì§€ë¥¼ ìœ„í•´ ì›Œì»¤ ìˆ˜ ì¡°ì ˆ
SAVE_INTERVAL = 10  # 10ê°œ ë‹¨ìœ„ ì €ì¥
BASE_DELAY = 0.5  # ê¸°ë³¸ API í˜¸ì¶œ ê°„ ë”œë ˆì´ (ì´ˆ)
MAX_RETRIES = 5  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

# -----------------------------------------------------------------------------
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Prompt Templates)
# -----------------------------------------------------------------------------

# 1. í‚¤ì›Œë“œí˜• (BM25 ìœ ë¦¬)
PROMPT_KEYWORD = """
ë‹¹ì‹ ì€ 'í•œêµ­ì‚¬ ì‹œí—˜ì„ ì¤€ë¹„í•˜ëŠ” í•™ìƒ'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ **ì‚¬ì‹¤, ì—°ë„, ì¸ë¬¼, ì‚¬ê±´ëª…**ì„ í™•ì¸í•˜ëŠ” ë‹¨ë‹µí˜• ì§ˆë¬¸ì„ **í•˜ë‚˜ë§Œ** ë§Œë“œì„¸ìš”.

[ê·œì¹™]
1. í•µì‹¬ ê³ ìœ ëª…ì‚¬(í‚¤ì›Œë“œ)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì„œ ì§ˆë¬¸í•˜ì„¸ìš”.
2. ëª…í™•í•œ ì •ë‹µì´ ë‚˜ì˜¤ë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë³´ì„¸ìš”.
3. **ë°˜ë“œì‹œ ì§ˆë¬¸ í•˜ë‚˜ë§Œ** ì¶œë ¥í•˜ì„¸ìš”. (ë²ˆí˜¸ë‚˜ ì—¬ëŸ¬ ì§ˆë¬¸ ê¸ˆì§€)
4. ì˜ˆ: "ì´ì„±ê³„ê°€ ìœ„í™”ë„ íšŒêµ°ì„ ë‹¨í–‰í•œ ë…„ë„ëŠ” ì–¸ì œì¸ê°€ìš”?"

[í…ìŠ¤íŠ¸]
{text}

[ì§ˆë¬¸ ìƒì„± (ì§ˆë¬¸ í•˜ë‚˜ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥)]
"""

# 2. ë¬¸ë§¥/ìŠ¤í† ë¦¬í˜• (Hybrid ìœ ë¦¬)
PROMPT_CONTEXT = """
ë‹¹ì‹ ì€ 'ì—­ì‚¬ ì´ì•¼ê¸°ë¥¼ ë“£ëŠ” ê´€ëŒê°'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ **ì¸ê³¼ê´€ê³„, ì´ìœ , ë°°ê²½**ì— ëŒ€í•´ ë¬¼ì–´ë³´ëŠ” ì§ˆë¬¸ì„ **í•˜ë‚˜ë§Œ** ë§Œë“œì„¸ìš”.

[ê·œì¹™]
1. ë‹¨ìˆœ ì‚¬ì‹¤ë³´ë‹¤ëŠ” "ì™œ?", "ì–´ë–»ê²Œ?", "ê·¸ ê²°ê³¼ëŠ”?" ìœ„ì£¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”.
2. ë¬¸ë§¥ì„ ì´í•´í•´ì•¼ ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”.
3. **ë°˜ë“œì‹œ ì§ˆë¬¸ í•˜ë‚˜ë§Œ** ì¶œë ¥í•˜ì„¸ìš”. (ë²ˆí˜¸ë‚˜ ì—¬ëŸ¬ ì§ˆë¬¸ ê¸ˆì§€)
4. ì˜ˆ: "ì´ì„±ê³„ê°€ ìš”ë™ ì •ë²Œì„ ë°˜ëŒ€í•˜ê³  ê²°êµ­ íšŒêµ°í•˜ê²Œ ëœ ê²°ì •ì ì¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

[í…ìŠ¤íŠ¸]
{text}

[ì§ˆë¬¸ ìƒì„± (ì§ˆë¬¸ í•˜ë‚˜ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥)]
"""

# 3. ì¶”ìƒ/í’€ì–´ì“°ê¸°í˜• (Vector ìœ ë¦¬)
PROMPT_ABSTRACT = """
ë‹¹ì‹ ì€ 'ì—­ì‚¬ ìš©ì–´ê°€ ì˜ ê¸°ì–µë‚˜ì§€ ì•ŠëŠ” ì¼ë°˜ì¸'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ë¬¼ì–´ë³´ë˜, **í•µì‹¬ ê³ ìœ ëª…ì‚¬ë¥¼ ì“°ì§€ ë§ê³  í’€ì–´ì„œ** ì§ˆë¬¸í•˜ì„¸ìš”.

[ê·œì¹™]
1. **ì ˆëŒ€ ë³¸ë¬¸ì˜ í•µì‹¬ ê³ ìœ ëª…ì‚¬(ì¸ë¬¼ëª…, ì‚¬ê±´ëª… ë“±)ë¥¼ ì§ì ‘ ì“°ì§€ ë§ˆì„¸ìš”.**
2. "ê·¸ê±° ìˆì–ì•„", "ê·¸ ì‚¬ëŒ", "ê·¸ ì‚¬ê±´" ì²˜ëŸ¼ ëŒ€ëª…ì‚¬ë‚˜ ë¬˜ì‚¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
3. ì˜ˆ: "ë¹„ ë§ì´ ì˜¨ë‹¤ê³  êµ°ëŒ€ ëŒë ¤ì„œ ì™• ì«“ì•„ë‚¸ ê·¸ ì‚¬ê±´ì´ ë­ì˜ˆìš”?" (ìœ„í™”ë„ íšŒêµ° ì–¸ê¸‰ X)

[í…ìŠ¤íŠ¸]
{text}

[ì§ˆë¬¸ ìƒì„± (ì§ˆë¬¸ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ)]
"""

# -----------------------------------------------------------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# -----------------------------------------------------------------------------

def log_message(message):
    """ë¡œê·¸ íŒŒì¼ì— ë©”ì‹œì§€ ê¸°ë¡"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] {message}\n"
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(log_entry)
    print(message)  # ì½˜ì†”ì—ë„ ì¶œë ¥

def call_gemini_api(payload, retry_count=0):
    """ì§€ìˆ˜ ë°±ì˜¤í”„(Exponential Backoff) ì ìš©ëœ API í˜¸ì¶œ"""
    headers = {"Content-Type": "application/json"}
    
    try:
        # ê¸°ë³¸ ë”œë ˆì´
        time.sleep(BASE_DELAY + random.uniform(0, 0.2))
        
        response = requests.post(
            f"{GEMINI_API_URL}?key={API_KEY}",
            json=payload,
            headers=headers,
            timeout=30
        )
        
        if response.status_code == 200:
            return response.json()
        
        # 429 (Too Many Requests) ë˜ëŠ” 500ë²ˆëŒ€ ì—ëŸ¬ ì‹œ ì¬ì‹œë„
        if response.status_code == 429 or response.status_code >= 500:
            if retry_count < MAX_RETRIES:
                # ì§€ìˆ˜ ë°±ì˜¤í”„: 2^retry_count ì´ˆ + ëœë¤ ì¶”ê°€
                wait_time = (2 ** retry_count) + random.uniform(0, 1)
                log_message(f"[Retry {retry_count + 1}/{MAX_RETRIES}] Status {response.status_code}. Waiting {wait_time:.1f}s...")
                time.sleep(wait_time)
                return call_gemini_api(payload, retry_count + 1)
            else:
                log_message(f"[Fail] Max retries reached. Status: {response.status_code}")
                return None
        
        # ê¸°íƒ€ ì—ëŸ¬ (400, 404 ë“±)
        log_message(f"[Error] Status {response.status_code}: {response.text[:200]}")
        return None

    except requests.exceptions.Timeout:
        if retry_count < MAX_RETRIES:
            wait_time = (2 ** retry_count) + random.uniform(0, 1)
            log_message(f"[Timeout Retry {retry_count + 1}/{MAX_RETRIES}] Waiting {wait_time:.1f}s...")
            time.sleep(wait_time)
            return call_gemini_api(payload, retry_count + 1)
        return None
    except Exception as e:
        if retry_count < MAX_RETRIES:
            wait_time = (2 ** retry_count)
            log_message(f"[Exception Retry {retry_count + 1}/{MAX_RETRIES}] {str(e)[:100]}. Waiting {wait_time:.1f}s...")
            time.sleep(wait_time)
            return call_gemini_api(payload, retry_count + 1)
        log_message(f"[Fail] Exception: {str(e)[:200]}")
        return None

def clean_question(question):
    """ìƒì„±ëœ ì§ˆë¬¸ ì „ì²˜ë¦¬ (ë²ˆí˜¸ ì œê±°, ë§ˆí¬ë‹¤ìš´ ì œê±° ë“±)"""
    question = question.strip()
    
    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
    if question.startswith("```"):
        question = question.replace("```json", "").replace("```", "").strip()
    
    # ì—¬ëŸ¬ ì§ˆë¬¸ì´ ìƒì„±ëœ ê²½ìš° ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ ì¶”ì¶œ
    lines = question.split('\n')
    first_question = lines[0]
    
    # "1. ", "2. " ê°™ì€ ë²ˆí˜¸ ì œê±°
    if first_question.strip() and first_question.strip()[0].isdigit():
        parts = first_question.split('.', 1)
        if len(parts) > 1:
            first_question = parts[1].strip()
        else:
            # "1)" í˜•ì‹ ì²˜ë¦¬
            parts = first_question.split(')', 1)
            if len(parts) > 1:
                first_question = parts[1].strip()
    
    return first_question.strip()

def generate_single_question(chunk, q_type):
    """ë‹¨ì¼ ì§ˆë¬¸ ìƒì„±"""
    text = chunk['text']
    chunk_id = chunk['chunk_id']
    
    # í”„ë¡¬í”„íŠ¸ ì„ íƒ
    if q_type == 'keyword':
        prompt_text = PROMPT_KEYWORD.format(text=text)
        temperature = 0.5
    elif q_type == 'context':
        prompt_text = PROMPT_CONTEXT.format(text=text)
        temperature = 0.6
    else:  # abstract
        prompt_text = PROMPT_ABSTRACT.format(text=text)
        temperature = 0.8  # ì¶”ìƒí˜•ì€ ì°½ì˜ì„± í•„ìš”
    
    payload = {
        "contents": [{
            "parts": [{
                "text": prompt_text
            }]
        }],
        "generationConfig": {
            "temperature": temperature,
            "maxOutputTokens": 150,
        }
    }
    
    api_result = call_gemini_api(payload)
    
    if api_result and 'candidates' in api_result and api_result['candidates']:
        question = api_result['candidates'][0]['content']['parts'][0]['text'].strip()
        question = clean_question(question)
        
        if question and len(question) > 10:  # ìµœì†Œ ê¸¸ì´ ê²€ì¦
            return {
                "chunk_id": chunk_id,
                "type": q_type,
                "question": question,
                "source_text": text,
                "source_metadata": chunk.get('metadata', {}),
                "generated_at": datetime.now().isoformat()
            }
    
    return None

def process_chunk(chunk):
    """ì²­í¬ ì²˜ë¦¬: 3ê°€ì§€ ìœ í˜• ëª¨ë‘ ìƒì„±"""
    results = []
    
    # ëª¨ë“  ì²­í¬ì— ëŒ€í•´ 3ê°€ì§€ ìœ í˜• ëª¨ë‘ ìƒì„±
    for q_type in ['keyword', 'context', 'abstract']:
        result = generate_single_question(chunk, q_type)
        if result:
            results.append(result)
        else:
            # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ë‹¤ë¥¸ ìœ í˜•ì€ ì‹œë„)
            pass
    
    return results

def load_processed_chunks():
    """ì´ë¯¸ ì²˜ë¦¬ëœ ì²­í¬ ID ëª©ë¡ ë¡œë“œ (ì´ì–´í•˜ê¸° ê¸°ëŠ¥)"""
    processed_chunks = set()
    
    if OUTPUT_FILE.exists():
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        try:
                            data = json.loads(line)
                            processed_chunks.add(data['chunk_id'])
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            log_message(f"[Warning] Failed to load processed chunks: {e}")
    
    return processed_chunks

def save_questions(questions, stats):
    """ì§ˆë¬¸ë“¤ì„ JSONL íŒŒì¼ì— ì €ì¥ (Append ëª¨ë“œ)"""
    if not questions:
        return
    
    try:
        with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
            for item in questions:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        stats['total_saved'] += len(questions)
        # ëª¨ë“  ì§ˆë¬¸ì˜ íƒ€ì…ë³„ë¡œ ì¹´ìš´íŠ¸
        for q in questions:
            stats['by_type'][q['type']] = stats['by_type'].get(q['type'], 0) + 1
        
        # í†µê³„ íŒŒì¼ ì €ì¥
        with open(STATS_FILE, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        log_message(f"ğŸ’¾ Saved {len(questions)} questions (Total: {stats['total_saved']})")
    except Exception as e:
        log_message(f"[Error] Failed to save questions: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    log_message("=" * 70)
    log_message("ğŸ§ª TEST MODE: Balanced QA Dataset Generation (30 chunks only)")
    log_message("=" * 70)
    
    # 1. ì´ì–´í•˜ê¸° (Resume) ë¡œì§
    processed_chunks = load_processed_chunks()
    log_message(f"ğŸ“‚ Resume: {len(processed_chunks)} chunks already processed")
    
    # 2. ì²­í¬ ë¡œë“œ ë° í•„í„°ë§
    log_message(f"ğŸ“‚ Loading chunks from: {CHUNK_FILE}")
    with open(CHUNK_FILE, 'r', encoding='utf-8') as f:
        all_chunks = json.load(f)
    
    log_message(f"   âœ… Total chunks: {len(all_chunks)}")
    
    target_chunks = [c for c in all_chunks if c['chunk_id'] not in processed_chunks]
    
    # ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 30ê°œë¡œ ì œí•œ
    TEST_LIMIT = 30
    if len(target_chunks) > TEST_LIMIT:
        target_chunks = target_chunks[:TEST_LIMIT]
        log_message(f"   ğŸ§ª TEST MODE: Limited to {TEST_LIMIT} chunks")
    
    log_message(f"   ğŸ“Š Remaining chunks: {len(target_chunks)}")
    
    if not target_chunks:
        log_message("âœ… All chunks already processed!")
        return
    
    # 3. í†µê³„ ì´ˆê¸°í™”
    stats = {
        'started_at': datetime.now().isoformat(),
        'total_chunks': len(all_chunks),
        'processed_chunks': len(processed_chunks),
        'remaining_chunks': len(target_chunks),
        'total_saved': 0,
        'by_type': {'keyword': 0, 'context': 0, 'abstract': 0}
    }
    
    # 4. ì‹¤í–‰
    buffer = []
    completed_count = 0
    
    log_message(f"ğŸ”„ Starting generation (Workers: {MAX_WORKERS}, Save Interval: {SAVE_INTERVAL})")
    
    try:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {executor.submit(process_chunk, chunk): chunk for chunk in target_chunks}
            
            for future in tqdm(as_completed(futures), total=len(target_chunks), desc="   Progress"):
                chunk = futures[future]
                result_list = future.result()
                
                if result_list:
                    buffer.extend(result_list)
                    completed_count += 1
                
                # 10ê°œ ë‹¨ìœ„ ì €ì¥
                if len(buffer) >= SAVE_INTERVAL:
                    save_questions(buffer, stats)
                    buffer = []  # ë²„í¼ ë¹„ìš°ê¸°
                
                # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ë¡œê·¸
                if completed_count % 50 == 0:
                    log_message(f"ğŸ“Š Progress: {completed_count}/{len(target_chunks)} chunks processed, {stats['total_saved']} questions saved")
        
        # ë‚¨ì€ ë²„í¼ ì €ì¥
        if buffer:
            save_questions(buffer, stats)
        
        # ìµœì¢… í†µê³„
        stats['completed_at'] = datetime.now().isoformat()
        stats['final_saved'] = stats['total_saved']
        
        with open(STATS_FILE, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        log_message("=" * 70)
        log_message("âœ… Generation Completed!")
        log_message(f"   Total questions saved: {stats['total_saved']}")
        log_message(f"   By type: Keyword={stats['by_type'].get('keyword', 0)}, "
                   f"Context={stats['by_type'].get('context', 0)}, "
                   f"Abstract={stats['by_type'].get('abstract', 0)}")
        log_message("=" * 70)
        
    except KeyboardInterrupt:
        log_message("\nâš ï¸ Interrupted by user. Saving remaining buffer...")
        if buffer:
            save_questions(buffer, stats)
        log_message("ğŸ’¾ Progress saved. You can resume by running the script again.")
    except Exception as e:
        log_message(f"âŒ Fatal error: {e}")
        if buffer:
            save_questions(buffer, stats)
        raise

if __name__ == "__main__":
    main()

