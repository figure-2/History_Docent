import json
import time
import os
import random
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import requests

# -----------------------------------------------------------------------------
# ì„¤ì • (Configuration)
# -----------------------------------------------------------------------------
# API ì„¤ì •
ENV_FILE = Path("/home/pencilfoxs/00_new/.env2")
API_KEY = None

if ENV_FILE.exists():
    with open(ENV_FILE, 'r') as f:
        for line in f:
            # ê³µë°± í¬í•¨ ê°€ëŠ¥ì„± ê³ ë ¤
            if "GOOGLE_API_KEY_2" in line and "=" in line:
                API_KEY = line.split("=", 1)[1].strip()
                break

if not API_KEY:
    print("âš ï¸ Error: GOOGLE_API_KEY_2 not found in .env2")
    exit(1)

# Gemini 2.0 Flash (Experimental) - ì§€ì‹œ ì´í–‰ ëŠ¥ë ¥ì´ ì¢‹ê³  ë¹ ë¦„
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent"

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
CHUNK_FILE = BASE_DIR / "02_Chunking/output/all_chunks.json"
OUTPUT_DIR = BASE_DIR / "06_LLM_Evaluation"
OUTPUT_DIR.mkdir(exist_ok=True)

# ìƒì„± ì„¤ì •
SAMPLE_SIZE = 60   # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ìš© (ìœ í˜•ë³„ ì•½ 20ê°œ)
MAX_WORKERS = 10   # ë³‘ë ¬ ì²˜ë¦¬

# -----------------------------------------------------------------------------
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Prompt Templates)
# -----------------------------------------------------------------------------

# 1. í‚¤ì›Œë“œí˜• (BM25 ìœ ë¦¬) - 30%
PROMPT_KEYWORD = """
ë‹¹ì‹ ì€ 'í•œêµ­ì‚¬ ì‹œí—˜ì„ ì¤€ë¹„í•˜ëŠ” í•™ìƒ'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ **ì‚¬ì‹¤, ì—°ë„, ì¸ë¬¼, ì‚¬ê±´ëª…**ì„ í™•ì¸í•˜ëŠ” ë‹¨ë‹µí˜• ì§ˆë¬¸ì„ **í•˜ë‚˜ë§Œ** ë§Œë“œì„¸ìš”.

[ê·œì¹™]
1. í•µì‹¬ ê³ ìœ ëª…ì‚¬(í‚¤ì›Œë“œ)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì„œ ì§ˆë¬¸í•˜ì„¸ìš”.
2. ëª…í™•í•œ ì •ë‹µì´ ë‚˜ì˜¤ë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë³´ì„¸ìš”.
3. **ë°˜ë“œì‹œ ì§ˆë¬¸ í•˜ë‚˜ë§Œ** ì¶œë ¥í•˜ì„¸ìš”. (ë²ˆí˜¸ë‚˜ ì—¬ëŸ¬ ì§ˆë¬¸ ê¸ˆì§€)
4. ì˜ˆ: "ì´ì„±ê³„ê°€ ìœ„í™”ë„ íšŒêµ°ì„ ë‹¨í–‰í•œ ë…„ë„ëŠ” ì–¸ì œì¸ê°€ìš”?"

[í…ìŠ¤íŠ¸]
{text}

[ì§ˆë¬¸ ìƒì„± (ì§ˆë¬¸ í•˜ë‚˜ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥)]
"""

# 2. ë¬¸ë§¥/ìŠ¤í† ë¦¬í˜• (Hybrid ìœ ë¦¬) - 40%
PROMPT_CONTEXT = """
ë‹¹ì‹ ì€ 'ì—­ì‚¬ ì´ì•¼ê¸°ë¥¼ ë“£ëŠ” ê´€ëŒê°'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ **ì¸ê³¼ê´€ê³„, ì´ìœ , ë°°ê²½**ì— ëŒ€í•´ ë¬¼ì–´ë³´ëŠ” ì§ˆë¬¸ì„ **í•˜ë‚˜ë§Œ** ë§Œë“œì„¸ìš”.

[ê·œì¹™]
1. ë‹¨ìˆœ ì‚¬ì‹¤ë³´ë‹¤ëŠ” "ì™œ?", "ì–´ë–»ê²Œ?", "ê·¸ ê²°ê³¼ëŠ”?" ìœ„ì£¼ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”.
2. ë¬¸ë§¥ì„ ì´í•´í•´ì•¼ ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”.
3. **ë°˜ë“œì‹œ ì§ˆë¬¸ í•˜ë‚˜ë§Œ** ì¶œë ¥í•˜ì„¸ìš”. (ë²ˆí˜¸ë‚˜ ì—¬ëŸ¬ ì§ˆë¬¸ ê¸ˆì§€)
4. ì˜ˆ: "ì´ì„±ê³„ê°€ ìš”ë™ ì •ë²Œì„ ë°˜ëŒ€í•˜ê³  ê²°êµ­ íšŒêµ°í•˜ê²Œ ëœ ê²°ì •ì ì¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

[í…ìŠ¤íŠ¸]
{text}

[ì§ˆë¬¸ ìƒì„± (ì§ˆë¬¸ í•˜ë‚˜ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥)]
"""

# 3. ì¶”ìƒ/í’€ì–´ì“°ê¸°í˜• (Vector ìœ ë¦¬) - 30%
PROMPT_ABSTRACT = """
ë‹¹ì‹ ì€ 'ì—­ì‚¬ ìš©ì–´ê°€ ì˜ ê¸°ì–µë‚˜ì§€ ì•ŠëŠ” ì¼ë°˜ì¸'ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ë¬¼ì–´ë³´ë˜, **í•µì‹¬ ê³ ìœ ëª…ì‚¬ë¥¼ ì“°ì§€ ë§ê³  í’€ì–´ì„œ** ì§ˆë¬¸í•˜ì„¸ìš”.

[ê·œì¹™]
1. **ì ˆëŒ€ ë³¸ë¬¸ì˜ í•µì‹¬ ê³ ìœ ëª…ì‚¬(ì¸ë¬¼ëª…, ì‚¬ê±´ëª… ë“±)ë¥¼ ì§ì ‘ ì“°ì§€ ë§ˆì„¸ìš”.**
2. "ê·¸ê±° ìˆì–ì•„", "ê·¸ ì‚¬ëŒ", "ê·¸ ì‚¬ê±´" ì²˜ëŸ¼ ëŒ€ëª…ì‚¬ë‚˜ ë¬˜ì‚¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
3. ì˜ˆ: "ë¹„ ë§ì´ ì˜¨ë‹¤ê³  êµ°ëŒ€ ëŒë ¤ì„œ ì™• ì«“ì•„ë‚¸ ê·¸ ì‚¬ê±´ì´ ë­ì˜ˆìš”?" (ìœ„í™”ë„ íšŒêµ° ì–¸ê¸‰ X)

[í…ìŠ¤íŠ¸]
{text}

[ì§ˆë¬¸ ìƒì„± (ì§ˆë¬¸ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ)]
"""

def generate_question(chunk, index, q_type=None):
    """Gemini APIë¥¼ í˜¸ì¶œí•˜ì—¬ ìœ í˜•ë³„ ì§ˆë¬¸ ìƒì„±"""
    text = chunk['text']
    
    # q_typeì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ëœë¤ ì„ íƒ (ê°€ì¤‘ì¹˜: 30%, 40%, 30%)
    if q_type is None:
        q_type = random.choices(
            ['keyword', 'context', 'abstract'], 
            weights=[0.3, 0.4, 0.3],
            k=1
        )[0]
    
    # í”„ë¡¬í”„íŠ¸ ì„ íƒ
    if q_type == 'keyword':
        prompt_text = PROMPT_KEYWORD.format(text=text)
        temperature = 0.5
    elif q_type == 'context':
        prompt_text = PROMPT_CONTEXT.format(text=text)
        temperature = 0.6
    else: # abstract
        prompt_text = PROMPT_ABSTRACT.format(text=text)
        temperature = 0.8  # ì¶”ìƒí˜•ì€ ì°½ì˜ì„± í•„ìš”
    
    payload = {
        "contents": [{
            "parts": [{
                "text": prompt_text
            }]
        }],
        "generationConfig": {
            "temperature": temperature,
            "maxOutputTokens": 150,
        }
    }
    
    headers = {"Content-Type": "application/json"}
    
    try:
        time.sleep(random.uniform(0.3, 0.6)) # Rate Limit ë°©ì§€ (429 ì—ëŸ¬ ë°©ì§€)
        
        response = requests.post(
            f"{GEMINI_API_URL}?key={API_KEY}",
            json=payload,
            headers=headers,
            timeout=30
        )
        
        if response.status_code != 200:
            print(f"   âš ï¸ API Error (Status {response.status_code}) for chunk {chunk['chunk_id']}")
            return None
            
        result = response.json()
        
        if 'candidates' in result and result['candidates']:
            question = result['candidates'][0]['content']['parts'][0]['text'].strip()
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            if question.startswith("```"):
                question = question.replace("```json", "").replace("```", "").strip()
            
            # ì—¬ëŸ¬ ì§ˆë¬¸ì´ ìƒì„±ëœ ê²½ìš° ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ ì¶”ì¶œ
            # "1. ì§ˆë¬¸" ë˜ëŠ” "ì§ˆë¬¸\n2. ì§ˆë¬¸" í˜•ì‹ ì²˜ë¦¬
            lines = question.split('\n')
            first_question = lines[0]
            # "1. ", "2. " ê°™ì€ ë²ˆí˜¸ ì œê±°
            if first_question.strip().startswith(('1.', '2.', '3.', '4.', '5.')):
                first_question = first_question.split('.', 1)[1].strip()
            question = first_question.strip()
            
            return {
                "id": f"q_{index:05d}_{q_type}",
                "chunk_id": chunk['chunk_id'],
                "question": question,
                "type": q_type,
                "source_text": text,
                "source_metadata": chunk.get('metadata', {})
            }
        else:
            print(f"   âš ï¸ No candidates in response for chunk {chunk['chunk_id']}")
            return None
            
    except Exception as e:
        print(f"   âš ï¸ Exception for chunk {chunk['chunk_id']}: {e}")
        return None

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ê· í˜• ì¡íŒ QA ë°ì´í„°ì…‹ ìƒì„±")
    parser.add_argument("--mode", choices=["sample", "full"], default="sample",
                       help="sample: ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (60ê°œ), full: ì „ì²´ ì²­í¬ ëŒ€ìƒ (3,719ê°œ)")
    parser.add_argument("--output", type=str, default=None,
                       help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: balanced_qa_dataset_{mode}.json)")
    
    args = parser.parse_args()
    
    print(f"ğŸš€ Balanced QA Dataset Generation (API Key: ...{API_KEY[-4:]})")
    print(f"   - Mode: {args.mode}")
    print(f"   - Types: Keyword(30%), Context(40%), Abstract(30%)")
    
    # 1. ì²­í¬ ë¡œë“œ
    print(f"\nğŸ“‚ Loading chunks from: {CHUNK_FILE}")
    with open(CHUNK_FILE, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    
    print(f"   âœ… Total chunks: {len(chunks)}")
    
    # 2. ìƒ˜í”Œë§ ë˜ëŠ” ì „ì²´ ì„ íƒ
    if args.mode == "sample":
        selected_chunks = random.sample(chunks, min(SAMPLE_SIZE, len(chunks)))
        print(f"   ğŸ“Š Sample size: {len(selected_chunks)} chunks")
    else:  # full
        selected_chunks = chunks
        print(f"   ğŸ“Š Full mode: {len(selected_chunks)} chunks (1 question per chunk)")
    
    # 3. ì¶œë ¥ íŒŒì¼ëª… ê²°ì •
    if args.output:
        output_file = OUTPUT_DIR / args.output
    else:
        output_file = OUTPUT_DIR / f"balanced_qa_dataset_{args.mode}.json"
    
    dataset = []
    
    # 4. ë³‘ë ¬ ì²˜ë¦¬
    print(f"\nğŸ”„ Generating questions...")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(generate_question, chunk, i): i for i, chunk in enumerate(selected_chunks)}
        
        for future in tqdm(as_completed(futures), total=len(selected_chunks), desc="   Progress"):
            result = future.result()
            if result:
                dataset.append(result)
    
    # 5. ì €ì¥
    print(f"\nğŸ’¾ Saving to: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… Saved {len(dataset)} questions ({len(dataset)/len(selected_chunks)*100:.1f}% success rate)")
    
    # 6. ìœ í˜•ë³„ í†µê³„
    print("\n--- [Type Statistics] ---")
    type_counts = {}
    for item in dataset:
        q_type = item['type']
        type_counts[q_type] = type_counts.get(q_type, 0) + 1
    
    for q_type in ['keyword', 'context', 'abstract']:
        count = type_counts.get(q_type, 0)
        pct = (count / len(dataset) * 100) if dataset else 0
        print(f"   {q_type:10s}: {count:4d} ({pct:5.1f}%)")
    
    # 7. ìœ í˜•ë³„ ë¯¸ë¦¬ë³´ê¸° (ê²€ì¦)
    print("\n--- [Type-based Preview] ---")
    for q_type in ['keyword', 'context', 'abstract']:
        samples = [d for d in dataset if d['type'] == q_type][:2]
        if samples:
            print(f"\n[Type: {q_type.upper()}]")
            for s in samples:
                print(f"  Q: {s['question']}")
                print(f"  Context: {s['source_text'][:60]}...")
                print()

if __name__ == "__main__":
    main()

