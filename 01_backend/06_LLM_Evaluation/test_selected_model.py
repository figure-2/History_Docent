#!/usr/bin/env python3
"""
ì„ ì •ëœ LLM ëª¨ë¸ (Bllossom-8B) ì „ì²´ Validation Set í…ŒìŠ¤íŠ¸
- ëª©ì : ì„ ì •ëœ ëª¨ë¸ì˜ ì „ì²´ ì„±ëŠ¥ ê²€ì¦
- ë°ì´í„°ì…‹: validation_set_20.json (ì „ì²´ 2,223ê°œ)
- ê¸°ëŠ¥: ì¤‘ê°„ ì €ì¥, ì¬ê°œ ê¸°ëŠ¥, ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì§€ì›
"""

import json
import time
import torch
import gc
import pandas as pd
import os
import sys
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Set
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
from konlpy.tag import Okt
import numpy as np
import chromadb
from datetime import datetime

# -----------------------------------------------------------------------------
# ì„¤ì •
# -----------------------------------------------------------------------------
BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
BENCHMARK_DATA = BASE_DIR / "03_Embedding/data/validation_set_20.json"
CHUNK_FILE = BASE_DIR / "02_Chunking/output/all_chunks.json"
VECTORDB_DIR = BASE_DIR / "04_VectorDB/chroma_db"
COLLECTION_NAME = "korean_history_chunks"
RESULTS_DIR = BASE_DIR / "06_LLM_Evaluation/results"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# ì„ ì •ëœ ëª¨ë¸
SELECTED_MODEL = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
EMBEDDING_MODEL = "BAAI/bge-m3"
RERANKER_MODEL = "Dongjin-kr/ko-reranker"

# ì¤‘ê°„ ì €ì¥ ì„¤ì •
CHECKPOINT_INTERVAL = 50  # 50ê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
SAVE_PATH = RESULTS_DIR / "llm_selected_model_full_test.csv"
LOG_PATH = RESULTS_DIR / "full_test_progress.log"
PROGRESS_PATH = RESULTS_DIR / "full_test_progress.json"

# -----------------------------------------------------------------------------
# RAG í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# -----------------------------------------------------------------------------
class BM25Retriever:
    def __init__(self, chunk_dict: Dict):
        self.chunk_dict = chunk_dict
        self.chunk_ids = list(chunk_dict.keys())
        self.tokenizer = Okt()
        print("   ğŸ§® BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        texts = []
        for chunk_id in tqdm(self.chunk_ids, desc="   BM25 í† í¬ë‚˜ì´ì§•", leave=False):
            text = chunk_dict[chunk_id]['text']
            tokens = [t for t in self.tokenizer.morphs(text, stem=True) if t.strip()]
            texts.append(tokens)
        self.bm25 = BM25Okapi(texts)
    
    def search(self, query: str, top_k: int) -> List[Dict]:
        query_tokens = [t for t in self.tokenizer.morphs(query, stem=True) if t.strip()]
        scores = self.bm25.get_scores(query_tokens)
        top_indices = np.argsort(scores)[::-1][:top_k]
        return [{'chunk_id': self.chunk_ids[i], 'text': self.chunk_dict[self.chunk_ids[i]]['text'], 'score': float(scores[i])} for i in top_indices]

class VectorRetriever:
    def __init__(self, collection, model_name):
        self.collection = collection
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = SentenceTransformer(model_name, device=self.device)
        
    def search(self, query: str, top_k: int) -> List[Dict]:
        query_emb = self.model.encode(query, normalize_embeddings=True, show_progress_bar=False).tolist()
        results = self.collection.query(query_embeddings=[query_emb], n_results=top_k)
        ret = []
        if results['ids'] and len(results['ids'][0]) > 0:
            for i, chunk_id in enumerate(results['ids'][0]):
                ret.append({
                    'chunk_id': chunk_id,
                    'text': results['documents'][0][i],
                    'score': 1.0 - results['distances'][0][i]
                })
        return ret

class HybridRetriever:
    def __init__(self, bm25, vector):
        self.bm25 = bm25
        self.vector = vector
    
    def search_weighted(self, query: str, top_k: int, v_weight: float = 0.6, b_weight: float = 0.4) -> List[Dict]:
        bm25_res = self.bm25.search(query, top_k * 2)
        vector_res = self.vector.search(query, top_k * 2)
        
        scores = {}
        for res, weight in [(bm25_res, b_weight), (vector_res, v_weight)]:
            if not res: continue
            max_s = max(r['score'] for r in res)
            min_s = min(r['score'] for r in res)
            denom = max_s - min_s if max_s != min_s else 1.0
            for r in res:
                norm = (r['score'] - min_s) / denom
                scores[r['chunk_id']] = scores.get(r['chunk_id'], 0) + weight * norm
                
        sorted_ids = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        return [{'chunk_id': cid, 'text': self.bm25.chunk_dict[cid]['text'], 'score': sc} for cid, sc in sorted_ids]

# -----------------------------------------------------------------------------
# RAG íŒŒì´í”„ë¼ì¸
# -----------------------------------------------------------------------------
def setup_rag_pipeline():
    print("ğŸ› ï¸ RAG íŒŒì´í”„ë¼ì¸(Retriever + Reranker) ì´ˆê¸°í™” ì¤‘...")
    with open(CHUNK_FILE, 'r') as f: chunks = json.load(f)
    chunk_dict = {c['chunk_id']: c for c in chunks}
    
    bm25 = BM25Retriever(chunk_dict)
    client = chromadb.PersistentClient(path=str(VECTORDB_DIR))
    vector = VectorRetriever(client.get_collection(COLLECTION_NAME), EMBEDDING_MODEL)
    hybrid = HybridRetriever(bm25, vector)
    
    reranker = CrossEncoder(RERANKER_MODEL, device="cuda", automodel_args={"torch_dtype": torch.float16})
    
    return hybrid, reranker

def get_rag_context(query, hybrid, reranker, top_k=3):
    candidates = hybrid.search_weighted(query, top_k=50)
    if not candidates: return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    pairs = [[query, doc['text']] for doc in candidates]
    scores = reranker.predict(pairs)
    for i, doc in enumerate(candidates):
        doc['rerank_score'] = float(scores[i])
    
    reranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)[:top_k]
    return "\n\n".join([f"ë¬¸ì„œ {i+1}: {doc['text']}" for i, doc in enumerate(reranked)])

def get_prompt(query, context):
    return f"""ë‹¹ì‹ ì€ í•œêµ­ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ [ì§ˆë¬¸]ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³ , ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ë¶€ì¡±í•˜ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{query}

[ë‹µë³€]
"""

def clear_gpu():
    gc.collect()
    torch.cuda.empty_cache()

def log_message(message: str, log_file: Path = LOG_PATH):
    """ë¡œê·¸ ë©”ì‹œì§€ë¥¼ íŒŒì¼ê³¼ stdoutì— ì¶œë ¥"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_line = f"[{timestamp}] {message}\n"
    
    # íŒŒì¼ì— ë¡œê·¸ ê¸°ë¡
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(log_line)
    
    # stdoutì—ë„ ì¶œë ¥ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‹œ nohup.outì— ê¸°ë¡ë¨)
    print(message, flush=True)

def load_progress() -> Dict:
    """ì§„í–‰ ìƒí™© ë¡œë“œ"""
    if PROGRESS_PATH.exists():
        try:
            with open(PROGRESS_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {"completed_ids": [], "last_checkpoint": None}
    return {"completed_ids": [], "last_checkpoint": None}

def save_progress(completed_ids: Set[str], checkpoint_time: str):
    """ì§„í–‰ ìƒí™© ì €ì¥"""
    progress = {
        "completed_ids": list(completed_ids),
        "last_checkpoint": checkpoint_time,
        "total_completed": len(completed_ids)
    }
    with open(PROGRESS_PATH, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)

def load_existing_results() -> pd.DataFrame:
    """ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ"""
    if SAVE_PATH.exists():
        try:
            df = pd.read_csv(SAVE_PATH)
            log_message(f"ğŸ“‚ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ: {len(df)}ê°œ ì‘ë‹µ")
            return df
        except Exception as e:
            log_message(f"âš ï¸ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    return pd.DataFrame()

def save_results(results: List[Dict], append: bool = False):
    """ê²°ê³¼ ì €ì¥ (ì¤‘ê°„ ì €ì¥ ì§€ì›)"""
    df = pd.DataFrame(results)
    if append and SAVE_PATH.exists():
        # ê¸°ì¡´ ê²°ê³¼ì™€ ë³‘í•©
        existing_df = load_existing_results()
        if len(existing_df) > 0:
            # ì¤‘ë³µ ì œê±° (query_id ê¸°ì¤€)
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            combined_df = combined_df.drop_duplicates(subset=['query_id'], keep='last')
            df = combined_df
    
    df.to_csv(SAVE_PATH, index=False)
    log_message(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ ì‘ë‹µ (íŒŒì¼: {SAVE_PATH})")

# -----------------------------------------------------------------------------
# ë©”ì¸ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def main():
    print("=" * 80)
    print(f"ğŸš€ ì„ ì •ëœ ëª¨ë¸ ì „ì²´ Validation Set í…ŒìŠ¤íŠ¸: {SELECTED_MODEL}")
    print("=" * 80)
    
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    with open(BENCHMARK_DATA, 'r') as f: dataset = json.load(f)
    print(f"âœ… ì´ {len(dataset)}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")
    
    # RAG íŒŒì´í”„ë¼ì¸ ì„¤ì •
    hybrid, reranker = setup_rag_pipeline()
    
    # RAG Context ìƒì„±
    print("\nâš™ï¸  RAG Context ìƒì„± ì¤‘...")
    for item in tqdm(dataset, desc="Context ìƒì„±"):
        item['rag_context'] = get_rag_context(item['query'], hybrid, reranker)
    
    del hybrid, reranker
    clear_gpu()
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {SELECTED_MODEL}")
    hf_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")
    tokenizer = AutoTokenizer.from_pretrained(SELECTED_MODEL, trust_remote_code=True, token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        SELECTED_MODEL, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True, token=hf_token
    )
    
    # ì¶”ë¡  ì‹¤í–‰
    print(f"\nğŸš€ ì¶”ë¡  ì‹œì‘ (ì´ {len(dataset)}ê°œ ì§ˆë¬¸)...")
    results = []
    
    for item in tqdm(dataset, desc="Generating"):
        prompt = get_prompt(item['query'], item['rag_context'])
        start_time = time.time()
        
        try:
            messages = [{"role": "user", "content": prompt}]
            inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")
        except:
            inputs = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")
        
        outputs = model.generate(
            inputs, max_new_tokens=512, temperature=0.1, do_sample=True,
            pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id
        )
        response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
        
        results.append({
            "model": SELECTED_MODEL,
            "query_id": item.get('id', f"q_{len(results)}"),
            "query": item['query'],
            "response": response.strip(),
            "latency": time.time() - start_time,
            "type": item['type'],
            "chunk_id": item.get('chunk_id', ''),
            "gold_text": item.get('gold_text', '')
        })
    
    # ê²°ê³¼ ì €ì¥
    df = pd.DataFrame(results)
    save_path = RESULTS_DIR / "llm_selected_model_full_test.csv"
    df.to_csv(save_path, index=False)
    
    # í†µê³„ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"ì´ ì§ˆë¬¸ ìˆ˜: {len(results)}ê°œ")
    print(f"í‰ê·  ì§€ì—°ì‹œê°„: {df['latency'].mean():.2f}ì´ˆ")
    print(f"ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬:")
    print(df['type'].value_counts())
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {save_path}")
    
    del model, tokenizer
    clear_gpu()

if __name__ == "__main__":
    main()
