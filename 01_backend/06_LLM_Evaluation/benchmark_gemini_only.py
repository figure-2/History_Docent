#!/usr/bin/env python3
"""
Gemini APIë§Œ ì‹¤í–‰í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import time
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import google.generativeai as genai
from dotenv import load_dotenv
import os

BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
BENCHMARK_DATA = BASE_DIR / "03_Embedding/data/validation_set_20.json"
RESULTS_DIR = BASE_DIR / "06_LLM_Evaluation/results"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

SAMPLE_SIZE = 50

def load_env():
    load_dotenv("/home/pencilfoxs/00_new/.env2")
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

def get_prompt(query, context):
    return f"""ë‹¹ì‹ ì€ í•œêµ­ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ [ì§ˆë¬¸]ì— ëŒ€í•´ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³ , ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ë¶€ì¡±í•˜ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{query}

[ë‹µë³€]
"""

def load_existing_responses():
    """ê¸°ì¡´ì— ìƒì„±ëœ RAG Contextê°€ ìˆëŠ”ì§€ í™•ì¸"""
    existing_file = RESULTS_DIR / "llm_benchmark_responses.csv"
    if existing_file.exists():
        df = pd.read_csv(existing_file)
        # ì´ë¯¸ RAG contextê°€ ìˆëŠ” ë°ì´í„° ì°¾ê¸°
        if 'rag_context' in df.columns or any('rag_context' in str(v) for v in df.values):
            print("âœ… ê¸°ì¡´ RAG Context ë°œê²¬")
            return df
    return None

def get_rag_context_from_existing(query_id, existing_df):
    """ê¸°ì¡´ ê²°ê³¼ì—ì„œ ê°™ì€ query_idì˜ rag_context ì°¾ê¸°"""
    if existing_df is not None:
        # query_idë¡œ ë§¤ì¹­ ì‹œë„
        matches = existing_df[existing_df['query_id'] == query_id]
        if len(matches) > 0:
            # ê°™ì€ ëª¨ë¸ì˜ rag_contextê°€ ìˆë‹¤ë©´ ì‚¬ìš©
            return matches.iloc[0].get('rag_context', None)
    return None

def generate_gemini(model_name, dataset, existing_df=None):
    print(f"ğŸŒ Gemini API í˜¸ì¶œ ì¤‘: {model_name}")
    results = []
    
    # ëª¨ë¸ëª… ì‹œë„ (2.5 -> 2.0 -> 1.5 ìˆœì„œ)
    model_names_to_try = ["gemini-2.0-flash-exp", "gemini-1.5-flash"]
    if "2.5" in model_name.lower():
        model_names_to_try.insert(0, "gemini-2.5-flash")
    
    model = None
    actual_model_name = None
    
    for try_name in model_names_to_try:
        try:
            model = genai.GenerativeModel(try_name)
            # í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
            model.generate_content("test")
            actual_model_name = try_name
            print(f"âœ… {try_name} ì‚¬ìš© ê°€ëŠ¥")
            break
        except Exception as e:
            print(f"âš ï¸ {try_name} ì‹¤íŒ¨: {e}")
            continue
    
    if model is None:
        print("âŒ ëª¨ë“  Gemini ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨")
        return []

    for item in tqdm(dataset, desc=f"   Generating ({actual_model_name})"):
        # RAG Context ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ê²°ê³¼ì—ì„œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±)
        rag_context = item.get('rag_context', '')
        if not rag_context and existing_df is not None:
            rag_context = get_rag_context_from_existing(item['id'], existing_df)
        
        if not rag_context:
            rag_context = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        prompt = get_prompt(item['query'], rag_context)
        
        start_time = time.time()
        try:
            response = model.generate_content(prompt)
            text = response.text.strip()
        except Exception as e:
            text = f"Error: {e}"
            print(f"   âš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")
        
        end_time = time.time()
        
        results.append({
            "model": actual_model_name,
            "query_id": item['id'],
            "query": item['query'],
            "response": text,
            "latency": end_time - start_time,
            "type": item['type'],
            "rag_context": rag_context[:200] + "..." if len(rag_context) > 200 else rag_context
        })
        time.sleep(0.5)  # Rate limit ë°©ì§€
        
    return results

def main():
    load_env()
    
    print("ğŸ“‚ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
    with open(BENCHMARK_DATA, 'r') as f: full_data = json.load(f)
    
    # ê¸°ì¡´ ê²°ê³¼ í™•ì¸
    existing_df = load_existing_responses()
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„ (ê¸°ì¡´ì— RAG contextê°€ ìˆë‹¤ë©´ ì‚¬ìš©)
    test_data = []
    counts = {"keyword": 0, "context": 0, "abstract": 0}
    target = SAMPLE_SIZE // 3
    
    for item in full_data:
        q_type = item['type']
        if counts[q_type] < target + (1 if q_type == 'abstract' and SAMPLE_SIZE % 3 != 0 else 0):
            item['id'] = f"bench_{len(test_data)}"
            
            # ê¸°ì¡´ ê²°ê³¼ì—ì„œ rag_context ì°¾ê¸°
            if existing_df is not None:
                existing_context = get_rag_context_from_existing(item['id'], existing_df)
                if existing_context:
                    item['rag_context'] = existing_context
            
            test_data.append(item)
            counts[q_type] += 1
        if len(test_data) >= SAMPLE_SIZE:
            break
    
    # RAG Contextê°€ ì—†ëŠ” ê²½ìš°, ê°„ë‹¨íˆ gold_text ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
    for item in test_data:
        if 'rag_context' not in item or not item['rag_context']:
            item['rag_context'] = item.get('gold_text', 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    
    print(f"âœ… {len(test_data)}ê°œ ìƒ˜í”Œ ì¤€ë¹„ ì™„ë£Œ")
    
    # Gemini ì‹¤í–‰
    results = generate_gemini("gemini-2.5-flash", test_data, existing_df)
    
    if results:
        # ê¸°ì¡´ ê²°ê³¼ì™€ ë³‘í•©
        if existing_df is not None:
            df_new = pd.DataFrame(results)
            df_combined = pd.concat([existing_df, df_new], ignore_index=True)
            df_combined.to_csv(RESULTS_DIR / "llm_benchmark_responses.csv", index=False)
            print(f"ğŸ’¾ ê¸°ì¡´ ê²°ê³¼ì— ì¶”ê°€ ì €ì¥ ì™„ë£Œ (ì´ {len(df_combined)}ê°œ)")
        else:
            df = pd.DataFrame(results)
            df.to_csv(RESULTS_DIR / "llm_benchmark_responses.csv", index=False)
            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ")
    else:
        print("âŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

