# GCP 인스턴스 강제 종료 원인 분석 보고서

**작성 일시:** 2025-11-23 14:50

---

## 1. 사건 개요

### 1.1 발생 시점
- **마지막 재부팅:** 2025-11-23 14:10 (현재 시간 기준 35분 전)
- **이전 재부팅:** 2025-11-23 06:55-06:58 (약 7시간 전)
- **작업 내용:** 전체 Validation Set 테스트 실행 중

### 1.2 증상
- GCP 인스턴스가 강제 종료됨
- SSH 연결 끊김
- 진행 중이던 작업 중단

---

## 2. 원인 분석

### 2.1 주요 원인: 메모리 부족 (OOM - Out of Memory)

#### 증거 1: CUDA OOM 에러
```
❌ Error: CUDA out of memory. Tried to allocate 1.45 GiB. 
GPU 0 has a total capacity of 39.39 GiB of which 537.25 MiB is free. 
Process 206315 has 29.35 GiB memory in use. 
Process 311348 has 2.54 GiB memory in use.
```

**분석:**
- Qwen-14B 모델 실행 중 GPU 메모리 부족 발생
- GPU 메모리 사용량: 29.35 GiB + 2.54 GiB = 31.89 GiB
- 여유 메모리: 537.25 MiB (약 0.5GB)
- 추가 할당 시도: 1.45 GiB → 실패

#### 증거 2: Java 프로세스 크래시 (hs_err_pid55226.log)
```
Memory: 4k page, physical 87518944k(1594644k free), swap 0k(0 free)
```

**분석:**
- 시스템 메모리: 87.5GB 총량
- 여유 메모리: 1.59GB (약 1.8%)
- **Swap 메모리: 0 (스왑 없음)**
- Qwen-32B 모델 실행 중 Java 프로세스 크래시

#### 증거 3: 시스템 재부팅 이력
```
reboot   system boot  6.8.0-1043-gcp   Sun Nov 23 14:10   still running
reboot   system boot  6.8.0-1043-gcp   Sun Nov 23 06:58 - 14:10  (07:11)
```

**분석:**
- 11월 23일 하루에만 여러 번 재부팅 발생
- 패턴: 메모리 부족 → OOM Killer → 시스템 재부팅

---

## 3. 상세 원인 분석

### 3.1 메모리 사용 패턴

**GPU 메모리:**
- Qwen-14B 모델: 약 29.35 GiB
- 기타 프로세스: 약 2.54 GiB
- 총 사용량: 약 31.89 GiB / 40 GiB (79.7%)
- 여유: 약 0.5 GiB (1.3%)

**시스템 메모리:**
- 총 메모리: 87.5 GB
- 사용 중: 약 85.9 GB
- 여유: 약 1.6 GB (1.8%)
- **Swap: 0 (스왑 없음)**

### 3.2 문제점

1. **Swap 메모리 부재**
   - Swap이 0으로 설정되어 있어 메모리 부족 시 대체 공간 없음
   - OOM Killer가 프로세스를 강제 종료하거나 시스템 재부팅 유발

2. **여러 모델 동시 실행**
   - Qwen-14B와 Qwen-32B 모델이 동시에 메모리를 사용
   - GPU 메모리와 시스템 메모리 모두 부족

3. **메모리 정리 부족**
   - 이전 모델 실행 후 메모리 정리가 완전히 되지 않음
   - 메모리 누수 가능성

4. **대용량 모델 실행**
   - Qwen-32B 모델은 매우 큰 메모리 요구량
   - 40GB GPU로는 부족할 수 있음

---

## 4. 근본 원인

### 4.1 직접 원인
- **메모리 부족 (OOM)**: GPU 메모리와 시스템 메모리 모두 부족
- **Swap 부재**: 메모리 부족 시 대체 공간 없음
- **여러 프로세스 동시 실행**: 메모리 경쟁 발생

### 4.2 간접 원인
- **메모리 관리 부족**: 이전 모델 실행 후 완전한 메모리 정리 미흡
- **대용량 모델 실행**: Qwen-32B 같은 대용량 모델 실행 시도
- **모니터링 부족**: 메모리 사용량 실시간 모니터링 없음

---

## 5. 해결 방안

### 5.1 즉시 조치

#### 1. Swap 메모리 활성화
```bash
# Swap 파일 생성 (예: 32GB)
sudo fallocate -l 32G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 영구적으로 활성화
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

**효과:**
- 메모리 부족 시 Swap으로 대체
- OOM Killer 발생 가능성 감소

#### 2. 메모리 사용량 모니터링
```bash
# 실시간 모니터링 스크립트
watch -n 5 'free -h && nvidia-smi --query-gpu=memory.used,memory.total --format=csv'
```

#### 3. 프로세스 순차 실행 강화
- 한 번에 하나의 모델만 실행
- 각 모델 실행 후 완전한 메모리 정리 확인
- GPU 메모리 사용량 확인 후 다음 모델 실행

### 5.2 코드 개선

#### 1. 메모리 정리 강화
```python
def clear_gpu():
    gc.collect()
    torch.cuda.empty_cache()
    # 추가: Python 가비지 컬렉션 강제 실행
    import gc
    gc.collect()
    # 추가: CUDA 컨텍스트 정리
    torch.cuda.synchronize()
    torch.cuda.empty_cache()
```

#### 2. 메모리 사용량 체크
```python
def check_memory():
    """메모리 사용량 확인"""
    import torch
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_used = torch.cuda.memory_allocated(0) / 1024**3
        gpu_free = gpu_memory - gpu_used
        
        if gpu_free < 5:  # 5GB 미만이면 경고
            log_message(f"⚠️ GPU 메모리 부족: {gpu_free:.2f}GB 여유")
            return False
    return True
```

#### 3. 배치 처리
- 전체 데이터를 작은 배치로 나누어 처리
- 각 배치 완료 후 메모리 정리
- 중간 저장으로 진행 상황 보존

### 5.3 하드웨어 개선 (장기)

1. **더 큰 GPU 메모리**
   - A100 80GB 또는 H100 사용 고려
   - 또는 여러 GPU로 분산 처리

2. **시스템 메모리 증가**
   - 현재 87.5GB → 128GB 이상 권장

3. **Swap 메모리 설정**
   - 최소 32GB Swap 권장
   - SSD 사용 시 성능 저하 최소화

---

## 6. 재발 방지 대책

### 6.1 실행 전 체크리스트

- [ ] GPU 메모리 여유 확인 (최소 10GB 이상)
- [ ] 시스템 메모리 여유 확인 (최소 10GB 이상)
- [ ] Swap 메모리 활성화 확인
- [ ] 이전 프로세스 완전 종료 확인
- [ ] 메모리 모니터링 시작

### 6.2 실행 중 모니터링

- [ ] GPU 메모리 사용량 실시간 모니터링
- [ ] 시스템 메모리 사용량 확인
- [ ] 프로세스별 메모리 사용량 확인
- [ ] 메모리 부족 경고 설정

### 6.3 실행 후 정리

- [ ] 모델 언로드 확인
- [ ] GPU 메모리 정리 확인
- [ ] 시스템 메모리 정리 확인
- [ ] 다음 모델 실행 전 대기 (메모리 안정화)

---

## 7. 권장 사항

### 7.1 즉시 실행 가능

1. **Swap 메모리 활성화** (최우선)
   - 메모리 부족 시 즉시 대응 가능
   - 시스템 안정성 향상

2. **메모리 모니터링 스크립트 작성**
   - 실시간 메모리 사용량 확인
   - 경고 알림 설정

3. **코드 개선**
   - 메모리 정리 강화
   - 메모리 사용량 체크 추가

### 7.2 중기 개선

1. **배치 처리 구현**
   - 전체 데이터를 작은 배치로 분할
   - 각 배치 완료 후 메모리 정리

2. **프로세스 관리 개선**
   - 한 번에 하나의 모델만 실행
   - 메모리 사용량 확인 후 다음 모델 실행

### 7.3 장기 개선

1. **하드웨어 업그레이드**
   - 더 큰 GPU 메모리
   - 더 큰 시스템 메모리

2. **분산 처리**
   - 여러 GPU로 분산 처리
   - 모델 병렬화

---

## 8. 결론

### 8.1 종료 원인

**주요 원인:** 메모리 부족 (OOM - Out of Memory)
- GPU 메모리 부족 (31.89 GiB / 40 GiB 사용)
- 시스템 메모리 부족 (85.9 GB / 87.5 GB 사용)
- Swap 메모리 부재 (0)

**결과:**
- OOM Killer가 프로세스 강제 종료
- 시스템 재부팅 발생

### 8.2 즉시 조치 필요

1. **Swap 메모리 활성화** (최우선)
2. **메모리 모니터링 시스템 구축**
3. **코드 개선 (메모리 정리 강화)**

### 8.3 예상 효과

- Swap 활성화 후 메모리 부족 시 대체 공간 제공
- OOM Killer 발생 가능성 대폭 감소
- 시스템 안정성 향상

---

**작성자:** AI Assistant  
**작성 일시:** 2025-11-23 14:50  
**상태:** ✅ 원인 분석 완료, 해결 방안 제시

