#!/usr/bin/env python3
"""
ê· í˜•ì¡íŒ QA ë°ì´í„°ì…‹ìœ¼ë¡œ ì„ë² ë”© ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í‰ê°€
- ìƒˆë¡œìš´ ë°ì´í„°ì…‹(11,140ê°œ)ìœ¼ë¡œ 7ê°œ ì„ë² ë”© ëª¨ë¸ ì¬í‰ê°€
- ì§ˆë¬¸ ìœ í˜•ë³„(Keyword, Context, Abstract) ì„±ëŠ¥ ë¶„ì„ í¬í•¨
"""

import time
import json
import torch
import numpy as np
import pandas as pd
import requests
import os
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict

# -----------------------------------------------------------------------------
# ì„¤ì • (Configuration)
# -----------------------------------------------------------------------------
DATA_DIR = Path("/home/pencilfoxs/00_new/History_Docent/03_Embedding/data")
# âœ… Validation Set ì‚¬ìš© (ê³¼ì í•© ë°©ì§€ - ëª¨ë¸ ì„ ì • ë‹¨ê³„)
# âš ï¸ Test Setì€ ìµœì¢… í‰ê°€ì—ë§Œ ì‚¬ìš©!
BENCHMARK_FILE = DATA_DIR / "validation_set_20.json"
RESULTS_DIR = Path("/home/pencilfoxs/00_new/History_Docent/03_Embedding/results")
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# API í‚¤ ë¡œë“œ
ENV_FILE = Path("/home/pencilfoxs/00_new/.env2")
if ENV_FILE.exists():
    with open(ENV_FILE, 'r') as f:
        for line in f:
            if line.startswith("GOOGLE_API_KEY="):
                os.environ["GOOGLE_API_KEY"] = line.split("=", 1)[1].strip()

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# âœ… í‰ê°€í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (7ê°œ í›„ë³´)
MODELS = {
    # 1. Multilingual ê°•ì
    "BGE-m3": "BAAI/bge-m3",
    
    # 2. ìµœì‹  SOTA (Instruction ê¸°ë°˜)
    "Jina-v3": "jinaai/jina-embeddings-v3",
    
    # 3. ì„±ëŠ¥ ì¢‹ì€ ëŒ€í˜• ëª¨ë¸
    "GTE-large": "Alibaba-NLP/gte-large-en-v1.5",
    
    # 4. ê¾¸ì¤€íˆ ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸
    "E5-large": "intfloat/multilingual-e5-large",
    
    # 5. í•œêµ­ì–´ íŠ¹í™” (Baseline)
    "Ko-SBERT": "jhgan/ko-sbert-nli", 
    
    # 6. Google Open Source (Gemma ê¸°ë°˜)
    "EmbeddingGemma": "google/embedding-gemma-2b-en", 
    
    # 7. Google API (ìµœì‹ )
    "Gemini-API": "models/text-embedding-004" 
}

# -----------------------------------------------------------------------------
# í‰ê°€ í•¨ìˆ˜ (Evaluation Functions)
# -----------------------------------------------------------------------------

def load_benchmark_data():
    """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ"""
    if not BENCHMARK_FILE.exists():
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {BENCHMARK_FILE}")
    with open(BENCHMARK_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

def get_gemini_embeddings(texts, model_name="models/text-embedding-004"):
    """Gemini REST APIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±"""
    url = f"https://generativelanguage.googleapis.com/v1beta/{model_name}:batchEmbedContents?key={GOOGLE_API_KEY}"
    headers = {"Content-Type": "application/json"}
    
    all_embeddings = []
    batch_size = 50  # API í•œë„ ê³ ë ¤
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        payload = {
            "requests": [{"model": model_name, "content": {"parts": [{"text": text}]}} for text in batch_texts]
        }
        
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            if response.status_code != 200:
                print(f"Error API: {response.text}")
                all_embeddings.extend([np.zeros(768) for _ in batch_texts])
                continue
                
            result = response.json()
            embeddings = [e['values'] for e in result['embeddings']]
            all_embeddings.extend(embeddings)
            time.sleep(0.5)  # Rate limit ë°©ì§€
        except Exception as e:
            print(f"Gemini API Error: {e}")
            all_embeddings.extend([np.zeros(768) for _ in batch_texts])
            
    return np.array(all_embeddings)

def get_embeddings(model_name, model_path, texts, device="cuda" if torch.cuda.is_available() else "cpu"):
    """ëª¨ë¸ë³„ ì„ë² ë”© ìƒì„± í•¨ìˆ˜"""
    print(f"   Creating embeddings for {len(texts)} texts with {model_name}...")
    start_time = time.time()
    
    # 1. Google Gemini API
    if "Gemini-API" in model_name:
        embeddings = get_gemini_embeddings(texts, model_path)
    
    # 2. Jina (Trust Remote Code í•„ìš”)
    elif "Jina" in model_name:
        model = SentenceTransformer(model_path, trust_remote_code=True, device=device)
        embeddings = model.encode(texts, normalize_embeddings=True, batch_size=8)
        
    # 3. GTE (Trust Remote Code í•„ìš”)
    elif "GTE" in model_name:
        model = SentenceTransformer(model_path, trust_remote_code=True, device=device)
        embeddings = model.encode(texts, normalize_embeddings=True)
    
    # 4. E5 (Prefix í•„ìš”)
    elif "E5" in model_name:
        model = SentenceTransformer(model_path, device=device)
        embeddings = model.encode(texts, normalize_embeddings=True)
    
    # 5. Embedding Gemma
    elif "Gemma" in model_name:
        try:
            model = SentenceTransformer(model_path, trust_remote_code=True, device=device)
            embeddings = model.encode(texts, normalize_embeddings=True, batch_size=4)
        except Exception as e:
            print(f"Gemma ë¡œë“œ ì‹¤íŒ¨: {e}")
            return np.zeros((len(texts), 768)), 0
    
    # 6. ì¼ë°˜ ëª¨ë¸ (BGE, Ko-SBERT ë“±)
    else:
        model = SentenceTransformer(model_path, device=device)
        embeddings = model.encode(texts, normalize_embeddings=True)
        
    elapsed = time.time() - start_time
    speed = elapsed / len(texts) * 1000  # ms per text
    return np.array(embeddings), speed

def evaluate_model(model_name, model_path, dataset, device="cuda" if torch.cuda.is_available() else "cpu"):
    """ëª¨ë¸ í‰ê°€ í•¨ìˆ˜"""
    queries = [item['query'] for item in dataset]
    golds = [item['gold_text'] for item in dataset]
    types = [item.get('type', 'unknown') for item in dataset]
    
    # E5 ëª¨ë¸ì¼ ê²½ìš° Prefix ì¶”ê°€ ì²˜ë¦¬
    if "E5" in model_name:
        q_texts = [f"query: {q}" for q in queries]
        c_texts = [f"passage: {g}" for g in golds]
    else:
        q_texts = queries
        c_texts = golds
    
    # 1. ì„ë² ë”© ìƒì„±
    print(f"   ğŸ“ ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì¤‘...")
    q_embs, q_speed = get_embeddings(model_name, model_path, q_texts, device)
    print(f"   ğŸ“ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
    c_embs, c_speed = get_embeddings(model_name, model_path, c_texts, device)
    
    # 2. ìœ ì‚¬ë„ ê³„ì‚° (ì§ˆë¬¸-ë¬¸ì„œ ë§¤íŠ¸ë¦­ìŠ¤)
    print(f"   ğŸ” ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
    similarities = cosine_similarity(q_embs, c_embs)
    
    # 3. ì§€í‘œ ê³„ì‚° (ì „ì²´)
    mrr_sum = 0
    hits_1 = 0
    hits_3 = 0
    hits_5 = 0
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ ì§€í‘œ
    type_metrics = defaultdict(lambda: {'mrr_sum': 0, 'hits_1': 0, 'hits_3': 0, 'hits_5': 0, 'count': 0})
    
    n = len(queries)
    for i in range(n):
        target_idx = i  # ië²ˆì§¸ ì§ˆë¬¸ì˜ ì •ë‹µì€ ië²ˆì§¸ ë¬¸ì„œ
        
        # ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        sorted_indices = np.argsort(similarities[i])[::-1]
        
        # ìˆœìœ„ (1-based)
        rank = np.where(sorted_indices == target_idx)[0][0] + 1
        
        mrr_sum += 1.0 / rank
        if rank <= 1: hits_1 += 1
        if rank <= 3: hits_3 += 1
        if rank <= 5: hits_5 += 1
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ì§‘ê³„
        q_type = types[i]
        type_metrics[q_type]['mrr_sum'] += 1.0 / rank
        type_metrics[q_type]['count'] += 1
        if rank <= 1: type_metrics[q_type]['hits_1'] += 1
        if rank <= 3: type_metrics[q_type]['hits_3'] += 1
        if rank <= 5: type_metrics[q_type]['hits_5'] += 1
    
    # ì „ì²´ ë©”íŠ¸ë¦­
    metrics = {
        "Model": model_name,
        "MRR": round(mrr_sum / n, 3),
        "Recall@1": round(hits_1 / n, 3),
        "Recall@3": round(hits_3 / n, 3),
        "Recall@5": round(hits_5 / n, 3),
        "Latency(ms)": round((q_speed + c_speed) / 2, 1)
    }
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ ë©”íŠ¸ë¦­ ì¶”ê°€
    for q_type, type_data in type_metrics.items():
        count = type_data['count']
        if count > 0:
            metrics[f"{q_type}_MRR"] = round(type_data['mrr_sum'] / count, 3)
            metrics[f"{q_type}_R@1"] = round(type_data['hits_1'] / count, 3)
            metrics[f"{q_type}_R@3"] = round(type_data['hits_3'] / count, 3)
            metrics[f"{q_type}_R@5"] = round(type_data['hits_5'] / count, 3)
    
    return metrics

# -----------------------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# -----------------------------------------------------------------------------
def main():
    print("ğŸš€ [Korean History Docent] ì„ë² ë”© ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (Validation Set)")
    print(f"ğŸ“‚ ë°ì´í„°ì…‹: {BENCHMARK_FILE}")
    print(f"ğŸ“Š í‰ê°€ ëª¨ë¸ ìˆ˜: {len(MODELS)}ê°œ")
    print("âš ï¸  Validation Set ì‚¬ìš©: ëª¨ë¸ ì„ ì • ë‹¨ê³„ (ê³¼ì í•© ë°©ì§€)")
    print("=" * 80)
    
    try:
        dataset = load_benchmark_data()
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    print(f"ğŸ“Š í‰ê°€ ë°ì´í„° ìˆ˜: {len(dataset)}ê°œ")
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ í†µê³„
    type_counts = defaultdict(int)
    for item in dataset:
        type_counts[item.get('type', 'unknown')] += 1
    print("ğŸ“Š ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬:")
    for q_type, count in sorted(type_counts.items()):
        print(f"   - {q_type}: {count}ê°œ ({count/len(dataset)*100:.1f}%)")
    print("=" * 80)
    
    results = []
    # GPU 0ë²ˆì— í• ë‹¹
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    if torch.cuda.is_available():
        torch.cuda.set_device(0)  # GPU 0ë²ˆ ëª…ì‹œì  ì„¤ì •
        print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device} (GPU 0ë²ˆ)")
        print(f"   GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print("")
    
    for name, path in tqdm(MODELS.items(), desc="ëª¨ë¸ í‰ê°€ ì§„í–‰ ì¤‘"):
        print(f"\n{'='*80}")
        print(f"ğŸ” í‰ê°€ ì¤‘: {name}")
        print(f"{'='*80}")
        try:
            metrics = evaluate_model(name, path, dataset, device)
            results.append(metrics)
            print(f"\n   âœ… {name} í‰ê°€ ì™„ë£Œ:")
            print(f"      MRR: {metrics['MRR']}, Recall@1: {metrics['Recall@1']}")
            print(f"      Latency: {metrics['Latency(ms)']}ms")
        except Exception as e:
            print(f"\n   âŒ {name} í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    # ê²°ê³¼ ì¶œë ¥
    if results:
        df = pd.DataFrame(results)
        df = df.sort_values(by="MRR", ascending=False)
        
        print("\n" + "=" * 80)
        print("ğŸ† [ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼] ğŸ†")
        print("=" * 80)
        print(df.to_markdown(index=False))
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ê²°ê³¼ë„ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ğŸ“Š [ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥ ë¶„ì„]")
        print("=" * 80)
        type_columns = [col for col in df.columns if any(t in col for t in ['keyword', 'context', 'abstract'])]
        if type_columns:
            type_df = df[['Model'] + type_columns]
            print(type_df.to_markdown(index=False))
        
        # ê²°ê³¼ ì €ì¥
        csv_path = RESULTS_DIR / "benchmark_results_validation_set.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {csv_path}")
        
        # JSONìœ¼ë¡œë„ ì €ì¥ (ìƒì„¸ ì •ë³´ í¬í•¨)
        json_path = RESULTS_DIR / "benchmark_results_validation_set.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ìƒì„¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_path}")
    else:
        print("\nâŒ ìƒì„±ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()


