import json
import random
import time
import os
import requests
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# -----------------------------------------------------------------------------
# ì„¤ì • (Configuration)
# -----------------------------------------------------------------------------
BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
CHUNK_FILE = BASE_DIR / "02_Chunking/output/all_chunks.json"
OUTPUT_DIR = BASE_DIR / "03_Embedding/data"

# âœ… ê¸°ì¡´ íŒŒì¼(500ê°œ)ê³¼ ìµœì¢… íŒŒì¼(2000ê°œ) ê²½ë¡œ
EXISTING_FILE = OUTPUT_DIR / "korean_history_benchmark_500.json"
OUTPUT_FILE = OUTPUT_DIR / "korean_history_benchmark_2000.json"

TARGET_SIZE = 2000  # ëª©í‘œ ì§ˆë¬¸ ê°œìˆ˜

# API í‚¤ ë¡œë“œ (.env2 íŒŒì¼ì—ì„œ ì§ì ‘ ì½ê¸°)
ENV_FILE = Path("/home/pencilfoxs/00_new/.env2")
API_KEY = None
if ENV_FILE.exists():
    with open(ENV_FILE, 'r') as f:
        for line in f:
            if line.startswith("GOOGLE_API_KEY="):
                API_KEY = line.split("=", 1)[1].strip()
                break

# .env2ì—ì„œ ëª» ì°¾ì•˜ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
if not API_KEY:
    API_KEY = os.getenv("GOOGLE_API_KEY")

# -----------------------------------------------------------------------------
# Gemini API ì„¤ì • (REST API ì§ì ‘ í˜¸ì¶œ)
# -----------------------------------------------------------------------------
if not API_KEY:
    print("âš ï¸ Error: GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit(1)

# REST API ì—”ë“œí¬ì¸íŠ¸ (ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: gemini-2.5-flash ì‚¬ìš©)
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent"

# -----------------------------------------------------------------------------
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# -----------------------------------------------------------------------------
PROMPT_TEMPLATE = """
ë„ˆëŠ” í•œêµ­ì‚¬ ì „ë¬¸ê°€ì´ì ìˆ˜ëŠ¥ ì¶œì œ ìœ„ì›ì´ë‹¤.
ì•„ë˜ ì œê³µëœ [ì—­ì‚¬ ë¬¸ì„œ ì¡°ê°]ì„ ì½ê³ , ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê³ í’ˆì§ˆì˜ ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•˜ë¼.

[ì—­ì‚¬ ë¬¸ì„œ ì¡°ê°]
{chunk_text}

[ì¡°ê±´]
1. ì§ˆë¬¸ ìœ í˜•ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ë¼:
   - ì‚¬ì‹¤ í™•ì¸ (Fact): ì—°ë„, ì¸ë¬¼, ì‚¬ê±´ëª… ë“±ì„ ë¬»ëŠ” ì§ˆë¬¸
   - ì¸ê³¼ ì¶”ë¡  (Reasoning): ì‚¬ê±´ì˜ ì›ì¸, ê²°ê³¼, ì˜ë„ë¥¼ ë¬»ëŠ” ì§ˆë¬¸
   - ë³µí•© ì´í•´ (Complex): ì—¬ëŸ¬ ì •ë³´ë¥¼ ì¢…í•©í•´ì•¼ ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
2. ì •ë‹µì´ ë°˜ë“œì‹œ ìœ„ ë¬¸ì„œ ì¡°ê° ì•ˆì— í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.
3. ì§ˆë¬¸ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ë¼.
4. ì¶œë ¥ í˜•ì‹ì€ ì˜¤ì§ JSON í¬ë§·ë§Œ ì¶œë ¥í•˜ë¼. (ë§ˆí¬ë‹¤ìš´ backticks ì—†ì´)

[ì¶œë ¥ ì˜ˆì‹œ]
{{
  "query": "ì„¸ì¢…ëŒ€ì™•ì´ í›ˆë¯¼ì •ìŒì„ ì°½ì œí•œ ì£¼ëœ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?",
  "type": "Reasoning",
  "difficulty": "Medium"
}}
"""

# -----------------------------------------------------------------------------
# ë©”ì¸ ë¡œì§
# -----------------------------------------------------------------------------
def generate_questions():
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ì´ì–´í•˜ê¸°)
    benchmark_dataset = []
    used_chunk_ids = set()
    
    # 500ê°œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¨¼ì € ë¡œë“œ
    if EXISTING_FILE.exists():
        print(f"ğŸ“‚ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘: {EXISTING_FILE}")
        with open(EXISTING_FILE, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
            benchmark_dataset.extend(existing_data)
            for item in existing_data:
                used_chunk_ids.add(item['chunk_id'])
        print(f"   âœ… ê¸°ì¡´ {len(benchmark_dataset)}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ.")
    
    # ì´ë¯¸ 2000ê°œ ë„˜ìœ¼ë©´ ì¢…ë£Œ
    current_count = len(benchmark_dataset)
    if current_count >= TARGET_SIZE:
        print(f"ğŸ‰ ì´ë¯¸ ëª©í‘œ ê°œìˆ˜({TARGET_SIZE}ê°œ)ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    needed_count = TARGET_SIZE - current_count
    print(f"ğŸš€ ì¶”ê°€ ìƒì„± í•„ìš” ê°œìˆ˜: {needed_count}ê°œ")
    
    # 2. ì²­í¬ ë°ì´í„° ë¡œë“œ
    print(f"ğŸ“‚ ì²­í¬ íŒŒì¼ ë¡œë”© ì¤‘: {CHUNK_FILE}")
    with open(CHUNK_FILE, 'r', encoding='utf-8') as f:
        all_chunks = json.load(f)
    
    # 3. ìœ íš¨ ì²­í¬ í•„í„°ë§ & ì¤‘ë³µ ì œê±°
    valid_chunks = []
    for c in all_chunks:
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶©ë¶„í•˜ê³  + ì•„ì§ ì§ˆë¬¸ ì•ˆ ë§Œë“  ì²­í¬
        if (len(c['text']) > 100 and 
            c['metadata']['type'] == 'text' and 
            c['chunk_id'] not in used_chunk_ids):
            valid_chunks.append(c)
            
    print(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ í›„ë³´ ì²­í¬: {len(valid_chunks)}ê°œ")
    
    if len(valid_chunks) < needed_count:
        print(f"âš ï¸ ì£¼ì˜: ë‚¨ì€ ì²­í¬({len(valid_chunks)}ê°œ)ê°€ ëª©í‘œ({needed_count}ê°œ)ë³´ë‹¤ ì ìŠµë‹ˆë‹¤. ì „ë¶€ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        selected_chunks = valid_chunks
    else:
        selected_chunks = random.sample(valid_chunks, needed_count)
    
    print(f"ğŸš€ ì‹ ê·œ ì§ˆë¬¸ ìƒì„± ì‹œì‘ (ëª©í‘œ: {len(selected_chunks)}ê°œ)...")
    
    # ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
    def generate_single_question(i_offset, chunk):
        try:
            # REST API ì§ì ‘ í˜¸ì¶œ
            payload = {
                "contents": [{
                    "parts": [{
                        "text": PROMPT_TEMPLATE.format(chunk_text=chunk['text'])
                    }]
                }]
            }
            
            headers = {
                "Content-Type": "application/json"
            }
            
            response = requests.post(
                f"{GEMINI_API_URL}?key={API_KEY}",
                json=payload,
                headers=headers,
                timeout=30
            )
            
            if response.status_code != 200:
                raise Exception(f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code} - {response.text[:200]}")
            
            result = response.json()
            
            # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if 'candidates' in result and len(result['candidates']) > 0:
                text_response = result['candidates'][0]['content']['parts'][0]['text'].strip()
            else:
                raise Exception(f"ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {result}")
            
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° ë¡œì§
            if text_response.startswith("```"):
                text_response = text_response.replace("```json", "").replace("```", "").strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                q_data = json.loads(text_response)
            except json.JSONDecodeError as je:
                raise Exception(f"JSON íŒŒì‹± ì‹¤íŒ¨: {je}")
            
            # ë°ì´í„°ì…‹ ì—”íŠ¸ë¦¬ ìƒì„±
            # IDëŠ” ê¸°ì¡´ ê°œìˆ˜ + í˜„ì¬ ì¸ë±ìŠ¤ë¡œ ìƒì„±
            entry = {
                "id": f"q_{i_offset:05d}",  # 5ìë¦¬ë¡œ ëŠ˜ë¦¼ (q_00001)
                "chunk_id": chunk['chunk_id'],
                "query": q_data.get("query", ""),
                "type": q_data.get("type", "General"),
                "difficulty": q_data.get("difficulty", "Medium"),
                "gold_text": chunk['text'],
                "source": chunk['metadata']['source']
            }
            return (entry, None)
        except Exception as e:
            return (None, str(e))
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (ì†ë„ ìœ„í•´ ì›Œì»¤ 10ê°œ)
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {}
        # ì¸ë±ìŠ¤ëŠ” (ê¸°ì¡´ ê°œìˆ˜ + 0) ë¶€í„° ì‹œì‘
        start_idx = current_count
        
        for i, chunk in enumerate(selected_chunks):
            real_idx = start_idx + i
            futures[executor.submit(generate_single_question, real_idx, chunk)] = real_idx
            
        for future in tqdm(as_completed(futures), total=len(selected_chunks), desc="ì§ˆë¬¸ ìƒì„± ì¤‘"):
            idx = futures[future]
            entry, error = future.result()
            
            if entry:
                benchmark_dataset.append(entry)
            else:
                if error:
                    print(f"âš ï¸ [Index {idx}] ìƒì„± ì‹¤íŒ¨: {error}")
                
            # ì¤‘ê°„ ì €ì¥ (100ê°œë§ˆë‹¤)
            if len(benchmark_dataset) % 100 == 0 and len(benchmark_dataset) > 0:
                with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                    json.dump(benchmark_dataset, f, ensure_ascii=False, indent=2)
                print(f"   ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({len(benchmark_dataset)}ê°œ)")

    # 4. ìµœì¢… ì €ì¥
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(benchmark_dataset, f, ensure_ascii=False, indent=2)
        
    print(f"\nâœ… ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ í™•ì¥ ì™„ë£Œ!")
    print(f"   ì´ ì§ˆë¬¸: {len(benchmark_dataset)}ê°œ")
    print(f"   ì €ì¥ ê²½ë¡œ: {OUTPUT_FILE}")

if __name__ == "__main__":
    generate_questions()

