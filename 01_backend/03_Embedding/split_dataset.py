#!/usr/bin/env python3
"""
ë°ì´í„°ì…‹ ë¶„í•  ìŠ¤í¬ë¦½íŠ¸ (ê³¼ì í•© ë°©ì§€)
- Chunk ID ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€
- Train(60%) / Validation(20%) / Test(20%) ë¶„í• 
- ì§ˆë¬¸ ìœ í˜•ë³„ ë¹„ìœ¨ ìœ ì§€
"""

import json
import random
from pathlib import Path
from collections import defaultdict

# -----------------------------------------------------------------------------
# ì„¤ì •
# -----------------------------------------------------------------------------
DATA_DIR = Path("/home/pencilfoxs/00_new/History_Docent/03_Embedding/data")
INPUT_FILE = DATA_DIR / "korean_history_benchmark_balanced_11140.json"
SEED = 42  # ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ ê³ ì • ì‹œë“œ

# ë¶„í•  ë¹„ìœ¨
TRAIN_RATIO = 0.6
VAL_RATIO = 0.2
TEST_RATIO = 0.2

# -----------------------------------------------------------------------------
# í†µê³„ ì¶œë ¥ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def print_stats(name, data):
    """ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥"""
    print(f"\nğŸ“Š {name} í†µê³„:")
    print(f"  ì´ ê°œìˆ˜: {len(data)}ê°œ")
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬
    type_counts = defaultdict(int)
    for item in data:
        type_counts[item.get('type', 'unknown')] += 1
    
    total = len(data)
    if total > 0:
        for q_type in ['keyword', 'context', 'abstract']:
            count = type_counts[q_type]
            if count > 0:
                print(f"  - {q_type:<8}: {count:>4}ê°œ ({count/total*100:.1f}%)")

# -----------------------------------------------------------------------------
# ë©”ì¸ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def main():
    print("=" * 80)
    print("ğŸš€ ë°ì´í„°ì…‹ ë¶„í•  ì‹œì‘ (Chunk ID ê¸°ì¤€ - ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)")
    print("=" * 80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {INPUT_FILE}")
    if not INPUT_FILE.exists():
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_FILE}")
    
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… ì´ ë°ì´í„° ë¡œë“œ: {len(data)}ê°œ")
    
    # 2. Chunk ID ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
    # âš ï¸ ì¤‘ìš”: ê°™ì€ ì§€ë¬¸(Chunk)ì—ì„œ ë‚˜ì˜¨ ì§ˆë¬¸ë“¤ì€ ë°˜ë“œì‹œ ê°™ì€ ì„¸íŠ¸ì— ìˆì–´ì•¼ í•¨
    # ì´ë ‡ê²Œ í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ì„œ ë³¸ ì§€ë¬¸ì„ í‰ê°€ ë°ì´í„°ì—ì„œë„ ë³´ê²Œ ë˜ì–´ ë°ì´í„° ëˆ„ìˆ˜ ë°œìƒ!
    print("\nğŸ” Chunk ID ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™” ì¤‘...")
    chunk_groups = defaultdict(list)
    for item in data:
        chunk_id = item.get('chunk_id', 'unknown')
        chunk_groups[chunk_id].append(item)
    
    chunk_ids = list(chunk_groups.keys())
    print(f"âœ… ì´ ê³ ìœ  ì²­í¬ ìˆ˜: {len(chunk_ids)}ê°œ")
    
    # ì²­í¬ë³„ ì§ˆë¬¸ ìˆ˜ í™•ì¸
    chunk_sizes = [len(chunk_groups[cid]) for cid in chunk_ids]
    print(f"   - ì²­í¬ë‹¹ í‰ê·  ì§ˆë¬¸ ìˆ˜: {sum(chunk_sizes)/len(chunk_sizes):.1f}ê°œ")
    print(f"   - ìµœì†Œ ì§ˆë¬¸ ìˆ˜: {min(chunk_sizes)}ê°œ")
    print(f"   - ìµœëŒ€ ì§ˆë¬¸ ìˆ˜: {max(chunk_sizes)}ê°œ")
    
    # 3. ì…”í”Œ ë° ë¶„í•  (6:2:2)
    print(f"\nğŸ² Random Seed ê³ ì •: {SEED} (ì¬í˜„ ê°€ëŠ¥ì„± ë³´ì¥)")
    random.seed(SEED)
    random.shuffle(chunk_ids)
    
    n_chunks = len(chunk_ids)
    n_train = int(n_chunks * TRAIN_RATIO)
    n_val = int(n_chunks * VAL_RATIO)
    # ë‚˜ë¨¸ì§€ëŠ” test
    
    train_chunks = chunk_ids[:n_train]
    val_chunks = chunk_ids[n_train:n_train+n_val]
    test_chunks = chunk_ids[n_train+n_val:]
    
    print(f"\nğŸ“Š ë¶„í•  ê²°ê³¼:")
    print(f"  - Train ì²­í¬: {len(train_chunks)}ê°œ ({len(train_chunks)/n_chunks*100:.1f}%)")
    print(f"  - Validation ì²­í¬: {len(val_chunks)}ê°œ ({len(val_chunks)/n_chunks*100:.1f}%)")
    print(f"  - Test ì²­í¬: {len(test_chunks)}ê°œ ({len(test_chunks)/n_chunks*100:.1f}%)")
    
    # 4. ë°ì´í„°ì…‹ ìƒì„±
    print("\nğŸ“ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    train_set = []
    val_set = []
    test_set = []
    
    for cid in train_chunks:
        train_set.extend(chunk_groups[cid])
    for cid in val_chunks:
        val_set.extend(chunk_groups[cid])
    for cid in test_chunks:
        test_set.extend(chunk_groups[cid])
    
    print(f"  âœ… Train Set: {len(train_set)}ê°œ")
    print(f"  âœ… Validation Set: {len(val_set)}ê°œ")
    print(f"  âœ… Test Set: {len(test_set)}ê°œ")
    
    # 5. ê²°ê³¼ ì €ì¥
    output_files = {
        "train": DATA_DIR / "train_set_60.json",
        "validation": DATA_DIR / "validation_set_20.json",
        "test": DATA_DIR / "test_set_20.json"
    }
    
    print("\nğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...")
    with open(output_files["train"], 'w', encoding='utf-8') as f:
        json.dump(train_set, f, ensure_ascii=False, indent=2)
    print(f"  âœ… {output_files['train'].name}")
        
    with open(output_files["validation"], 'w', encoding='utf-8') as f:
        json.dump(val_set, f, ensure_ascii=False, indent=2)
    print(f"  âœ… {output_files['validation'].name}")
        
    with open(output_files["test"], 'w', encoding='utf-8') as f:
        json.dump(test_set, f, ensure_ascii=False, indent=2)
    print(f"  âœ… {output_files['test'].name}")
    
    # 6. ê²€ì¦ í†µê³„ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ë¶„í•  ê²°ê³¼ ê²€ì¦")
    print("=" * 80)
    print_stats("ì „ì²´ ë°ì´í„° (ì›ë³¸)", data)
    print("-" * 80)
    print_stats(f"Train Set ({len(train_set)}ê°œ)", train_set)
    print_stats(f"Validation Set ({len(val_set)}ê°œ)", val_set)
    print_stats(f"Test Set ({len(test_set)}ê°œ)", test_set)
    print("=" * 80)
    
    # 7. ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦
    print("\nğŸ” ë°ì´í„° ëˆ„ìˆ˜ ê²€ì¦ ì¤‘...")
    train_chunk_ids = set(train_chunks)
    val_chunk_ids = set(val_chunks)
    test_chunk_ids = set(test_chunks)
    
    # ê²¹ì¹˜ëŠ” ì²­í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
    train_val_overlap = train_chunk_ids & val_chunk_ids
    train_test_overlap = train_chunk_ids & test_chunk_ids
    val_test_overlap = val_chunk_ids & test_chunk_ids
    
    if train_val_overlap or train_test_overlap or val_test_overlap:
        print("  âŒ ê²½ê³ : ì²­í¬ê°€ ê²¹ì¹©ë‹ˆë‹¤! ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜!")
        if train_val_overlap:
            print(f"     Train-Val ê²¹ì¹¨: {len(train_val_overlap)}ê°œ")
        if train_test_overlap:
            print(f"     Train-Test ê²¹ì¹¨: {len(train_test_overlap)}ê°œ")
        if val_test_overlap:
            print(f"     Val-Test ê²¹ì¹¨: {len(val_test_overlap)}ê°œ")
    else:
        print("  âœ… ì™„ë²½! ëª¨ë“  ì„¸íŠ¸ê°€ ë…ë¦½ì ì…ë‹ˆë‹¤. ë°ì´í„° ëˆ„ìˆ˜ ì—†ìŒ!")
    
    print("\n" + "=" * 80)
    print("âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ!")
    print("=" * 80)
    print("\nğŸ“‹ ì‚¬ìš© ê°€ì´ë“œ:")
    print("  1. ì„ë² ë”© ëª¨ë¸ ì„ ì • â†’ validation_set_20.json ì‚¬ìš©")
    print("  2. ì„ë² ë”© ëª¨ë¸ íŒŒì¸íŠœë‹ â†’ train_set_60.json (í•™ìŠµ), validation_set_20.json (ê²€ì¦)")
    print("  3. ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€ â†’ validation_set_20.json ì‚¬ìš©")
    print("  4. ë¦¬ë­ì»¤ íŒŒì¸íŠœë‹ â†’ train_set_60.json (í•™ìŠµ), validation_set_20.json (ê²€ì¦)")
    print("  5. ìµœì¢… í‰ê°€ â†’ test_set_20.json ì‚¬ìš© (í•œ ë²ˆë§Œ!)")
    print("\nâš ï¸  ì£¼ì˜: Test Setì€ ëª¨ë“  ê°œë°œ/íŠœë‹ ì™„ë£Œ í›„ ìµœì¢… í‰ê°€ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”!")
    print("=" * 80)

if __name__ == "__main__":
    main()

