# [Embedding] 임베딩 모델 선정 및 검증 로그
- **작성 일시:** 2025-11-22 10:50
- **작성자:** Pencilfoxs
- **최종 업데이트:** 2025-11-22 10:50 (2000개 데이터셋 벤치마크 완료)

---

## 1. 가설 (Hypothesis)

### 1.1 초기 가정
- **가설 1:** 한국어와 영어, 그리고 역사적 고유명사(한자어 포함)를 모두 잘 처리해야 하므로, **다국어(Multilingual) 성능이 입증된 모델**이 한국어 전용 모델보다 RAG 검색 성능(MRR)이 더 높을 것이다.
- **가설 2:** Google의 최신 임베딩 모델(Gemini-API, EmbeddingGemma)은 상용 API의 안정성과 최신 기술력으로 높은 성능을 보일 것으로 예상한다.
- **가설 3:** 평가 데이터셋의 품질(Quality)이 양(Quantity)보다 중요하다. 고품질 질문으로 구성된 소규모 데이터셋이 저품질 대규모 데이터셋보다 더 신뢰할 수 있는 평가 결과를 제공할 것이다.
- **예상 결과:** BGE-m3 또는 Jina-v3 같은 최신 다국어 모델이 Ko-SBERT(한국어 베이스라인)보다 우수할 것으로 예상하며, Google 모델도 경쟁력 있는 성능을 보일 것으로 기대한다.

---

## 2. 실험 설계 (Experiment Design)

### 2.1 테스트 데이터셋 구축 배경
- **목적:** 한국사 RAG 시스템의 실제 검색 성능을 측정하기 위해, 실제 사용자 질문과 유사한 고품질 질문-답변 쌍이 필요했다.
- **방법:** LLM 기반 합성 데이터 생성(Synthetic Data Generation) 방식을 채택.
- **도구:** Google Gemini-2.5-Flash API
- **프롬프트 전략:** "한국사 전문가이자 수능 출제 위원" 페르소나를 적용하여, 단순 사실 확인뿐만 아니라 인과 추론, 복합 이해 질문까지 포함하도록 설계.

### 2.2 실험 규모 (Experiment Scale)

#### 2.2.1 1차 데이터셋 (초기 검증용)
- **데이터셋 크기:** 총 **496개** 질문-답변 쌍 (목표 500개 중 496개 성공)
- **데이터 출처:**
  - 소스: `02_Chunking/output/all_chunks.json` (청킹 완료된 텍스트 청크)
  - 필터링 조건: 텍스트 길이 100자 이상, `metadata.type == "text"`인 청크만 선별
  - 샘플링: 랜덤 샘플링으로 500개 청크 선택
- **생성 일시:** 2025-11-22 08:00 ~ 08:30

#### 2.2.2 2차 데이터셋 확장 (최종 검증용)
- **확장 배경:** 500개 데이터셋으로 초기 벤치마크를 완료한 후, 통계적 유의성을 높이고 엣지 케이스를 더 많이 포함하기 위해 데이터셋을 확장하기로 결정.
- **확장 방법:** 기존 500개 데이터를 유지한 채, 중복되지 않는 청크에서 추가로 1500개를 생성하여 총 2000개로 확장.
- **데이터셋 크기:** 총 **2000개** 질문-답변 쌍
- **생성 일시:** 2025-11-22 09:00 ~ 10:30 (약 1.5시간 소요)
- **생성 코드:** `generate_benchmark_v2.py` (이어하기 기능 추가)

#### 2.2.3 질문 유형 분포
- 사실 확인 (Fact): 연도, 인물, 사건명 등
- 인과 추론 (Reasoning): 사건의 원인, 결과, 의도
- 복합 이해 (Complex): 여러 정보를 종합해야 답할 수 있는 질문

#### 2.2.4 검증 방법
- 각 질문(Query)에 대해 원본 청크(Gold Passage)가 정답으로 설정됨
- 전체 질문을 Corpus로 사용하여, 각 모델이 정답을 상위 몇 번째에 랭크하는지 측정

### 2.3 후보군 선정 이유 (Why Candidates)

| 모델명 | 선정 이유 | 특징 및 기대 효과 |
|---|---|---|
| **BGE-m3** | MTEB(Massive Text Embedding Benchmark) 리더보드 상위권, 다국어 처리 강점 | 희소(Sparse) 임베딩 지원, 긴 문맥(8192 토큰) 처리 가능, 100+ 언어 지원 |
| **Jina-v3** | 최신 SOTA 모델, Task별 최적화 가능 | 8192 토큰 지원, 검색(Retrieval) 특화 모드(`retrieval.query` / `retrieval.passage`) |
| **E5-large** | 꾸준히 상위권을 유지하는 검증된 모델 | `query:` / `passage:` 접두사 사용 방식으로 검색 성능 최적화 |
| **Ko-SBERT** | 한국어 자연어 처리의 베이스라인 모델 | 빠르고 가벼움, 한국어 특화 학습, 속도 대비 성능 확인용 |
| **GTE-large** | Alibaba의 고성능 임베딩 모델 | 긴 문맥 이해도 우수, 다국어 지원 |
| **Gemini-API** | Google의 최신 상용 임베딩 API | `text-embedding-004` 모델, 클라우드 기반 성능 확인용, API 안정성 테스트 |
| **EmbeddingGemma** | Google의 오픈 소스 LLM 기반 임베딩 | Gemma-2b 기반, 로컬 LLM 임베딩 가능성 확인, 온디바이스 실행 가능성 |

### 2.4 비교 기준 (Criteria)
- **1차 필터링 (정량 평가):**
  - **MRR (Mean Reciprocal Rank):** 정답이 상위에 랭크될수록 높은 점수. 검색 품질의 핵심 지표. (가중치: 50%)
  - **Recall@1:** 정답이 1등으로 나온 비율. 사용자 경험에 직접적 영향. (가중치: 30%)
  - **Recall@3, Recall@5:** 정답이 상위 3개, 5개 안에 포함된 비율. (가중치: 10% 각)
- **2차 필터링 (정성 평가):**
  - 실제 데이터 샘플(최소 20개 이상)을 사용하여 모델의 출력 결과를 육안으로 검증
  - 한국어 고유명사, 한자어, 역사적 맥락을 정확히 파악하는지 확인
- **부가 고려사항:**
  - **Latency (ms):** 쿼리/문서 하나당 임베딩 생성 평균 소요 시간. 실시간 서비스 가능 여부 판단.
  - **모델 크기 및 배포 용이성:** 로컬 구동 가능 여부, 메모리 요구사항

---

## 3. 검증 결과 (Validation)

### 3.1 정량 평가 결과 (Quantitative Results)

#### 3.1.1 500개 데이터셋 벤치마크 결과

| Rank | Model | MRR | Recall@1 | Recall@3 | Recall@5 | Latency (ms) | 평가 |
|---|---|---|---|---|---|---|---|
| **1** | **BGE-m3** | **0.932** | **0.895** | **0.970** | **0.980** | 20.5 | **최고 성능 (Winner)** |
| 2 | Jina-v3 | 0.915 | 0.867 | 0.958 | 0.972 | 14.1 | 성능 우수 & 가장 빠름 |
| 3 | E5-large | 0.913 | 0.869 | 0.956 | 0.964 | 17.0 | 안정적 성능 |
| 4 | Ko-SBERT | 0.702 | 0.625 | 0.746 | 0.798 | **4.3** | 가장 빠르나 정확도 부족 |
| 5 | GTE-large | 0.321 | 0.234 | 0.353 | 0.405 | 42.4 | 성능 저조 (원인 분석 필요) |
| 6 | Gemini-API | 0.076 | 0.050 | 0.077 | 0.093 | 19.8 | **실패 (API 호출/형식 이슈)** |
| 7 | EmbeddingGemma | 0.014 | 0.002 | 0.006 | 0.010 | 0.0 | **실패 (모델 로드 오류)** |

#### 3.1.2 2000개 데이터셋 벤치마크 결과 (최종)

| Rank | Model | MRR | Recall@1 | Recall@3 | Recall@5 | Latency (ms) | 평가 |
|---|---|---|---|---|---|---|---|
| **1** | **BGE-m3** | **0.834** | **0.754** | **0.902** | **0.933** | 10.1 | **최고 성능 (Winner)** |
| 2 | **E5-large** | 0.814 | 0.728 | 0.884 | 0.923 | **8.8** | 안정적 성능 & 가장 빠름 |
| 3 | **Jina-v3** | 0.804 | 0.709 | 0.888 | 0.920 | 6.9 | 성능 우수 |
| 4 | Ko-SBERT | 0.587 | 0.496 | 0.634 | 0.691 | **1.8** | 가장 빠르나 정확도 부족 |
| 5 | GTE-large | 0.224 | 0.159 | 0.243 | 0.287 | 25.8 | 성능 저조 |
| 6 | Gemini-API | 0.041 | 0.028 | 0.042 | 0.051 | 31.0 | **실패 (API 호출/형식 이슈)** |
| 7 | EmbeddingGemma | 0.004 | 0.001 | 0.002 | 0.003 | 0.0 | **실패 (모델 로드 오류)** |

#### 3.1.3 데이터셋 규모별 성능 비교 분석

| 모델 | 500개 데이터셋 | 2000개 데이터셋 | 차이 | 분석 |
|---|---|---|---|---|
| **BGE-m3** | MRR 0.932 (1위) | MRR 0.834 (1위) | -0.098 | 데이터가 늘어나도 1위 유지, 성능 안정적 |
| **E5-large** | MRR 0.913 (3위) | MRR 0.814 (2위) | -0.099 | 순위 상승 (3위 → 2위), 안정적 성능 |
| **Jina-v3** | MRR 0.915 (2위) | MRR 0.804 (3위) | -0.111 | 순위 하락 (2위 → 3위), 여전히 우수 |
| **Ko-SBERT** | MRR 0.702 (4위) | MRR 0.587 (4위) | -0.115 | 순위 유지, 성능 차이는 일관적 |

**주요 발견:**
- 모든 모델의 MRR이 2000개 데이터셋에서 낮아짐: 더 다양한 난이도와 엣지 케이스가 포함되어 평가가 더 엄격해졌음을 의미.
- BGE-m3는 두 데이터셋 모두에서 1위를 유지하며, 성능 안정성이 입증됨.
- E5-large는 2000개 데이터셋에서 순위가 상승(3위 → 2위)하여, 대규모 데이터에서의 안정성이 확인됨.

#### 3.1.4 과거 프로젝트 데이터셋(1912개) 비교 분석

**과거 프로젝트 데이터셋 특성:**
- **출처:** `/home/pencilfoxs/History_Docent_PJ_gemini/7_Evaluation/train_qa_dataset.json` + `eval_qa_dataset.json`
- **생성 도구:** EleutherAI/polyglot-ko-1.3b (1.3B 소형 모델)
- **프롬프트:** 단순 요약 요청 ("다음 내용을 한 문장의 질문으로 요약해줘")
- **특징:** 질문 품질이 상대적으로 낮음 (문법 오류, 단순 키워드 나열 등)

**1912개 데이터셋 벤치마크 결과:**

| Rank | Model | MRR | Recall@1 | 분석 |
|---|---|---|---|---|
| 1 | Jina-v3 | 0.518 | 0.425 | 1위 |
| 2 | E5-large | 0.502 | 0.418 | 2위 |
| 3 | BGE-m3 | 0.499 | 0.413 | 3위 |
| 4 | Ko-SBERT | 0.353 | 0.286 | 4위 |

**비교 분석:**
- **2000개 고품질 데이터셋:** BGE-m3 (MRR 0.834) > E5-large (MRR 0.814) > Jina-v3 (MRR 0.804)
- **1912개 저품질 데이터셋:** Jina-v3 (MRR 0.518) > E5-large (MRR 0.502) > BGE-m3 (MRR 0.499)

**결론:**
- 저품질 질문에서는 Jina-v3가 약간 우세하나, **고품질 질문(실제 사용자 질문과 유사)에서는 BGE-m3가 압도적으로 우수함.**
- 이는 **데이터 품질(Quality)이 양(Quantity)보다 중요함**을 입증하는 결과.
- 실제 서비스에서는 고품질 질문이 더 많으므로, BGE-m3를 선택하는 것이 올바른 판단.

### 3.2 정성 평가 및 실패 분석 (Qualitative & Failure Analysis)

#### 3.2.1 정성 평가 개요
- **평가 방법:** 2000개 벤치마크 데이터셋에서 **50개 샘플을 랜덤 추출**하여 실제 검색 결과를 육안으로 검증
- **평가 일시:** 2025-11-22 11:31
- **평가 환경:** ChromaDB 벡터 데이터베이스, BGE-m3 임베딩 모델 (CUDA 사용)
- **평가 결과:**
  - **Recall@1:** 38개 (76.0%) - 정답이 1위로 검색된 비율
  - **Recall@3:** 43개 (86.0%) - 정답이 상위 3위 안에 포함된 비율
  - **Recall@5:** 47개 (94.0%) - 정답이 상위 5위 안에 포함된 비율
  - **실패 케이스:** 3개 (6.0%) - 상위 5위 안에 정답이 없는 경우

#### 3.2.2 성공 사례 분석 (50개 샘플 중 대표 사례)

**BGE-m3 (성공):**
- **성공 요인:**
  - 한국어 고유명사(예: '손기정', '훈민정음', '세종대왕')를 정확하게 인식하고, 역사적 맥락을 잘 파악함.
  - 다국어 학습 데이터로 인해 한자어와 영어가 혼재된 텍스트도 효과적으로 처리.
  - 긴 문맥(최대 8192 토큰) 지원으로 복합 이해 질문에도 강점을 보임.
- **대표 성공 사례 (50개 샘플 중):**
  
  **Case 1: 단순 사실 확인 (Rank 1)**
  - 질문: "공민왕이 제주 목호들에게 말 2천 필을 명나라로 보내라고 명령한 주요 목적은 무엇이었습니까?"
  - 정답 청크가 1등으로 랭크됨 (거리: 0.5087). "공민왕", "제주 목호", "명나라" 등의 키워드를 정확히 매칭.
  
  **Case 2: 복합 추론 질문 (Rank 1)**
  - 질문: "제시된 문서의 내용을 종합하여 볼 때, 세종이 문치주의 통치 체제를 확립하고자 집현전을 재설치한 목적을 달성하기 위해 젊은 인재들을 어떻게 등용하고 활용하였는가?"
  - 정답 청크가 1등으로 랭크됨 (거리: 0.6564). "세종", "집현전", "문치주의" 등 복합 키워드를 정확히 연결.
  
  **Case 3: 인물 관계 추론 (Rank 2)**
  - 질문: "문서에 제시된 내용에 따르면, 김유신이 김춘추와 문희의 혼례를 성사시키기 위해 '이 소동을 벌인' 주된 이유는 무엇인가?"
  - 정답 청크가 2등으로 랭크됨 (거리: 0.6768). 인물 관계와 사건의 인과관계를 잘 파악.
  
  **Case 4: 역사적 사건 분석 (Rank 1)**
  - 질문: "선조가 피난을 강행하자 백성들이 경복궁, 창경궁, 창덕궁 등의 궁궐을 불태우고 형조와 장례원에 보관 중이던 노비 문서를 소각한 주된 원인은 무엇이라고 이 문서에서 추론할 수 있는가?"
  - 정답 청크가 1등으로 랭크됨 (거리: 0.7867). "임진왜란", "선조 피난" 등 역사적 맥락을 정확히 이해.
  
  **Case 5: 근현대사 질문 (Rank 1)**
  - 질문: "다음 자료는 1920년대 조선물산장려운동에 대한 것이다. 자료를 통해 이 운동의 발발 배경과 전개 과정에서 민족 경제를 지키기 위한 어떤 노력이 이루어졌는지 '정세권'의 활동을 중심으로 종합적으로 설명하시오."
  - 정답 청크가 1등으로 랭크됨 (거리: 0.7629). "조선물산장려운동", "정세권" 등 근현대사 키워드를 정확히 매칭.
  
  **Case 6: 복잡한 정치사 질문 (Rank 2)**
  - 질문: "제공된 역사 문서 조각을 바탕으로, 한명회가 생전에 정치 일선에서 물러나게 된 구체적인 사건과 그가 죽은 후에도 부관참시형을 받게 된 직접적인 원인을 각각 설명하시오."
  - 정답 청크가 2등으로 랭크됨 (거리: 0.8028). 복잡한 정치사 질문에도 관련 문서를 정확히 찾아냄.
  
  **Case 7: 고려시대 질문 (Rank 1)**
  - 질문: "강감찬이 현종에게 개경을 버리고 남쪽으로 몽진할 것을 제안한 궁극적인 목적은 무엇이었는가?"
  - 정답 청크가 1등으로 랭크됨. 고려시대 인물과 사건을 정확히 매칭.
  
  **Case 8: 조선왕조실록 관련 질문 (Rank 1)**
  - 질문: "제시된 문서에 따르면, 『정종실록』의 본래 제목이 『공정왕실록』이었던 주된 이유는 무엇인가?"
  - 정답 청크가 1등으로 랭크됨 (거리: 0.7513). 실록 관련 전문 용어를 정확히 이해.
  
  **Case 9: 야사 기반 질문 (Rank 1)**
  - 질문: "야사에 따르면, 동궁에 불이 났을 때 세자가 불 속에서 나오려 하지 않았음에도 불구하고 결국 밖으로 나오게 된 주된 이유는 무엇인가?"
  - 정답 청크가 1등으로 랭크됨 (거리: 0.6580). 야사 관련 내용도 정확히 검색.
  
  **Case 10: 외교사 질문 (Rank 1)**
  - 질문: "삼전도비문(三田渡碑文)이 완성된 후, 비문을 찬술하는 것이 조선 조정에 미친 영향은 무엇이었는가?"
  - 정답 청크가 1등으로 랭크됨. 외교사 관련 질문도 정확히 처리.
  
  **주요 발견:**
  - **76%의 질문에서 정답이 1위로 검색됨** - 매우 높은 정확도
  - **94%의 질문에서 정답이 상위 5위 안에 포함됨** - 실용적인 수준의 검색 성능
  - **복잡한 추론 질문, 인물 관계 질문, 역사적 맥락 질문 모두 잘 처리됨**
  - **고려, 조선, 근현대 등 시대별로 골고루 정확한 검색 결과**

**E5-large (성공):**
- **성공 요인:**
  - `query:` / `passage:` 접두사 처리로 검색 성능 최적화.
  - 2000개 데이터셋에서 순위 상승(3위 → 2위)하여 대규모 데이터에서의 안정성 입증.
- **한계점:**
  - BGE-m3 대비 MRR 0.02 차이로, 일부 복잡한 추론 질문에서 정확도가 약간 낮음.

**Jina-v3 (성공):**
- **성공 요인:**
  - 최신 Instruction 기반 학습으로 검색 Task에 특화됨.
  - 속도가 가장 빠르며(6.9ms), 실시간 서비스에 적합.
- **한계점:**
  - 2000개 데이터셋에서 순위 하락(2위 → 3위), BGE-m3 대비 MRR 0.03 차이.

#### 3.2.3 실패 사례 분석 (50개 샘플 중 3개 실패)

**정성 평가 실패 케이스 (3개):**

**Case 23: 세조실록 관련 질문 (실패)**
- 질문: "제시된 『세조실록』 기록을 바탕으로 세조 즉위 초에 발생했던 정국 불안정..."
- 분석: 상위 5위 안에 정답이 포함되지 않음. 세조실록 관련 전문 용어나 특정 시기(즉위 초)에 대한 검색이 어려웠을 가능성.
- 원인 추정: 질문이 너무 길거나, 특정 시기(즉위 초)에 대한 정보가 청크에 명시적으로 없었을 수 있음.

**Case 36: 한자어 관련 질문 (실패)**
- 질문: "제공된 문서 조각에서 '弟(아우 제)'를 뜻하는 단어로 표기된 것은 무엇..."
- 분석: 한자어 해석 관련 질문으로, 검색 결과가 관련성이 낮았음.
- 원인 추정: 한자어 자체의 의미 해석보다는 문서의 맥락을 찾는 질문이었는데, 모델이 한자어의 의미 해석에 집중하여 관련 문서를 놓쳤을 가능성.

**Case 50: 임꺽정 관련 질문 (실패)**
- 질문: "제시된 기록을 바탕으로 볼 때, 임꺽정 도적단이 여러 마을에 빈번하게 출..."
- 분석: 상위 5위 안에 정답이 포함되지 않음.
- 원인 추정: 질문이 불완전하게 잘렸거나, "임꺽정 도적단"이라는 표현이 청크 내에서 다르게 표현되었을 가능성.

**실패 패턴 분석:**
- 실패율 6% (3/50)로 매우 낮은 수준
- 실패 케이스는 주로 **전문 용어 해석**, **불완전한 질문**, **특정 시기 명시** 등이 원인
- 대부분의 일반적인 질문(94%)은 정확히 처리됨

**모델별 실패 분석:**

**GTE-large (실패 - 성능 저조):**
- **문제:** 2000개 데이터셋에서 MRR 0.224로 매우 낮은 성능. Recall@1이 15.9%로, 10번 중 1.5번 정도만 정답을 찾음.
- **원인 분석:**
  1. **한국어 학습 데이터 부족:** GTE-large는 주로 영어 데이터로 학습되었을 가능성이 높음. 한국어 토크나이저가 한글을 효율적으로 처리하지 못했을 수 있음.
  2. **정규화 이슈:** 벤치마크 코드에서 `normalize_embeddings=True`를 사용했으나, GTE 모델의 출력 벡터 스케일이 다른 모델과 다를 수 있음.
  3. **모델 특성:** GTE는 긴 문맥 이해에 특화되어 있으나, 짧은 질문-문서 매칭에는 부적합할 수 있음.
- **조치:** 한국어 특화 모델이 아니므로 제외 결정.

**Gemini-API (실패 - API 호출 이슈):**
- **문제:** 2000개 데이터셋에서 MRR 0.041로 매우 낮음. Recall@1이 2.8%로 거의 랜덤 수준.
- **원인 분석:**
  1. **API 호출 코드 버그 가능성:** `batchEmbedContents` API 호출 시 벡터 차원이나 정렬 문제가 있었을 수 있음.
  2. **유사도 측정 방식 차이:** Gemini API의 임베딩 벡터가 Cosine Similarity 계산 시 스케일링이나 전처리 차이가 있었을 것으로 추정됨.
  3. **API 타임아웃:** 2000개 데이터셋 테스트 중 일부 요청에서 타임아웃 발생 (`Read timed out. (read timeout=30)`).
- **조치:** API 호출 코드를 재검토하거나, Google의 공식 예제를 참고하여 수정 필요. 다만, 로컬 구동 가능한 오픈소스 모델의 성능이 우수하여 우선순위를 낮춤.

**EmbeddingGemma (실패 - 모델 로드 오류):**
- **문제:** HuggingFace 모델 ID 오류로 로드 실패. `google/embedding-gemma-2b-en` 접근 불가.
- **원인 분석:**
  1. **모델 ID 오류:** HuggingFace에 해당 모델이 존재하지 않거나, 다른 이름으로 등록되어 있을 수 있음.
  2. **접근 권한 문제:** Private 모델일 경우 인증 토큰이 필요할 수 있음.
  3. **Transformers 라이브러리 버전 문제:** 최신 버전이 필요할 수 있음.
- **조치:** 모델 ID를 재확인하거나, Transformers 라이브러리로 직접 로드하는 방식으로 수정 가능. 다만, 벤치마크 결과 BGE-m3가 압도적 성능을 보여 우선순위를 낮춤.

---

## 4. 의사결정 (Conclusion & Pivot)

### 4.1 최종 선정 모델
- **선정 모델:** **BGE-m3 (`BAAI/bge-m3`)**
- **선정 근거:**
  1. **압도적인 성능:** 2000개 대규모 데이터셋에서 MRR 0.834로 1위 달성. Recall@1이 75.4%로, 실제 서비스에서 사용자 질문의 75% 이상을 정확히 답변할 수 있음.
  2. **데이터셋 규모에 관계없이 일관된 1위:** 500개 데이터셋(MRR 0.932)과 2000개 데이터셋(MRR 0.834) 모두에서 1위를 유지하여 성능 안정성 입증.
  3. **한국어 역사 문서 특수성 반영:** 한자어, 고유명사, 역사적 맥락을 잘 파악함.
  4. **속도:** 10.1ms로 실시간 서비스에 무리가 없는 수준.
  5. **로컬 구동 가능:** 오픈소스 모델로 API 비용 없이 사용 가능.

### 4.2 Pivot (계획 변경 사항)

#### 4.2.1 데이터셋 확장 결정
- **초기 계획:** 500개 데이터셋으로 초기 벤치마크를 완료한 후, 결과를 바탕으로 모델 선정 예정.
- **변경 사항:** 과거 프로젝트의 1912개 데이터셋과 비교 분석을 진행한 결과, 데이터셋 품질의 중요성을 깨달음. 고품질 데이터셋을 더 확장하여 통계적 유의성을 높이기로 결정.
- **실행:** 기존 500개 데이터를 유지한 채, 추가로 1500개를 생성하여 총 2000개로 확장 (2025-11-22 09:00 ~ 10:30).

#### 4.2.2 모델 선정 기준 재검토
- **초기 계획:** Google 생태계(Gemini-API, EmbeddingGemma) 활용을 고려했으나, 벤치마크 결과 로컬 구동이 가능한 오픈소스 모델(BGE-m3)의 성능이 월등히 좋아 이를 채택함.
- **대안 고려:** 
  - Jina-v3도 속도와 성능의 균형이 좋아 대안으로 고려했으나, 2000개 데이터셋에서 BGE-m3의 MRR 0.03 차이로 인해 최종적으로 BGE-m3를 선택함.
  - E5-large는 2000개 데이터셋에서 순위가 상승(3위 → 2위)하여 안정성이 입증되었으나, BGE-m3 대비 MRR 0.02 차이로 인해 최종 선정하지 않음.
- **Ko-SBERT 제외:** 속도는 가장 빠르나(1.8ms), 복잡한 역사적 추론 질문에는 성능이 떨어져 제외함.

### 4.3 최종 완료 사항
1. ✅ **전체 청크 임베딩 생성 완료:** `02_Chunking/output/all_chunks.json`의 모든 청크(3719개)에 대해 BGE-m3 모델로 임베딩 벡터 생성 완료 (2025-11-22 10:44).
2. ✅ **임베딩 파일 저장:** `03_Embedding/output/chunks_with_embeddings.json`에 저장 완료.
3. ⏳ **향후 계획:**
   - Vector DB 구축: ChromaDB 또는 FAISS를 사용하여 벡터 데이터베이스 구축.
   - 검색 모듈 개발: 사용자 질문을 임베딩하여 벡터 DB에서 유사도 기반 검색 수행.

---

## 5. 참고 자료
- **벤치마크 결과 파일:**
  - `results/benchmark_results_500.csv` (500개 데이터셋)
  - `results/benchmark_results_2000.csv` (2000개 데이터셋, 최종)
  - `results/benchmark_results_1912.csv` (과거 프로젝트 데이터셋, 참고용)
- **벤치마크 실행 로그:**
  - `benchmark_execution.log` (500개)
  - `benchmark_execution_2000.log` (2000개, 최종)
- **벤치마크 데이터셋:**
  - `data/korean_history_benchmark_500.json` (초기)
  - `data/korean_history_benchmark_2000.json` (최종)
- **정성 평가 보고서:**
  - `../04_VectorDB/qualitative_analysis_report.txt` (50개 샘플 상세 분석)
  - `../04_VectorDB/qualitative_analysis.log` (실행 로그)
- **벤치마크 코드:** `benchmark_embeddings.py`
- **정성 평가 코드:** `../04_VectorDB/verify_vectordb_qualitative.py`
- **임베딩 생성 코드:** `generate_embeddings.py`
- **임베딩 결과 파일:** `output/chunks_with_embeddings.json` (3719개 청크)
