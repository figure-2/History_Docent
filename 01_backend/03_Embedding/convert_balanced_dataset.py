#!/usr/bin/env python3
"""
ê· í˜•ì¡íŒ QA ë°ì´í„°ì…‹ì„ ì„ë² ë”© ë²¤ì¹˜ë§ˆí¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- ìƒˆë¡œìš´ ë°ì´í„°ì…‹(11,140ê°œ)ì„ í‰ê°€ìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- ì§ˆë¬¸ ìœ í˜•ë³„ë¡œë„ ë¶„ë¦¬í•˜ì—¬ í‰ê°€ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±
"""

import json
from pathlib import Path
from collections import defaultdict

# ê²½ë¡œ ì„¤ì •
QA_DATASET = Path("/home/pencilfoxs/00_new/History_Docent/06_LLM_Evaluation/balanced_qa_dataset_full.jsonl")
CHUNK_FILE = Path("/home/pencilfoxs/00_new/History_Docent/02_Chunking/output/all_chunks.json")
OUTPUT_DIR = Path("/home/pencilfoxs/00_new/History_Docent/03_Embedding/data")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

def load_chunks():
    """ì²­í¬ íŒŒì¼ ë¡œë“œ"""
    with open(CHUNK_FILE, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    return {chunk['chunk_id']: chunk for chunk in chunks}

def convert_dataset():
    """QA ë°ì´í„°ì…‹ì„ ë²¤ì¹˜ë§ˆí¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    chunks = load_chunks()
    
    # ì „ì²´ ë°ì´í„°ì…‹
    all_benchmark = []
    # ì§ˆë¬¸ ìœ í˜•ë³„ ë°ì´í„°ì…‹
    by_type = defaultdict(list)
    
    with open(QA_DATASET, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
                
            data = json.loads(line)
            chunk_id = data.get('chunk_id')
            question = data.get('question')
            q_type = data.get('type')
            
            if not chunk_id or not question:
                continue
            
            # ì²­í¬ ì°¾ê¸°
            chunk = chunks.get(chunk_id)
            if not chunk:
                continue
            
            # ë²¤ì¹˜ë§ˆí¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            benchmark_item = {
                'query': question,
                'gold_text': chunk.get('text', ''),
                'chunk_id': chunk_id,
                'type': q_type,
                'metadata': chunk.get('metadata', {})
            }
            
            all_benchmark.append(benchmark_item)
            by_type[q_type].append(benchmark_item)
    
    # ì €ì¥
    # 1. ì „ì²´ ë°ì´í„°ì…‹
    output_file = OUTPUT_DIR / "korean_history_benchmark_balanced_11140.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_benchmark, f, ensure_ascii=False, indent=2)
    print(f"âœ… ì „ì²´ ë°ì´í„°ì…‹ ì €ì¥: {output_file} ({len(all_benchmark)}ê°œ)")
    
    # 2. ì§ˆë¬¸ ìœ í˜•ë³„ ë°ì´í„°ì…‹
    for q_type, items in by_type.items():
        type_file = OUTPUT_DIR / f"korean_history_benchmark_balanced_{q_type}_{len(items)}.json"
        with open(type_file, 'w', encoding='utf-8') as f:
            json.dump(items, f, ensure_ascii=False, indent=2)
        print(f"âœ… {q_type} ë°ì´í„°ì…‹ ì €ì¥: {type_file} ({len(items)}ê°œ)")
    
    # 3. í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ë°ì´í„°ì…‹ í†µê³„:")
    print(f"  ì „ì²´: {len(all_benchmark)}ê°œ")
    for q_type, items in by_type.items():
        print(f"  {q_type}: {len(items)}ê°œ ({len(items)/len(all_benchmark)*100:.1f}%)")
    
    return all_benchmark, by_type

if __name__ == "__main__":
    print("ğŸ”„ ê· í˜•ì¡íŒ QA ë°ì´í„°ì…‹ ë³€í™˜ ì‹œì‘...")
    convert_dataset()
    print("âœ… ë³€í™˜ ì™„ë£Œ!")


