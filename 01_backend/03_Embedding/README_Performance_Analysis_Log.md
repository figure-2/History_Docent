# 임베딩 모델 성능 진단 및 전략 수립 로그

**작성 일시:** 2025-11-23 06:10

---

## 1. 가설 (Hypothesis)

### 1.1 문제 인식
- **언제:** 2025-11-23 06:10
- **어디서:** 임베딩 모델 선정 벤치마크 완료 후
- **무엇을:** 선정된 최고 성능 모델(BGE-m3)의 MRR 점수가 0.489로, 일반적인 RAG 시스템 목표치(0.7~0.8)보다 낮게 측정됨
- **왜:** 이 낮은 점수가 프로젝트 진행에 문제가 되는지, 아니면 수용 가능한 수준인지 판단 필요

### 1.2 초기 가설
- **가설 1:** 낮은 점수는 모델 자체의 결함보다는 데이터셋의 난이도(추론형 질문 비중) 때문일 것이다.
- **가설 2:** 현재 점수는 Base Model(파인튜닝 없음)의 한계이며, 리트리버 개선과 리랭커 도입으로 해결 가능할 것이다.
- **가설 3:** 질문 유형별로 성능 편차가 클 것이며, Abstract 질문에서 특히 낮을 것이다.

### 1.3 검증 목표
- 현재 MRR 점수의 원인을 데이터 관점에서 분석
- 현재 상태가 프로젝트 진행에 "괜찮은 수준"인지 판단
- 향후 성능 향상을 위한 구체적인 로드맵 수립

---

## 2. 실험 설계 (Experiment Design)

### 2.1 분석 대상
- **모델:** BGE-m3 (Validation Set 기준 1위)
- **데이터셋:** Validation Set (2,223개)
- **평가 지표:**
  - 전체 MRR: 0.489
  - Keyword MRR: 0.545
  - Context MRR: 0.537
  - Abstract MRR: 0.387

### 2.2 분석 기준 (Criteria)
1. **질문 유형별 편차 분석**
   - 특정 유형에서 성능 하락이 두드러지는가?
   - 각 유형별 MRR 차이가 통계적으로 유의미한가?

2. **Base Model의 한계 평가**
   - 파인튜닝 없이 도달 가능한 임계치인가?
   - 범용 모델로서의 성능이 합리적인가?

3. **개선 가능성 평가**
   - 리트리버 개선 시 예상되는 상승폭
   - 리랭커 도입 시 예상되는 상승폭

---

## 3. 검증 결과 (Validation)

### 3.1 정량 평가 결과

#### [표 1] 질문 유형별 MRR 성능 분석

| 질문 유형 | MRR | Recall@1 | 비고 |
|----------|-----|----------|------|
| Keyword | 0.545 | 0.299 | 가장 높은 성능 |
| Context | 0.537 | 0.283 | Keyword와 유사 |
| Abstract | 0.387 | 0.198 | 가장 낮은 성능 |
| **전체 평균** | **0.489** | **0.260** | - |

#### 성능 편차 분석
- **Keyword vs Abstract 차이:** 0.158 (29% 차이)
- **Context vs Abstract 차이:** 0.150 (28% 차이)
- **Keyword vs Context 차이:** 0.008 (1.5% 차이)

**결론:** Abstract 질문에서 성능이 크게 하락하며, 이로 인해 전체 평균이 낮아짐

### 3.2 실패 분석 (Failure Analysis)

#### 1. 데이터셋 난이도 (High Difficulty)
- **원인:** Abstract(추상적) 질문의 MRR이 0.387로 전체 평균을 크게 깎아먹고 있음
- **분석:**
  - 단순 키워드 매칭이 아닌, 문맥을 이해하고 추론해야 하는 질문이 전체의 66%(Context+Abstract)를 차지
  - Dense Retrieval(임베딩 검색)만으로는 추론형 질문을 해결하기 어려움
- **수치적 근거:**
  - Abstract MRR: 0.387 (Keyword 대비 29% 낮음)
  - Abstract Recall@1: 0.198 (Keyword 대비 34% 낮음)

#### 2. 유사 청크 간섭 (Hard Negatives)
- **원인:** 역사 도메인 특성상 '세종대왕', '임진왜란' 등 동일 키워드를 가진 청크가 다수 존재
- **분석:**
  - 정답 청크가 아니더라도 내용이 매우 유사한 청크가 1등으로 나오면 MRR은 떨어짐
  - 하지만 사용자 입장에서는 유용한 정보일 수 있음 (False Negative가 아닐 수 있음)
- **수치적 근거:**
  - Recall@5: 0.804 (상위 5개 중 정답 포함률 80%)
  - Recall@3: 0.747 (상위 3개 중 정답 포함률 75%)

#### 3. 파인튜닝 부재
- **원인:** 현재 BGE-m3는 범용 모델이며, 한국 역사 도메인에 특화되지 않음
- **분석:**
  - 범용 모델로 0.5에 가까운 점수는 파인튜닝 전 단계로서 충분함
  - 도메인 특화 학습을 통해 성능 향상 가능
- **수치적 근거:**
  - 현재 MRR: 0.489 (Base Model)
  - 예상 향상: 파인튜닝 후 0.6~0.7 수준 기대

### 3.3 정성 평가 (Qualitative Analysis)

#### 성공 사례
- **Keyword 질문:** MRR 0.545로 준수한 성능
- **Context 질문:** MRR 0.537로 Keyword와 유사한 수준
- **Recall@5:** 0.804로 상위 5개 중 정답 포함률이 높음

#### 실패 사례
- **Abstract 질문:** MRR 0.387로 가장 낮은 성능
- **Recall@1:** 0.260으로 정답이 1등으로 나올 확률이 낮음

### 3.4 비교 분석

#### 이전 벤치마크와의 비교
- **이전 (전체 데이터셋, keyword 편향):**
  - Ko-SBERT: MRR 0.638 (1위)
  - BGE-m3: MRR 0.441 (3위)
- **현재 (Validation Set, 균형잡힌):**
  - BGE-m3: MRR 0.489 (1위)
  - Ko-SBERT: MRR 0.393 (4위)

**분석:**
- 균형잡힌 데이터셋에서는 BGE-m3가 더 우수
- 이전 선택(BGE-m3)이 올바른 결정이었음이 증명됨

---

## 4. 의사결정 (Conclusion & Pivot)

### 4.1 최종 판단: "진행 (Go)"

**결론:** 현재 점수(0.489)는 **"Base Model로서 수용 가능한 시작점"**이다.

**근거:**
1. Keyword 및 Context 질문에서는 준수한 성능(0.54~0.55)을 보임
2. 낮은 점수는 모델의 결함이 아닌, 어려운 태스크(Abstract)에 기인
3. Recall@5가 0.804로 높아, 상위 5개 중 정답을 찾을 확률이 높음
4. 벡터 DB는 이미 BGE-m3로 구축되어 있어 재구축 불필요

### 4.2 계획 유지 사항

1. **임베딩 모델 유지**
   - BGE-m3를 최종 선택으로 확정
   - 벡터 DB 재구축 불필요 (이미 BGE-m3로 구축됨)

2. **리트리버 선정 진행**
   - BM25, Vector, Hybrid 비교 평가
   - Validation Set으로 평가

### 4.3 성능 향상 전략 (Roadmap)

#### 1단계: 리트리버 개선 (Hybrid Search)
- **목표:** BM25를 도입하여 희귀 키워드 매칭 보완
- **예상 효과:** MRR 0.48 → 0.6 수준 향상
- **시기:** 즉시 진행

#### 2단계: 리랭커 도입 (Reranking)
- **목표:** 1차 검색된 후보군을 정밀 재정렬하여 정답을 1순위로 올림
- **예상 효과:** MRR 0.6 → 0.8+ 수준으로 비약적 상승
- **시기:** 리트리버 선정 이후

#### 3단계: 임베딩 파인튜닝
- **목표:** Train Set(6,686개)을 사용하여 도메인 특화 학습
- **예상 효과:** MRR 0.6 → 0.7+ 수준 향상
- **시기:** 리트리버 선정 이후

### 4.4 다음 Action Item

1. **[즉시]** 리트리버 선정 벤치마크 (BM25 vs Vector vs Hybrid) 실행
2. **[이후]** 리랭커 모델 선정 및 파인튜닝 준비

---

## 5. 참고 자료

- 임베딩 모델 선정 로그: `README_Embedding_Selection_Log.md`
- 벤치마크 결과: `results/benchmark_results_validation_set.csv`
- 벡터 DB 전략: `../04_VectorDB/SPEC_VectorDB_Strategy.md`

---

**작성자:** AI Assistant & Pencilfoxs  
**검토:** 완료  
**상태:** 승인됨  
**최종 업데이트:** 2025-11-23 06:10


