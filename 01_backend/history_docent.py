#!/usr/bin/env python3
"""
History Docent - í†µí•© RAG ì‹œìŠ¤í…œ
- ê²€ìƒ‰(Retrieval): BM25 + Reranker (05_Retrieval_Optimization ëª¨ë“ˆ ì‚¬ìš©)
- ìƒì„±(Generation): Bllossom-8B LLM
"""
import sys
import os
import time
import torch

# ìƒìœ„ í´ë”(05_Retrieval_Optimization)ì—ì„œ ëª¨ë“ˆì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€ (import ì „ì— ì‹¤í–‰)
sys.path.append(os.path.join(os.path.dirname(__file__), "05_Retrieval_Optimization"))

from retrieval_system import RetrievalSystem
from vllm import LLM, SamplingParams  # transformers ëŒ€ì‹  vLLM ì‚¬ìš©

class HistoryDocent:
    def __init__(self):
        self.retrieval_system = RetrievalSystem()
        self.model_id = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
        self.model = None
        self.history = []
        
        # vLLM Sampling Parameters ì„¤ì •
        self.sampling_params = SamplingParams(
            temperature=0.1,  # ì°½ì˜ì„± ì¡°ì ˆ (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ë¨)
            top_p=0.9,
            max_tokens=512,
            stop=["<|end_of_text|>", "<|eot_id|>"]  # ì¢…ë£Œ í† í° ì„¤ì •
        )

    def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”: ê²€ìƒ‰ ì‹œìŠ¤í…œ ë° vLLM ëª¨ë¸ ë¡œë“œ"""
        if self.model is not None:
            print("â„¹ï¸ ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return

        print("ğŸš€ History Docent ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.retrieval_system.initialize()
        
        # 2. vLLM ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ¤– vLLM ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_id}")
        # GPU ë©”ëª¨ë¦¬ ì ìœ ìœ¨ì„ 0.6ìœ¼ë¡œ ì œí•œ (ê²€ìƒ‰ ì‹œìŠ¤í…œë„ GPU ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ìœ  í™•ë³´)
        self.model = LLM(
            model=self.model_id, 
            dtype="float16",
            gpu_memory_utilization=0.6,  # 0.9 â†’ 0.6ìœ¼ë¡œ ë‚®ì¶¤ (ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ê³ ë ¤)
            tensor_parallel_size=1  # ë‹¨ì¼ GPU ì‚¬ìš©
        )
        print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")

    def generate_prompt(self, query: str, contexts: list) -> str:
        """RAG í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # contextsëŠ” RetrievalResult ê°ì²´ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
        context_str = "\n\n".join([f"- {ctx.text}" for ctx in contexts])
        
        # Bllossom ëª¨ë¸ í”„ë¡¬í”„íŠ¸ í¬ë§· ì¤€ìˆ˜
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ í•œêµ­ì‚¬ ì „ë¬¸ê°€ 'History Docent'ì…ë‹ˆë‹¤. ì•„ë˜ [ê²€ìƒ‰ ê²°ê³¼]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
[ê²€ìƒ‰ ê²°ê³¼]ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³ , ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.

[ê²€ìƒ‰ ê²°ê³¼]
{context_str}<|eot_id|><|start_header_id|>user<|end_header_id|>

{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        return prompt

    def chat(self, query: str) -> dict:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (RAG)"""
        if not self.model:
            self.initialize()
            
        start_time = time.time()
        
        # 1. ë¬¸ì„œ ê²€ìƒ‰
        print(f"ğŸ” ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘: {query}")
        search_results = self.retrieval_system.search(query, final_k=3)
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.generate_prompt(query, search_results)
        
        # 3. ë‹µë³€ ìƒì„± (vLLM)
        print("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘...")
        outputs = self.model.generate([prompt], self.sampling_params)
        generated_text = outputs[0].outputs[0].text.strip()
        
        elapsed_time = time.time() - start_time
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

        return {
            "answer": generated_text,
            "sources": [res.text[:100] + "..." for res in search_results],
            "latency": round(elapsed_time, 2)
        }

if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    docent = HistoryDocent()
    docent.initialize()
    
    test_query = "ì†ê¸°ì • ì„ ìˆ˜ëŠ” ì–´ë–¤ ì˜¬ë¦¼í”½ì—ì„œ ê¸ˆë©”ë‹¬ì„ ë•„ë‚˜ìš”?"
    result = docent.chat(test_query)
    
    print("\n" + "="*50)
    print(f"ì§ˆë¬¸: {test_query}")
    print(f"ë‹µë³€: {result['answer']}")
    print(f"ì‹œê°„: {result['latency']}ì´ˆ")
    print("="*50)

