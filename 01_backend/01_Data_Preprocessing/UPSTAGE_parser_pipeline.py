#!/usr/bin/env python3
"""
ì—…ìŠ¤í…Œì´ì§€ parser(2025 ìŠ¤í™) + í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” CLI ìŠ¤í¬ë¦½íŠ¸.

ê¸°ëŠ¥ ìš”ì•½
---------
1. ì§€ì •í•œ PDFë¥¼ batch-size ë‹¨ìœ„ë¡œ ë¶„í• 
2. ê° ë¶„í• ë³¸ì„ Upstage document-digitization APIë¡œ ë¶„ì„í•˜ì—¬ JSON ìƒì„±
3. ê¸°ì¡´ 2025 íŒŒì´í”„ë¼ì¸(`ëˆ„ë½_ë°ì´í„°.py`) ë¡œì§ì„ ì´ìš©í•´ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€/í‘œë¥¼ ì •ë¦¬í•˜ê³ 
   Geminië¥¼ í™œìš©í•œ ìš”ì•½ê³¼ Markdownì„ ìƒì„±
4. `document_analysis_results.*`, `text_summaries.json`, `image_summaries.json`,
   `table_markdowns.json` ë“± ì‚°ì¶œë¬¼ì„ ì €ì¥

í™˜ê²½ ë³€ìˆ˜
---------
* `UPSTAGE_API_KEY` : Upstage Document API Key (í•„ìˆ˜)
* `GEMINI_API_KEY`  : Google Gemini API Key (ë©€í‹°ëª¨ë‹¬ ìš”ì•½ ì‹œ í•„ìˆ˜)
  - ê¸°ë³¸ì ìœ¼ë¡œ `.env2` íŒŒì¼ì„ ë¡œë“œí•˜ë©°, `--env-file` ì¸ìë¡œ ë‹¤ë¥¸ íŒŒì¼ì„ ì§€ì •í•  ìˆ˜ ìˆìŒ

ì‚¬ìš© ì˜ˆì‹œ
---------
python History_Docent/01_Data_Preprocessing/UPSTAGE_parser_pipeline.py \\
    --base-dir History_Docent/1_Data_Preprocessing/ì¡°ì„ í¸_2025 \\
    --pdf data/ë²Œê±°ë²—ì€í•œêµ­ì‚¬-ì¡°ì„ í¸.pdf
"""

from __future__ import annotations

import argparse
import importlib.util
import json
import os
import sys
from pathlib import Path
from typing import Iterable, List

import pymupdf
import requests
from dotenv import load_dotenv
import re
import shutil
import unicodedata

# í•œê¸€ íŒŒì¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ë²ˆì—­ ê¸°ëŠ¥ ì œê±°


# ---------------------------------------------------------------------------
# í™˜ê²½ ë³€ìˆ˜ & ëª¨ë“ˆ ë¡œë“œ
# ---------------------------------------------------------------------------

def load_env_file(env_path: Path | None) -> None:
    """ì§€ì •ëœ env íŒŒì¼ì„ ìš°ì„  ë¡œë“œí•˜ê³ , ì—†ìœ¼ë©´ ê¸°ë³¸ .env íƒìƒ‰."""
    if env_path and env_path.exists():
        load_dotenv(env_path)
    else:
        load_dotenv()  # fallback (.env ë“±)


def require_env(name: str) -> str:
    value = os.environ.get(name)
    if not value:
        raise EnvironmentError(f"í™˜ê²½ ë³€ìˆ˜ {name} ì´(ê°€) ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return value


def load_parser_module(base_dir: Path):
    """ëˆ„ë½_ë°ì´í„°.py ëª¨ë“ˆì„ importlibìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    target = base_dir / "ëˆ„ë½_ë°ì´í„°.py"
    if not target.exists():
        raise FileNotFoundError(f"parser module not found: {target}")

    spec = importlib.util.spec_from_file_location("parser2025", target)
    module = importlib.util.module_from_spec(spec)
    assert spec and spec.loader
    spec.loader.exec_module(module)  # type: ignore[attr-defined]
    return module


# ---------------------------------------------------------------------------
# Upstage API í˜¸ì¶œ & ë°ì´í„° ì¤€ë¹„
# ---------------------------------------------------------------------------

UPSTAGE_ENDPOINT = "https://api.upstage.ai/v1/document-digitization"


def split_pdf(pdf_path: Path, batch_size: int) -> list[Path]:
    """PDFë¥¼ batch-size ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  ê²½ë¡œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    data_dir = pdf_path.parent
    pattern = f"{pdf_path.stem}_*.pdf"
    split_files = sorted(data_dir.glob(pattern))
    if split_files:
        return split_files

    print("âš ï¸  ë¶„í•  PDFê°€ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    doc = pymupdf.open(pdf_path)
    try:
        num_pages = len(doc)
        for start in range(0, num_pages, batch_size):
            end = min(start + batch_size, num_pages) - 1
            out_path = data_dir / f"{pdf_path.stem}_{start:04d}_{end:04d}.pdf"
            with pymupdf.open() as out_doc:
                out_doc.insert_pdf(doc, from_page=start, to_page=end)
                out_doc.save(out_path)
            split_files.append(out_path)
            print("ìƒì„±:", out_path)
    finally:
        doc.close()
    return split_files


def call_upstage(pdf_file: Path, api_key: str, timeout: int = 300) -> Path:
    """ë‹¨ì¼ ë¶„í•  PDFë¥¼ Upstage APIë¡œ ë¶„ì„í•´ JSONì„ ìƒì„±í•©ë‹ˆë‹¤."""
    headers = {"Authorization": f"Bearer {api_key}"}
    data = {
        "model": "document-parse",
        "ocr": "force",
        "coordinates": True,
        "chart_recognition": True,
        "merge_multipage_tables": True,
        "output_formats": ["html", "markdown"],
        "base64_encoding": ["figure", "table"],
    }
    files = {"document": open(pdf_file, "rb")}
    try:
        response = requests.post(
            UPSTAGE_ENDPOINT,
            headers=headers,
            data=data,
            files=files,
            timeout=timeout,
        )
    except requests.RequestException as exc:
        raise RuntimeError(f"Upstage ìš”ì²­ ì‹¤íŒ¨: {exc}") from exc
    finally:
        files["document"].close()

    if response.status_code != 200:
        try:
            detail = response.json()
        except Exception:
            detail = response.text
        raise RuntimeError(
            f"Upstage ë¶„ì„ ì‹¤íŒ¨ ({response.status_code}): {detail}"
        )

    output_file = pdf_file.with_suffix(".json")
    with open(output_file, "w", encoding="utf-8") as fout:
        json.dump(response.json(), fout, ensure_ascii=False)
    print(f"âœ… Upstage ë¶„ì„ ì™„ë£Œ: {output_file}")
    return output_file


def ensure_layout_json(
    split_files: Iterable[Path],
    api_key: str,
    force: bool = False,
) -> list[Path]:
    """ë¶„í•  PDF ëª©ë¡ì— ëŒ€í•´ JSONì´ ì—†ìœ¼ë©´ Upstage ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    json_files: list[Path] = []
    for pdf in split_files:
        json_path = pdf.with_suffix(".json")
        if json_path.exists() and not force:
            print(f"ê¸°ì¡´ JSON ì‚¬ìš©: {json_path}")
        else:
            json_path = call_upstage(pdf, api_key)
        json_files.append(json_path)
    return json_files


# ---------------------------------------------------------------------------
# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# ---------------------------------------------------------------------------

def run_pipeline(
    parser_module,
    pdf_path: Path,
    split_files: list[Path],
    batch_size: int,
    skip_gemini: bool,
):
    """ëˆ„ë½_ë°ì´í„°.pyì˜ í•¨ìˆ˜ë“¤ì„ ì´ìš©í•´ í›„ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    GraphState = parser_module.GraphState  # type: ignore[attr-defined]

    with pymupdf.open(pdf_path) as doc:
        num_pages = len(doc)

    state = GraphState(
        filepath=str(pdf_path),
        batch_size=batch_size,
        split_filepaths=[str(p) for p in split_files],
        page_numbers=list(range(num_pages)),
    )

    print("ğŸ“„ JSON ë³µì›...")
    state.update(parser_module.restore_state_from_files(state))

    print("ğŸ“Š í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ...")
    state.update(parser_module.extract_page_metadata(state))

    print("ğŸ” í˜ì´ì§€ ìš”ì†Œ ì¶”ì¶œ...")
    state.update(parser_module.extract_page_elements(state))

    print("ğŸ“ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±...")
    state.update(parser_module.extract_page_text(state))

    print("ğŸ§¾ í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±...")
    state.update(parser_module.create_text_summary(state))

    print("ğŸ–¼ï¸ ì´ë¯¸ì§€/í‘œ í¬ë¡­...")
    state.update(parser_module.crop_image(state))
    state.update(parser_module.crop_table(state))

    print("ğŸ“¦ ìš”ì•½ ë°°ì¹˜ ìƒì„±...")
    state.update(parser_module.create_image_summary_data_batches(state))
    state.update(parser_module.create_table_summary_data_batches(state))

    if skip_gemini:
        print("âš ï¸  --skip-gemini ì˜µì…˜ìœ¼ë¡œ ë©€í‹°ëª¨ë‹¬ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        print("ğŸ¤– Gemini ì´ë¯¸ì§€ ìš”ì•½...")
        state.update(parser_module.create_image_summary(state))

        print("ğŸ¤– Gemini í…Œì´ë¸” ìš”ì•½...")
        state.update(parser_module.create_table_summary(state))

        print("ğŸ“ Gemini í…Œì´ë¸” Markdown ë³€í™˜...")
        state.update(parser_module.create_table_markdown(state))

    print("ğŸ’¾ ê²°ê³¼ ì €ì¥...")
    parser_module.save_results(state)
    print("âœ… ì™„ë£Œ! ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Upstage + Gemini í†µí•© íŒŒì´í”„ë¼ì¸")
    parser.add_argument(
        "--template-dir",
        default="History_Docent/1_Data_Preprocessing/ì¡°ì„ í¸_2025",
        help="íŒŒì´í”„ë¼ì¸ í…œí”Œë¦¿ ë””ë ‰í† ë¦¬ (ëˆ„ë½_ë°ì´í„°.py í¬í•¨)",
    )
    parser.add_argument(
        "--pdf",
        default=None,
        help="ë‹¨ì¼ PDF ê²½ë¡œ (base-dir ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ í—ˆìš©)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="PDF ë¶„í•  ì‹œ ì‚¬ìš©í•  í˜ì´ì§€ ìˆ˜",
    )
    parser.add_argument(
        "--env-file",
        default=".env2",
        help="í™˜ê²½ ë³€ìˆ˜ ë¡œë“œë¥¼ ìœ„í•œ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: .env2)",
    )
    parser.add_argument(
        "--force-layout",
        action="store_true",
        help="ê¸°ì¡´ JSONì´ ìˆì–´ë„ Upstage ë¶„ì„ì„ ë‹¤ì‹œ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--input-dir",
        default="History_Docent/PDF_history",
        help="ë³µìˆ˜ PDF ì²˜ë¦¬ ì‹œ ì‚¬ìš©í•  ë””ë ‰í† ë¦¬",
    )
    parser.add_argument(
        "--output-root",
        default="History_Docent/01_Data_Preprocessing",
        help="ê²°ê³¼ ì €ì¥ ë£¨íŠ¸ í´ë”",
    )
    parser.add_argument(
        "--skip-gemini",
        action="store_true",
        help="Gemini ì´ë¯¸ì§€/í‘œ ìš”ì•½ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.",
    )
    return parser


def compute_target_paths(pdf_path: Path, output_root: Path, translator=None) -> Path:
    """ì›ë³¸ íŒŒì¼ëª…ì„ ê·¸ëŒ€ë¡œ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤ (í•œê¸€ í¬í•¨)."""
    # í™•ì¥ìë¥¼ ì œê±°í•œ íŒŒì¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    folder_name = pdf_path.stem
    return output_root / folder_name


def main(argv: list[str] | None = None):
    args = build_parser().parse_args(argv)

    load_env_file(Path(args.env_file) if args.env_file else None)
    upstage_key = require_env("UPSTAGE_API_KEY")
    if not args.skip_gemini:
        require_env("GEMINI_API_KEY")

    pdfs: list[Path]
    if args.pdf:
        pdf_path = Path(args.pdf).resolve()
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF not found: {pdf_path}")
        pdfs = [pdf_path]
    else:
        input_dir = Path(args.input_dir).resolve()
        if not input_dir.exists():
            raise FileNotFoundError(f"ì…ë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        pdfs = sorted(input_dir.glob("*.pdf"))
        if not pdfs:
            raise FileNotFoundError("ì…ë ¥ ë””ë ‰í† ë¦¬ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤.")

    template_dir = Path(args.template_dir).resolve()
    parser_module = load_parser_module(template_dir)

    for pdf_path in pdfs:
        target_base = compute_target_paths(
            pdf_path,
            Path(args.output_root).resolve(),
        )
        
        # ì´ë¯¸ ì™„ë£Œëœ PDFëŠ” ê±´ë„ˆë›°ê¸°
        result_file = target_base / "document_analysis_results.json"
        if result_file.exists():
            print(f"â­ï¸  ì´ë¯¸ ì™„ë£Œëœ PDF ê±´ë„ˆë›°ê¸°: {pdf_path.name} (ê²°ê³¼ íŒŒì¼ ì¡´ì¬: {result_file})")
            continue
        
        data_dir = target_base / "data"
        data_dir.mkdir(parents=True, exist_ok=True)

        local_pdf = data_dir / pdf_path.name
        if not local_pdf.exists() or os.path.getmtime(pdf_path) > os.path.getmtime(local_pdf):
            shutil.copy2(pdf_path, local_pdf)

        original_cwd = Path.cwd()
        try:
            os.chdir(target_base)
            split_files = split_pdf(local_pdf, args.batch_size)
            ensure_layout_json(split_files, upstage_key, force=args.force_layout)
            run_pipeline(
                parser_module=parser_module,
                pdf_path=local_pdf,
                split_files=split_files,
                batch_size=args.batch_size,
                skip_gemini=args.skip_gemini,
            )
        finally:
            os.chdir(original_cwd)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.exit("ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

