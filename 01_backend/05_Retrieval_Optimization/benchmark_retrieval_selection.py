#!/usr/bin/env python3
"""
ë¦¬íŠ¸ë¦¬ë²„ ì„ ì • ë²¤ì¹˜ë§ˆí¬ (Validation Set ì‚¬ìš©)
- ëª©ì : í•œêµ­ì‚¬ RAGì— ìµœì í™”ëœ ë¦¬íŠ¸ë¦¬ë²„ ì „ëµ ì„ ì •
- ë¹„êµêµ°:
  1. BM25 Only (í‚¤ì›Œë“œ ê¸°ë°˜)
  2. Vector Only (BGE-m3 ì„ë² ë”©)
  3. Hybrid Weighted (Vector + BM25 ê°€ì¤‘ì¹˜)
  4. Hybrid RRF (Reciprocal Rank Fusion)
- í‰ê°€ ì§€í‘œ: MRR, Recall@1, Recall@3, Recall@5
- ë°ì´í„°ì…‹: Validation Set (2,223ê°œ) - ê³¼ì í•© ë°©ì§€
"""

import json
import time
import numpy as np
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Any
from collections import defaultdict
import torch
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from rank_bm25 import BM25Okapi
from konlpy.tag import Okt
import re

# -----------------------------------------------------------------------------
# ì„¤ì •
# -----------------------------------------------------------------------------
BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
BENCHMARK_DATA = BASE_DIR / "03_Embedding/data/validation_set_20.json"
CHUNK_FILE = BASE_DIR / "02_Chunking/output/all_chunks.json"
VECTORDB_DIR = BASE_DIR / "04_VectorDB/chroma_db"
COLLECTION_NAME = "korean_history_chunks"
RESULTS_DIR = BASE_DIR / "05_Retrieval_Optimization/results"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# ì„ë² ë”© ëª¨ë¸
EMBEDDING_MODEL = "BAAI/bge-m3"

# -----------------------------------------------------------------------------
# ë°ì´í„° ë¡œë“œ
# -----------------------------------------------------------------------------
def load_benchmark_data():
    """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ"""
    print(f"ğŸ“‚ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ: {BENCHMARK_DATA}")
    with open(BENCHMARK_DATA, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"   âœ… {len(data)}ê°œ ì§ˆë¬¸ ë¡œë“œ ì™„ë£Œ")
    return data

def load_chunks():
    """ì „ì²´ ì²­í¬ ë°ì´í„° ë¡œë“œ"""
    print(f"ğŸ“‚ ì²­í¬ ë°ì´í„° ë¡œë“œ: {CHUNK_FILE}")
    with open(CHUNK_FILE, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    
    # chunk_idë¥¼ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
    chunk_dict = {chunk['chunk_id']: chunk for chunk in chunks}
    print(f"   âœ… {len(chunk_dict)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    return chunk_dict

def load_vectordb():
    """ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
    print(f"ğŸ“‚ ë²¡í„° DB ë¡œë“œ: {VECTORDB_DIR}")
    client = chromadb.PersistentClient(
        path=str(VECTORDB_DIR),
        settings=Settings(anonymized_telemetry=False)
    )
    collection = client.get_collection(name=COLLECTION_NAME)
    print(f"   âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ ({collection.count()}ê°œ ë¬¸ì„œ)")
    return collection

# -----------------------------------------------------------------------------
# ë¦¬íŠ¸ë¦¬ë²„ í´ë˜ìŠ¤
# -----------------------------------------------------------------------------
class BM25Retriever:
    """BM25 í‚¤ì›Œë“œ ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„ (OKT í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©)"""
    def __init__(self, dataset: List[Dict], chunk_dict: Dict):
        self.chunk_dict = chunk_dict
        self.chunk_ids = list(chunk_dict.keys())
        
        # OKT í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        print("   ğŸ”¤ OKT í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        self.tokenizer = Okt()
        
        # BM25ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„ (OKT í˜•íƒœì†Œ ë¶„ì„)
        print("   ğŸ§® BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘ (OKT í† í¬ë‚˜ì´ì§•)...")
        texts = []
        for chunk_id in tqdm(self.chunk_ids, desc="   í† í¬ë‚˜ì´ì§•", leave=False):
            text = chunk_dict[chunk_id]['text']
            # OKT í˜•íƒœì†Œ ë¶„ì„ (ì–´ê°„ ì¶”ì¶œ í¬í•¨)
            tokens = self.tokenizer.morphs(text, stem=True)
            # ë¹ˆ í† í° ì œê±°
            tokens = [t for t in tokens if t.strip()]
            texts.append(tokens)
        
        self.bm25 = BM25Okapi(texts)
        print("   âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ (OKT ì‚¬ìš©)")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """BM25 ê²€ìƒ‰"""
        # ì¿¼ë¦¬ í† í¬ë‚˜ì´ì§• (OKT ì‚¬ìš©)
        query_tokens = self.tokenizer.morphs(query, stem=True)
        query_tokens = [t for t in query_tokens if t.strip()]
        
        # BM25 ì ìˆ˜ ê³„ì‚°
        scores = self.bm25.get_scores(query_tokens)
        
        # ìƒìœ„ Kê°œ ì„ íƒ
        top_indices = np.argsort(scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            chunk_id = self.chunk_ids[idx]
            results.append({
                'chunk_id': chunk_id,
                'text': self.chunk_dict[chunk_id]['text'],
                'score': float(scores[idx])
            })
        
        return results

class VectorRetriever:
    """ë²¡í„° ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„ (BGE-m3)"""
    def __init__(self, collection, model_name: str = EMBEDDING_MODEL):
        self.collection = collection
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ğŸ“ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name} ({self.device})")
        self.model = SentenceTransformer(model_name, device=self.device)
        print("   âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.model.encode(
            query,
            normalize_embeddings=True,
            show_progress_bar=False
        ).tolist()
        
        # ChromaDB ê²€ìƒ‰
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        # ê²°ê³¼ ë³€í™˜
        ret = []
        if results['ids'] and len(results['ids'][0]) > 0:
            for i, chunk_id in enumerate(results['ids'][0]):
                ret.append({
                    'chunk_id': chunk_id,
                    'text': results['documents'][0][i],
                    'score': 1.0 - results['distances'][0][i]  # ê±°ë¦¬ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜
                })
        
        return ret

class HybridRetriever:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ (BM25 + Vector)"""
    def __init__(self, bm25_retriever: BM25Retriever, vector_retriever: VectorRetriever):
        self.bm25 = bm25_retriever
        self.vector = vector_retriever
    
    def search_weighted(self, query: str, top_k: int = 5, 
                       vector_weight: float = 0.6, bm25_weight: float = 0.4) -> List[Dict]:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        # ê°ê° ê²€ìƒ‰
        bm25_results = self.bm25.search(query, top_k=top_k * 2)
        vector_results = self.vector.search(query, top_k=top_k * 2)
        
        # ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
        scores = {}
        
        # BM25 ì ìˆ˜ ì •ê·œí™”
        if bm25_results:
            max_bm25 = max(r['score'] for r in bm25_results)
            min_bm25 = min(r['score'] for r in bm25_results)
            bm25_range = max_bm25 - min_bm25 if max_bm25 != min_bm25 else 1.0
            
            for r in bm25_results:
                chunk_id = r['chunk_id']
                normalized = (r['score'] - min_bm25) / bm25_range if bm25_range > 0 else 0.5
                scores[chunk_id] = scores.get(chunk_id, 0) + bm25_weight * normalized
        
        # Vector ì ìˆ˜ ì •ê·œí™”
        if vector_results:
            max_vector = max(r['score'] for r in vector_results)
            min_vector = min(r['score'] for r in vector_results)
            vector_range = max_vector - min_vector if max_vector != min_vector else 1.0
            
            for r in vector_results:
                chunk_id = r['chunk_id']
                normalized = (r['score'] - min_vector) / vector_range if vector_range > 0 else 0.5
                scores[chunk_id] = scores.get(chunk_id, 0) + vector_weight * normalized
        
        # ìƒìœ„ Kê°œ ì„ íƒ
        sorted_chunks = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        results = []
        for chunk_id, score in sorted_chunks:
            # í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            text = self.bm25.chunk_dict.get(chunk_id, {}).get('text', '')
            results.append({
                'chunk_id': chunk_id,
                'text': text,
                'score': score
            })
        
        return results
    
    def search_rrf(self, query: str, top_k: int = 5, k: int = 60) -> List[Dict]:
        """RRF (Reciprocal Rank Fusion) ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        # ê°ê° ê²€ìƒ‰
        bm25_results = self.bm25.search(query, top_k=k)
        vector_results = self.vector.search(query, top_k=k)
        
        # RRF ì ìˆ˜ ê³„ì‚°
        rrf_scores = {}
        
        # BM25 ìˆœìœ„ ê¸°ë°˜ RRF ì ìˆ˜
        for rank, r in enumerate(bm25_results, 1):
            chunk_id = r['chunk_id']
            rrf_scores[chunk_id] = rrf_scores.get(chunk_id, 0) + 1.0 / (k + rank)
        
        # Vector ìˆœìœ„ ê¸°ë°˜ RRF ì ìˆ˜
        for rank, r in enumerate(vector_results, 1):
            chunk_id = r['chunk_id']
            rrf_scores[chunk_id] = rrf_scores.get(chunk_id, 0) + 1.0 / (k + rank)
        
        # ìƒìœ„ Kê°œ ì„ íƒ
        sorted_chunks = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        results = []
        for chunk_id, score in sorted_chunks:
            text = self.bm25.chunk_dict.get(chunk_id, {}).get('text', '')
            results.append({
                'chunk_id': chunk_id,
                'text': text,
                'score': score
            })
        
        return results

# -----------------------------------------------------------------------------
# í‰ê°€ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def calculate_metrics(results: List[Dict], gold_chunk_id: str):
    """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ í‰ê°€ ì§€í‘œ ê³„ì‚°"""
    chunk_ids = [r['chunk_id'] for r in results]
    
    # Recall@K
    recall_1 = 1 if gold_chunk_id in chunk_ids[:1] else 0
    recall_3 = 1 if gold_chunk_id in chunk_ids[:3] else 0
    recall_5 = 1 if gold_chunk_id in chunk_ids[:5] else 0
    
    # MRR (Mean Reciprocal Rank)
    try:
        rank = chunk_ids.index(gold_chunk_id) + 1
        mrr = 1.0 / rank
    except ValueError:
        mrr = 0.0
        
    return recall_1, recall_3, recall_5, mrr

def evaluate_retriever(name: str, retriever, dataset: List[Dict], 
                      search_func, search_kwargs: Dict = None):
    """ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€"""
    print(f"\nğŸš€ [{name}] í‰ê°€ ì¤‘...")
    
    if search_kwargs is None:
        search_kwargs = {}
    
    metrics = {
        'recall_1': 0, 'recall_3': 0, 'recall_5': 0, 
        'mrr': 0, 'time': 0
    }
    
    type_metrics = defaultdict(lambda: {
        'recall_1': 0, 'recall_3': 0, 'recall_5': 0, 'mrr': 0, 'count': 0
    })
    
    failure_cases = []
    
    for item in tqdm(dataset, desc=f"   [{name}] ê²€ìƒ‰", leave=False):
        query = item['query']
        gold_id = item['chunk_id']
        q_type = item.get('type', 'unknown')
        
        start_time = time.time()
        results = search_func(query, **search_kwargs)
        search_time = time.time() - start_time
        
        r1, r3, r5, mrr = calculate_metrics(results, gold_id)
        
        metrics['recall_1'] += r1
        metrics['recall_3'] += r3
        metrics['recall_5'] += r5
        metrics['mrr'] += mrr
        metrics['time'] += search_time
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ì§‘ê³„
        type_metrics[q_type]['recall_1'] += r1
        type_metrics[q_type]['recall_3'] += r3
        type_metrics[q_type]['recall_5'] += r5
        type_metrics[q_type]['mrr'] += mrr
        type_metrics[q_type]['count'] += 1
        
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ê¸°ë¡
        if r1 == 0:
            failure_cases.append({
                'query': query,
                'gold_id': gold_id,
                'top_1_id': results[0]['chunk_id'] if results else None,
                'type': q_type
            })
    
    n = len(dataset)
    result = {
        'name': name,
        'MRR': metrics['mrr'] / n,
        'Recall@1': metrics['recall_1'] / n,
        'Recall@3': metrics['recall_3'] / n,
        'Recall@5': metrics['recall_5'] / n,
        'Latency(ms)': (metrics['time'] / n) * 1000,
        'failure_cases': failure_cases[:10]  # ìƒìœ„ 10ê°œë§Œ ì €ì¥
    }
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ ë©”íŠ¸ë¦­ ì¶”ê°€
    for q_type, type_data in type_metrics.items():
        count = type_data['count']
        if count > 0:
            result[f'{q_type}_MRR'] = type_data['mrr'] / count
            result[f'{q_type}_R@1'] = type_data['recall_1'] / count
            result[f'{q_type}_R@3'] = type_data['recall_3'] / count
            result[f'{q_type}_R@5'] = type_data['recall_5'] / count
    
    return result

# -----------------------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# -----------------------------------------------------------------------------
def main():
    print("=" * 80)
    print("ğŸš€ ë¦¬íŠ¸ë¦¬ë²„ ì„ ì • ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (Validation Set)")
    print("=" * 80)
    
    # ë°ì´í„° ë¡œë“œ
    dataset = load_benchmark_data()
    chunk_dict = load_chunks()
    collection = load_vectordb()
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ í†µê³„
    type_counts = defaultdict(int)
    for item in dataset:
        type_counts[item.get('type', 'unknown')] += 1
    print("\nğŸ“Š ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„í¬:")
    for q_type, count in sorted(type_counts.items()):
        print(f"   - {q_type}: {count}ê°œ ({count/len(dataset)*100:.1f}%)")
    
    # ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
    print("\nğŸ”§ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì¤‘...")
    bm25_retriever = BM25Retriever(dataset, chunk_dict)
    vector_retriever = VectorRetriever(collection)
    hybrid_retriever = HybridRetriever(bm25_retriever, vector_retriever)
    
    # í‰ê°€ ì‹¤í–‰
    results = []
    
    # 1. BM25 Only
    result_bm25 = evaluate_retriever(
        "BM25 Only",
        bm25_retriever,
        dataset,
        bm25_retriever.search,
        {'top_k': 5}
    )
    results.append(result_bm25)
    
    # 2. Vector Only
    result_vector = evaluate_retriever(
        "Vector Only",
        vector_retriever,
        dataset,
        vector_retriever.search,
        {'top_k': 5}
    )
    results.append(result_vector)
    
    # 3. Hybrid Weighted (0.6 Vector + 0.4 BM25)
    result_hybrid_weighted = evaluate_retriever(
        "Hybrid Weighted (0.6V+0.4B)",
        hybrid_retriever,
        dataset,
        hybrid_retriever.search_weighted,
        {'top_k': 5, 'vector_weight': 0.6, 'bm25_weight': 0.4}
    )
    results.append(result_hybrid_weighted)
    
    # 4. Hybrid RRF
    result_hybrid_rrf = evaluate_retriever(
        "Hybrid RRF",
        hybrid_retriever,
        dataset,
        hybrid_retriever.search_rrf,
        {'top_k': 5, 'k': 60}
    )
    results.append(result_hybrid_rrf)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ† ë¦¬íŠ¸ë¦¬ë²„ ì„ ì • ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
    print("=" * 80)
    
    import pandas as pd
    df = pd.DataFrame(results)
    df = df.sort_values(by='MRR', ascending=False)
    
    # ì£¼ìš” ì§€í‘œë§Œ ì¶œë ¥
    main_cols = ['name', 'MRR', 'Recall@1', 'Recall@3', 'Recall@5', 'Latency(ms)']
    print("\nğŸ“Š ì „ì²´ ì„±ëŠ¥ ë¹„êµ:")
    print(df[main_cols].to_string(index=False))
    
    # ì§ˆë¬¸ ìœ í˜•ë³„ ê²°ê³¼
    type_cols = [col for col in df.columns if any(t in col for t in ['keyword', 'context', 'abstract'])]
    if type_cols:
        print("\nğŸ“ˆ ì§ˆë¬¸ ìœ í˜•ë³„ ì„±ëŠ¥:")
        type_df = df[['name'] + type_cols]
        print(type_df.to_string(index=False))
    
    # ê²°ê³¼ ì €ì¥
    csv_path = RESULTS_DIR / "retrieval_selection_validation_set.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {csv_path}")
    
    json_path = RESULTS_DIR / "retrieval_selection_validation_set.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ ìƒì„¸ ê²°ê³¼ ì €ì¥: {json_path}")
    
    print("\n" + "=" * 80)
    print("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    main()

