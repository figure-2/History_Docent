"""
í˜•íƒœì†Œ ë¶„ì„ê¸° ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ (Tokenizer Benchmark)
- ëŒ€ìƒ: Kiwi, Okt, Kkma, Hannanum (4ê°œ)
- ëª©ì : í•œêµ­ì‚¬ RAG ì‹œìŠ¤í…œì˜ BM25 ê²€ìƒ‰ì— ê°€ì¥ ì í•©í•œ í† í¬ë‚˜ì´ì € ì„ ì •
- í‰ê°€ ì§€í‘œ: Recall@1, Recall@5, ì¸ë±ì‹± ì†ë„, ê²€ìƒ‰ ì†ë„
"""
import time
import json
import numpy as np
from pathlib import Path
from tqdm import tqdm
from rank_bm25 import BM25Okapi
import chromadb
from chromadb.config import Settings
import random

# í˜•íƒœì†Œ ë¶„ì„ê¸° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    print("âŒ kiwipiepy ì„¤ì¹˜ í•„ìš”: pip install kiwipiepy")
    KIWI_AVAILABLE = False

try:
    from konlpy.tag import Okt
    OKT_AVAILABLE = True
except ImportError:
    print("âŒ konlpy ì„¤ì¹˜ í•„ìš”: pip install konlpy")
    OKT_AVAILABLE = False

try:
    from konlpy.tag import Kkma
    KKMA_AVAILABLE = True
except ImportError:
    print("âŒ konlpy ì„¤ì¹˜ í•„ìš”: pip install konlpy")
    KKMA_AVAILABLE = False

try:
    from konlpy.tag import Hannanum
    HANNANUM_AVAILABLE = True
except ImportError:
    print("âŒ konlpy ì„¤ì¹˜ í•„ìš”: pip install konlpy")
    HANNANUM_AVAILABLE = False

# -----------------------------------------------------------------------------
# ì„¤ì •
# -----------------------------------------------------------------------------
BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
VECTORDB_DIR = BASE_DIR / "04_VectorDB/chroma_db"
COLLECTION_NAME = "korean_history_chunks"
BENCHMARK_DATA = BASE_DIR / "03_Embedding/data/korean_history_benchmark_2000.json"
OUTPUT_REPORT = BASE_DIR / "05_Retrieval_Optimization/tokenizer_benchmark_result.md"

SAMPLE_SIZE = 50  # í‰ê°€ìš© ìƒ˜í”Œ ìˆ˜

# -----------------------------------------------------------------------------
# í† í¬ë‚˜ì´ì € ë˜í¼ í´ë˜ìŠ¤
# -----------------------------------------------------------------------------
class TokenizerWrapper:
    def __init__(self, name):
        self.name = name
        if name == "Kiwi":
            self.processor = Kiwi()
        elif name == "Okt":
            self.processor = Okt()
        elif name == "Kkma":
            self.processor = Kkma()
        elif name == "Hannanum":
            self.processor = Hannanum()
            
    def tokenize(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        if self.name == "Kiwi":
            # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“± ì‹¤ì§ˆ í˜•íƒœì†Œ ì¶”ì¶œ
            tokens = [token.form for token in self.processor.tokenize(text)]
            # ë¹ˆ ë¬¸ìì—´ ì œê±°
            return [t for t in tokens if t.strip()]
        elif self.name == "Okt":
            # OktëŠ” morphs ì‚¬ìš© (ì–´ê°„ ì¶”ì¶œ í¬í•¨)
            tokens = self.processor.morphs(text, stem=True)
            return [t for t in tokens if t.strip()]
        elif self.name == "Kkma":
            # KkmaëŠ” morphs ì‚¬ìš©
            tokens = self.processor.morphs(text)
            return [t for t in tokens if t.strip()]
        elif self.name == "Hannanum":
            # Hannanumì€ morphs ì‚¬ìš©
            tokens = self.processor.morphs(text)
            return [t for t in tokens if t.strip()]
        return []

# -----------------------------------------------------------------------------
# ë²¤ì¹˜ë§ˆí¬ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def run_benchmark(tokenizer_name, documents, doc_ids, samples):
    print(f"\nğŸš€ [{tokenizer_name}] ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    tokenizer = TokenizerWrapper(tokenizer_name)
    
    # 1. ì¸ë±ì‹± ì†ë„ ì¸¡ì •
    print(f"   ğŸ“ í† í°í™” ì§„í–‰ ì¤‘...")
    start_time = time.time()
    tokenized_corpus = []
    for doc in tqdm(documents, desc=f"   [{tokenizer_name}] í† í°í™”", leave=False):
        tokens = tokenizer.tokenize(doc)
        tokenized_corpus.append(tokens)
    
    print(f"   ğŸ§® BM25 ì¸ë±ì‹± ì¤‘...")
    bm25 = BM25Okapi(tokenized_corpus)
    indexing_time = time.time() - start_time
    print(f"   â±ï¸  ì¸ë±ì‹± ì†Œìš” ì‹œê°„: {indexing_time:.2f}ì´ˆ")
    
    # 2. ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
    hits_1 = 0
    hits_5 = 0
    search_times = []
    
    print(f"   ğŸ” ê²€ìƒ‰ í‰ê°€ ì§„í–‰ ì¤‘...")
    for sample in tqdm(samples, desc=f"   [{tokenizer_name}] ê²€ìƒ‰", leave=False):
        query = sample['query']
        gold_id = sample['chunk_id']
        
        start_search = time.time()
        tokenized_query = tokenizer.tokenize(query)
        scores = bm25.get_scores(tokenized_query)
        
        # ìƒìœ„ 5ê°œ ì¶”ì¶œ
        top_n_indices = np.argsort(scores)[::-1][:5]
        top_ids = [doc_ids[i] for i in top_n_indices]
        
        search_times.append(time.time() - start_search)
        
        if gold_id in top_ids[:1]:
            hits_1 += 1
        if gold_id in top_ids:
            hits_5 += 1
            
    avg_search_time = sum(search_times) / len(search_times) if search_times else 0
    recall_1 = hits_1 / len(samples) * 100 if samples else 0
    recall_5 = hits_5 / len(samples) * 100 if samples else 0
    
    print(f"   ğŸ“Š ê²°ê³¼: Recall@1={recall_1:.1f}%, Recall@5={recall_5:.1f}%")
    print(f"   âš¡ í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_search_time*1000:.2f}ms")
    
    return {
        "name": tokenizer_name,
        "indexing_time": indexing_time,
        "recall_1": recall_1,
        "recall_5": recall_5,
        "avg_search_time": avg_search_time * 1000
    }

# -----------------------------------------------------------------------------
# ë©”ì¸ ë¡œì§
# -----------------------------------------------------------------------------
def main():
    print("=" * 60)
    print("í˜•íƒœì†Œ ë¶„ì„ê¸° BM25 ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ (ë¬¸ì„œ & í‰ê°€ ë°ì´í„°)
    print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # ChromaDBì—ì„œ ì „ì²´ ë¬¸ì„œ ë¡œë“œ
    client = chromadb.PersistentClient(
        path=str(VECTORDB_DIR), 
        settings=Settings(anonymized_telemetry=False)
    )
    collection = client.get_collection(name=COLLECTION_NAME)
    all_data = collection.get()
    documents = all_data['documents']
    doc_ids = all_data['ids']
    print(f"   âœ… ë¬¸ì„œ {len(documents)}ê°œ ë¡œë“œ ì™„ë£Œ")
    
    # í‰ê°€ ë°ì´í„° ë¡œë“œ (50ê°œ ìƒ˜í”Œë§)
    with open(BENCHMARK_DATA, 'r', encoding='utf-8') as f:
        all_samples = json.load(f)
    random.seed(42)
    samples = random.sample(all_samples, min(SAMPLE_SIZE, len(all_samples)))
    print(f"   âœ… í‰ê°€ìš© ìƒ˜í”Œ {len(samples)}ê°œ ì¤€ë¹„ ì™„ë£Œ")
    
    # 2. ë¹„êµ ëŒ€ìƒ ì„¤ì • (4ê°œ)
    candidates = []
    if KIWI_AVAILABLE:
        candidates.append("Kiwi")
    if OKT_AVAILABLE:
        candidates.append("Okt")
    if KKMA_AVAILABLE:
        candidates.append("Kkma")
    if HANNANUM_AVAILABLE:
        candidates.append("Hannanum")
    
    if not candidates:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“‹ ë¹„êµ ëŒ€ìƒ ({len(candidates)}ê°œ): {', '.join(candidates)}")
    
    results = []
    
    # 3. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    for name in candidates:
        try:
            res = run_benchmark(name, documents, doc_ids, samples)
            results.append(res)
        except Exception as e:
            print(f"   âŒ {name} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
    # 4. ê²°ê³¼ ë¦¬í¬íŠ¸ ì‘ì„±
    print("\n" + "="*60)
    print("ğŸ† ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
    print("="*60)
    print(f"{'Tokenizer':<10} | {'Recall@1':<10} | {'Recall@5':<10} | {'Indexing(s)':<12} | {'Search(ms)':<10}")
    print("-" * 65)
    
    with open(OUTPUT_REPORT, 'w', encoding='utf-8') as f:
        f.write("# í˜•íƒœì†Œ ë¶„ì„ê¸° BM25 ì„±ëŠ¥ ë¹„êµ\n\n")
        f.write(f"- í‰ê°€ ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"- í‰ê°€ ìƒ˜í”Œ ìˆ˜: {len(samples)}ê°œ\n")
        f.write(f"- ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ\n\n")
        f.write("| Tokenizer | Recall@1 | Recall@5 | Indexing(s) | Search(ms) |\n")
        f.write("|---|---|---|---|---|\n")
        
        for r in results:
            line = f"{r['name']:<10} | {r['recall_1']:.1f}%     | {r['recall_5']:.1f}%     | {r['indexing_time']:.2f}s       | {r['avg_search_time']:.2f}ms"
            print(line)
            f.write(f"| **{r['name']}** | {r['recall_1']:.1f}% | {r['recall_5']:.1f}% | {r['indexing_time']:.2f}s | {r['avg_search_time']:.2f}ms |\n")
        
        # ìŠ¹ì ì„ ì •
        if len(results) >= 2:
            winner = max(results, key=lambda x: x['recall_1'])
            f.write(f"\n## ğŸ† ìµœì¢… ì„ ì •: **{winner['name']}**\n\n")
            f.write(f"- Recall@1 ê¸°ì¤€ ìµœê³  ì„±ëŠ¥: {winner['recall_1']:.1f}%\n")
            f.write(f"- Recall@5: {winner['recall_5']:.1f}%\n")
            f.write(f"- ì¸ë±ì‹± ì‹œê°„: {winner['indexing_time']:.2f}ì´ˆ\n")
            f.write(f"- í‰ê·  ê²€ìƒ‰ ì‹œê°„: {winner['avg_search_time']:.2f}ms\n")
            
            print(f"\nğŸ† ìµœì¢… ì„ ì •: {winner['name']} (Recall@1: {winner['recall_1']:.1f}%)")
            
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {OUTPUT_REPORT}")

if __name__ == "__main__":
    main()

