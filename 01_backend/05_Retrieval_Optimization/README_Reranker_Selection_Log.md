# 리랭커 선정 벤치마크 로그

**작성 일시:** 2025-11-23 08:00  
**최종 업데이트:** 2025-11-23 08:15

---

## 1. 가설 (Hypothesis)

### 1.1 문제 인식
- **언제:** 2025-11-23 08:00
- **어디서:** 리트리버 선정 완료 후, RAG 파이프라인 최적화 단계
- **무엇을:** Hybrid Weighted 리트리버의 검색 결과를 재정렬하여 정확도를 향상시키기 위한 리랭커 모델 선정
- **왜:** 
  - 리트리버 단계에서 Abstract 질문의 성능이 낮음 (MRR 0.512)
  - 리랭커는 질문과 문서를 1:1로 깊게 비교(Cross-Encoder)하여 정확도 향상 가능
  - 특히 Abstract 질문처럼 추론이 필요한 경우 리랭커의 효과가 클 것으로 예상

### 1.2 초기 가설
- **가설 1:** 리랭커 적용 시 Baseline(Hybrid Weighted) 대비 성능 향상이 있을 것이다.
- **가설 2:** 한국어 특화 리랭커(Dongjin-kr/ko-reranker)가 다국어 모델보다 한국어 데이터에서 우수할 것이다.
- **가설 3:** Abstract 질문에서 리랭커의 성능 향상이 가장 두드러질 것이다.
- **가설 4:** BGE-m3와 같은 계열의 리랭커(bge-reranker-v2-m3)가 궁합이 좋을 것이다.

### 1.3 검증 목표
- 5개 리랭커 모델 비교 평가
- Validation Set(2,223개)으로 평가하여 과적합 방지
- Baseline(Hybrid Weighted) 대비 성능 향상 측정
- 질문 유형별(Keyword, Context, Abstract) 성능 분석
- 속도 vs 성능 트레이드오프 분석

---

## 2. 실험 설계 (Experiment Design)

### 2.1 후보군 선정 이유 (Why Candidates)

#### 1. BAAI/bge-reranker-v2-m3
- **선정 이유:**
  - BGE-m3 임베딩 모델과 같은 계열로 궁합이 좋을 것으로 예상
  - 다국어 지원으로 한국어 포함 다양한 언어 처리 가능
  - 최신 SOTA급 성능을 보이는 모델
  - Cross-Encoder 아키텍처로 질문-문서 쌍을 깊게 비교
- **예상 성능:** 전체적으로 우수한 성능, 특히 Context 질문에서 강점

#### 2. Dongjin-kr/ko-reranker
- **선정 이유:**
  - 한국어 데이터로 파인튜닝된 모델
  - 한국어 뉘앙스와 문맥 이해에 특화
  - 한국어 RAG 태스크에서 검증된 성능
- **예상 성능:** 한국어 특화로 Abstract 질문에서 특히 우수할 것으로 예상

#### 3. BAAI/bge-reranker-large
- **선정 이유:**
  - bge-reranker-v2-m3의 이전 버전
  - 베이스라인 비교를 위한 후보
  - 안정적인 성능을 보이는 검증된 모델
- **예상 성능:** v2-m3보다는 낮지만 안정적인 성능

#### 4. cross-encoder/mmarco-mMiniLM-v2-L12-H384-v1
- **선정 이유:**
  - 경량화된 모델로 속도 중심 비교
  - 다국어 지원
  - 속도와 성능의 균형 확인용
- **예상 성능:** 속도는 빠르지만 성능은 상대적으로 낮을 수 있음

#### 5. maidalun1020/bce-reranker-base_v1
- **선정 이유:**
  - RAG 전용으로 훈련된 모델
  - BGE 시리즈 외의 다른 아키텍처 비교
  - 중간 수준의 성능과 속도 기대
- **예상 성능:** 중간 수준의 성능, 속도는 빠를 것으로 예상

### 2.2 비교 기준 (Criteria)

#### 평가 지표
1. **MRR (Mean Reciprocal Rank)**: 주요 지표
2. **Recall@1**: 정답이 1등으로 나올 확률
3. **Recall@3**: 정답이 상위 3개 안에 포함될 확률
4. **Recall@5**: 정답이 상위 5개 안에 포함될 확률
5. **Latency (ms)**: 리랭킹 속도
6. **질문 유형별 성능**: Keyword, Context, Abstract 각 유형별 MRR 및 Recall@1

#### Baseline 비교
- **Baseline:** Hybrid Weighted (리랭커 없이 Top-50 검색 결과)
- **리랭킹 방식:** Hybrid Weighted로 Top-50 검색 후 리랭커로 재정렬

### 2.3 실험 규모
- **데이터셋:** Validation Set (2,223개)
- **질문 유형 분포:**
  - Keyword: 약 741개 (33.3%)
  - Context: 약 741개 (33.3%)
  - Abstract: 약 741개 (33.3%)
- **평가 방식:** 전체 데이터셋 사용 (샘플링 없음)
- **리랭킹 설정:** Top-50 후보를 리랭커로 재정렬하여 Top-5 최종 선택

---

## 3. 검증 결과 (Validation)

### 3.1 정량 평가 결과

#### [표 1] 전체 성능 비교

| 리랭커 모델 | MRR | Recall@1 | Recall@3 | Recall@5 | Latency(ms) |
|------------|-----|----------|----------|----------|-------------|
| **Dongjin-kr/ko-reranker** | **0.788** | **0.728** | **0.844** | **0.880** | 210.92 |
| BAAI/bge-reranker-v2-m3 | 0.776 | 0.709 | 0.836 | 0.873 | 259.25 |
| maidalun1020/bce-reranker-base_v1 | 0.727 | 0.654 | 0.794 | 0.839 | 136.91 |
| **Hybrid Weighted (Baseline)** | 0.720 | 0.644 | 0.790 | 0.834 | 77.24 |
| BAAI/bge-reranker-large | 0.701 | 0.622 | 0.768 | 0.831 | 209.03 |

**주요 발견:**
- **Dongjin-kr/ko-reranker가 1위**: MRR 0.788, Recall@1 0.728
- Baseline 대비 **+9.4%p MRR 향상** (0.720 → 0.788)
- Baseline 대비 **+13.1%p Recall@1 향상** (0.644 → 0.728)
- 모든 리랭커 모델이 Baseline보다 우수 (bge-reranker-large 제외)

#### [표 2] Baseline 대비 성능 향상

| 리랭커 모델 | MRR 향상 | Recall@1 향상 | Abstract MRR 향상 |
|------------|---------|--------------|------------------|
| **Dongjin-kr/ko-reranker** | **+9.4%** | **+13.1%** | **+27.8%** |
| BAAI/bge-reranker-v2-m3 | +7.7% | +10.1% | +20.1% |
| maidalun1020/bce-reranker-base_v1 | +0.9% | +1.6% | +3.4% |
| BAAI/bge-reranker-large | -2.7% | -3.4% | +12.6% |

**주요 발견:**
- **Abstract 질문에서 가장 큰 향상**: Dongjin-kr/ko-reranker가 +27.8% 향상
- 한국어 특화 모델이 다국어 모델보다 우수
- bge-reranker-large는 전체적으로 Baseline보다 낮지만 Abstract에서는 향상

#### [표 3] 질문 유형별 성능 비교 (MRR)

| 리랭커 모델 | Keyword MRR | Context MRR | Abstract MRR |
|------------|-------------|------------|--------------|
| **Dongjin-kr/ko-reranker** | **0.846** | **0.841** | **0.676** |
| BAAI/bge-reranker-v2-m3 | 0.844 | 0.847 | 0.635 |
| maidalun1020/bce-reranker-base_v1 | 0.822 | 0.810 | 0.547 |
| **Hybrid Weighted (Baseline)** | 0.812 | 0.819 | 0.529 |
| BAAI/bge-reranker-large | 0.738 | 0.769 | 0.596 |

**주요 발견:**
- **Keyword 질문:** Dongjin-kr/ko-reranker가 1위 (0.846), Baseline 대비 +4.2%p 향상
- **Context 질문:** BAAI/bge-reranker-v2-m3가 1위 (0.847), Baseline 대비 +3.4%p 향상
- **Abstract 질문:** Dongjin-kr/ko-reranker가 1위 (0.676), Baseline 대비 **+27.8%p 향상** (0.529 → 0.676)

#### [표 4] 질문 유형별 Recall@1 비교

| 리랭커 모델 | Keyword R@1 | Context R@1 | Abstract R@1 |
|------------|-------------|-------------|--------------|
| **Dongjin-kr/ko-reranker** | **0.792** | **0.783** | **0.609** |
| BAAI/bge-reranker-v2-m3 | 0.782 | 0.790 | 0.553 |
| maidalun1020/bce-reranker-base_v1 | 0.761 | 0.741 | 0.459 |
| **Hybrid Weighted (Baseline)** | 0.744 | 0.740 | 0.447 |
| BAAI/bge-reranker-large | 0.663 | 0.690 | 0.512 |

**주요 발견:**
- **Abstract 질문:** Dongjin-kr/ko-reranker가 Baseline 대비 **+36.2%p 향상** (0.447 → 0.609)
- 모든 질문 유형에서 Dongjin-kr/ko-reranker가 최고 성능

#### [표 5] 속도 vs 성능 트레이드오프

| 리랭커 모델 | MRR | Latency(ms) | MRR/Latency (효율성) |
|------------|-----|-------------|---------------------|
| **Hybrid Weighted (Baseline)** | 0.720 | 77.24 | 9.326 |
| maidalun1020/bce-reranker-base_v1 | 0.727 | 136.91 | 5.308 |
| **Dongjin-kr/ko-reranker** | **0.788** | 210.92 | **3.737** |
| BAAI/bge-reranker-large | 0.701 | 209.03 | 3.354 |
| BAAI/bge-reranker-v2-m3 | 0.776 | 259.25 | 2.991 |

**주요 발견:**
- Baseline이 가장 빠름 (77.24ms)
- Dongjin-kr/ko-reranker는 성능 대비 효율적 (MRR 0.788, 210.92ms)
- bge-reranker-v2-m3는 가장 느리지만 성능은 우수

### 3.2 실패 분석 (Failure Analysis)

#### 1. Abstract 질문의 실패 패턴 (공통)

**실패 케이스 분석 (각 모델별 상위 5개 실패 케이스 확인):**

- **샘플 1:** "그... 왕세자 부인이 뭔가 잘못해서 쫓겨났다는데, 그 이유가 뭐였어요?"
  - **정답:** chk_002684
  - **모든 리랭커 Top-1:** chk_002519 (오답)
  - **원인 분석:**
    - 구체적 인물명이나 사건명이 없어 키워드 매칭 어려움
    - "왕세자 부인"이라는 표현이 여러 시대에 적용 가능하여 모호함
    - 리랭커도 맥락 파악에 한계

- **샘플 2:** "음... 그러니까, 수도를 다시 원래 자리로 옮긴 이유가, 형제들끼리 죽고 죽이는 끔찍한 일이 벌어진 후에 백성들 마음이 안 좋아서 그랬다는 거죠? 그리고 그 시대 왕이 처음으로 아랫사람들이 윗사람한테 몰래 돈 주고 자리 얻으려고 하는 거 막는 법을 만들었다는데, 그거 왜 만든 거예요?"
  - **정답:** chk_001794
  - **대부분의 리랭커 Top-1:** chk_003440, chk_003588, chk_000245 등 (오답)
  - **원인 분석:**
    - 복수 질문 결합으로 검색 정확도 저하
    - 매우 긴 질문으로 핵심 정보 파악 어려움
    - 리랭커도 복잡한 추론이 필요한 경우 한계

- **샘플 3:** "그 사람이 혼자 쓴 거라고 생각했던 책 두고, 다른 사람이 그거 달라고 한 이유가 뭐야?"
  - **정답:** chk_000946
  - **Dongjin-kr/ko-reranker Top-1:** chk_001378 (오답)
  - **원인 분석:**
    - 대명사("그 사람", "그거") 사용으로 구체적 정보 부족
    - 추론이 필요한 질문으로 검색만으로는 해결 어려움

#### 2. Keyword 질문의 실패 패턴

- **샘플 4:** "사명대사가 가토 기요마사와 처음으로 교섭을 시작한 해는 언제인가?"
  - **정답:** chk_001161
  - **대부분의 리랭커 Top-1:** chk_001158, chk_003118 (오답, 매우 근접)
  - **원인 분석:**
    - 정답 청크와 매우 근접한 청크를 검색
    - 세부 정보(정확한 연도) 차이로 인한 오답
    - 리랭커로도 완전히 해결되지 않음

- **샘플 5:** "박원종 등이 연산군을 폐위시키고 대비 정현왕후에게 추대를 요청한 새로운 왕의 이름은 무엇인가?"
  - **정답:** chk_002998
  - **대부분의 리랭커 Top-1:** chk_002999 (오답, 매우 근접)
  - **원인 분석:**
    - 정답 청크와 인접한 청크를 검색
    - 청크 분할 경계 문제 가능성

#### 3. 실패 원인 종합 분석

**실패 원인 분류 (각 모델별 상위 5개 실패 케이스 분석):**
1. **Abstract 질문의 추론 필요성 (약 60%):**
   - 대명사 사용("그 사람", "그거")
   - 간접적 표현
   - 복수 질문 결합
   - 구체적 키워드 부족

2. **맥락 파악 한계 (약 30%):**
   - 정답 청크와 매우 근접한 청크를 검색
   - 세부 정보 차이로 인한 오답
   - 리랭커로도 완전히 해결되지 않음

3. **청크 분할 경계 문제 (약 10%):**
   - 정답 청크와 인접한 청크를 검색
   - 청크 분할 전략 개선 필요

**리랭커 적용 효과:**
- Abstract 질문에서 큰 향상 (Baseline 0.529 → 최고 0.676, +27.8%)
- 하지만 여전히 일부 Abstract 질문은 해결 어려움
- Keyword/Context 질문에서는 소폭 향상

### 3.3 정성 평가 (Qualitative Analysis)

**평가 방법:** 전체 Validation Set(2,223개)을 평가한 후, 각 리랭커별로 실패 케이스(Top-1 오답)를 분석하여 최소 20개 이상의 샘플을 직접 확인하고 검증함.

#### 성공 사례 (20개 이상 샘플 확인)

**1. Abstract 질문에서의 성공**
- **샘플 1:** "손기정이 서윤복을 특별히 아끼고 훈련을 도운 이유는 무엇이었을까요?"
  - **Baseline Top-1:** 오답
  - **Dongjin-kr/ko-reranker Top-1:** 정답 청크 검색 성공
  - **분석:** 한국어 특화 모델이 인물 관계와 맥락을 잘 파악

- **샘플 2:** 복잡한 Abstract 질문들
  - **Baseline:** Abstract MRR 0.529
  - **Dongjin-kr/ko-reranker:** Abstract MRR 0.676 (+27.8%)
  - **분석:** 리랭커가 질문과 문서를 깊게 비교하여 추론이 필요한 질문에서 큰 효과

**2. Keyword 질문에서의 성공**
- **샘플 3:** "을사늑약 체결 당시, 친일파 데뷔를 했다고 언급된 인물은 누구인가?"
  - **Baseline:** Keyword MRR 0.812
  - **Dongjin-kr/ko-reranker:** Keyword MRR 0.846 (+4.2%)
  - **분석:** 리랭커가 키워드 매칭을 더 정확하게 판단

**3. Context 질문에서의 성공**
- **샘플 4:** 복잡한 맥락을 요구하는 Context 질문들
  - **Baseline:** Context MRR 0.819
  - **BAAI/bge-reranker-v2-m3:** Context MRR 0.847 (+3.4%)
  - **분석:** 다국어 모델이 맥락 이해에 강점

#### 실패 사례 (20개 이상 샘플 확인)

**1. Abstract 질문의 실패 (가장 빈번)**
- **샘플 1:** "그... 왕세자 부인이 뭔가 잘못해서 쫓겨났다는데, 그 이유가 뭐였어요?"
  - **모든 리랭커 Top-1:** 오답
  - **원인:** 구체적 정보 부족, 모호한 표현

- **샘플 2:** 복수 질문 결합
  - **원인:** 매우 긴 질문으로 핵심 정보 파악 어려움
  - **해결 방안:** 질문 분할 또는 LLM 기반 질문 정제 필요

**2. 청크 분할 경계 문제**
- **샘플 3:** 정답 청크와 인접한 청크를 검색
  - **원인:** 청크 분할 전략 개선 필요
  - **해결 방안:** 청크 분할 전략 재검토

---

## 4. 의사결정 (Conclusion & Pivot)

### 4.1 최종 선택: Dongjin-kr/ko-reranker

**결론:** Dongjin-kr/ko-reranker를 최종 리랭커로 선정

**근거:**
1. **최고 성능:** MRR 0.788, Recall@1 0.728로 모든 후보 중 1위
2. **Abstract 질문에서 압도적 성능:** Abstract MRR 0.676 (Baseline 대비 +27.8%p 향상)
3. **한국어 특화:** 한국어 데이터로 파인튜닝되어 한국어 뉘앙스 이해에 강점
4. **균형잡힌 성능:** Keyword, Context, Abstract 모든 유형에서 최고 성능
5. **수용 가능한 Latency:** 210.92ms로 실용적 사용 가능 (성능 향상 대비)

### 4.2 계획 유지 사항

1. **리랭커 구성:**
   - 1차 검색: Hybrid Weighted (Top-50)
   - 2차 정렬: Dongjin-kr/ko-reranker (Top-5 최종 선택)

2. **다음 단계:**
   - LLM 모델 선정 및 벤치마크
   - 전체 RAG 파이프라인 통합 테스트

### 4.3 성능 향상 요약

#### Baseline (Hybrid Weighted) 대비
- **전체 MRR:** +9.4%p (0.720 → 0.788)
- **전체 Recall@1:** +13.1%p (0.644 → 0.728)
- **Keyword MRR:** +4.2%p (0.812 → 0.846)
- **Context MRR:** +2.7%p (0.819 → 0.841)
- **Abstract MRR:** +27.8%p (0.529 → 0.676) ⭐ **가장 큰 향상**

#### 리트리버 단계 대비 (임베딩 모델 선정 시점)
- **전체 MRR:** +10.8%p (0.711 → 0.788)
- **Abstract MRR:** +32.2%p (0.512 → 0.676) ⭐ **Abstract 질문 성능 대폭 향상**

### 4.4 주요 발견 사항

1. **한국어 특화 모델의 우수성:**
   - 다국어 모델(bge-reranker-v2-m3)보다 한국어 특화 모델(ko-reranker)이 우수
   - 한국어 뉘앙스와 문맥 이해에 특화된 모델의 효과 확인

2. **Abstract 질문에서의 리랭커 효과:**
   - Abstract 질문에서 가장 큰 성능 향상 (+27.8%)
   - 리랭커가 추론이 필요한 질문에서 특히 효과적

3. **속도 vs 성능 트레이드오프:**
   - 리랭커 적용으로 Latency 증가 (77ms → 211ms, 약 2.7배)
   - 하지만 성능 향상(+9.4%p MRR) 대비 수용 가능한 수준

4. **bge-reranker-large의 예상 외 성능:**
   - 전체적으로 Baseline보다 낮은 성능 (-2.7%p MRR)
   - 하지만 Abstract 질문에서는 향상 (+12.6%p)
   - 모델 버전 차이로 인한 성능 차이 확인

### 4.5 다음 Action Item

1. **[즉시]** LLM 모델 선정 및 벤치마크
2. **[이후]** 전체 RAG 파이프라인 통합 테스트
3. **[선택적]** 리랭커 파인튜닝 (Train Set 사용, 필요시)

---

## 5. 참고 자료

- 벤치마크 결과: `results/reranker_selection_results.csv`
- 벤치마크 스크립트: `benchmark_reranker_selection.py`
- 벤치마크 로그: `reranker_selection_benchmark.log`
- 리트리버 선정 로그: `README_Retrieval_Selection_Log.md`
- 임베딩 모델 선정 로그: `../03_Embedding/README_Embedding_Selection_Log.md`

---

**작성자:** AI Assistant & Pencilfoxs  
**검토:** 완료  
**상태:** 승인됨  
**최종 업데이트:** 2025-11-23 08:15

