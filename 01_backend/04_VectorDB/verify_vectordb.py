"""
ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
- ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- ë©”íƒ€ë°ì´í„° í•„í„°ë§ í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ì¸¡ì •
"""
import chromadb
from chromadb.config import Settings
from pathlib import Path
import torch
from sentence_transformers import SentenceTransformer
import time

# -----------------------------------------------------------------------------
# ì„¤ì •
# -----------------------------------------------------------------------------
BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
VECTORDB_DIR = BASE_DIR / "04_VectorDB/chroma_db"
COLLECTION_NAME = "korean_history_chunks"
MODEL_NAME = "BAAI/bge-m3"

# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ëª©ë¡
TEST_QUERIES = [
    "ì„¸ì¢…ëŒ€ì™•ì´ ë§Œë“  í•œê¸€",
    "ì„ì§„ì™œë€",
    "ì¡°ì„ ì™•ì¡°ì‹¤ë¡",
    "ì •ì•½ìš©ì˜ ì‹¤í•™",
    "ê³ ë ¤ì‹œëŒ€ ë¬´ì—­"
]

# -----------------------------------------------------------------------------
# ë©”ì¸ ë¡œì§
# -----------------------------------------------------------------------------
def main():
    print("=" * 60)
    print("ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. ChromaDB í´ë¼ì´ì–¸íŠ¸ ë° Collection ë¡œë“œ
    print(f"\nğŸ“‚ ChromaDB ë¡œë“œ ì¤‘...")
    client = chromadb.PersistentClient(
        path=str(VECTORDB_DIR),
        settings=Settings(anonymized_telemetry=False)
    )
    
    collection = client.get_collection(name=COLLECTION_NAME)
    count = collection.count()
    print(f"   âœ… Collection ë¡œë“œ ì™„ë£Œ: {count}ê°œ ë¬¸ì„œ")
    
    # 2. BGE-m3 ëª¨ë¸ ë¡œë“œ (CUDA ì‚¬ìš©)
    print(f"\nğŸ¤– BGE-m3 ëª¨ë¸ ë¡œë“œ ì¤‘ (CUDA ì‚¬ìš©)...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = SentenceTransformer(MODEL_NAME, device=device)
    print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {device})")
    
    # 3. ê¸°ë³¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print(f"\n" + "=" * 60)
    print("1. ê¸°ë³¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    for i, query in enumerate(TEST_QUERIES, 1):
        print(f"\n[{i}/{len(TEST_QUERIES)}] ì¿¼ë¦¬: '{query}'")
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        start_time = time.time()
        query_embedding = model.encode(
            query,
            normalize_embeddings=True,
            show_progress_bar=False
        ).tolist()
        embed_time = time.time() - start_time
        
        # ê²€ìƒ‰ ì‹¤í–‰
        start_time = time.time()
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=5
        )
        search_time = time.time() - start_time
        
        print(f"   â±ï¸  ì„ë² ë”© ìƒì„±: {embed_time*1000:.2f}ms, ê²€ìƒ‰: {search_time*1000:.2f}ms")
        print(f"   ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(results['ids'][0])}ê°œ ë¬¸ì„œ")
        
        if results['ids'][0]:
            top_id = results['ids'][0][0]
            top_doc = results['documents'][0][0]
            top_dist = results['distances'][0][0]
            top_meta = results['metadatas'][0][0]
            
            print(f"   ğŸ¥‡ 1ìœ„:")
            print(f"      - ID: {top_id}")
            print(f"      - ê±°ë¦¬: {top_dist:.4f}")
            print(f"      - ì†ŒìŠ¤: {top_meta.get('source', 'N/A')}")
            print(f"      - í˜ì´ì§€: {top_meta.get('page', 'N/A')}")
            print(f"      - ë¬¸ì„œ ì¼ë¶€: {top_doc[:150]}...")
    
    # 4. ë©”íƒ€ë°ì´í„° í•„í„°ë§ í…ŒìŠ¤íŠ¸
    print(f"\n" + "=" * 60)
    print("2. ë©”íƒ€ë°ì´í„° í•„í„°ë§ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    test_query = "ì„¸ì¢…ëŒ€ì™•"
    query_embedding = model.encode(
        test_query,
        normalize_embeddings=True,
        show_progress_bar=False
    ).tolist()
    
    # íŠ¹ì • ì†ŒìŠ¤ë¡œ í•„í„°ë§
    print(f"\nì¿¼ë¦¬: '{test_query}'")
    print("í•„í„°: source == 'ë²Œê±°ë²—ì€í•œêµ­ì‚¬-ì¡°ì„ í¸'")
    
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=3,
        where={"source": "ë²Œê±°ë²—ì€í•œêµ­ì‚¬-ì¡°ì„ í¸"}
    )
    
    print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(results['ids'][0])}ê°œ ë¬¸ì„œ")
    if results['ids'][0]:
        for i, (doc_id, doc, dist, meta) in enumerate(zip(
            results['ids'][0],
            results['documents'][0],
            results['distances'][0],
            results['metadatas'][0]
        ), 1):
            print(f"   {i}. {doc_id} (ê±°ë¦¬: {dist:.4f}, í˜ì´ì§€: {meta.get('page', 'N/A')})")
    
    # 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    print(f"\n" + "=" * 60)
    print("3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 60)
    
    num_tests = 10
    embed_times = []
    search_times = []
    
    print(f"\n{num_tests}íšŒ ë°˜ë³µ í…ŒìŠ¤íŠ¸ ì¤‘...")
    for i in range(num_tests):
        query = f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i}"
        
        # ì„ë² ë”© ìƒì„± ì‹œê°„ ì¸¡ì •
        start = time.time()
        query_embedding = model.encode(
            query,
            normalize_embeddings=True,
            show_progress_bar=False
        ).tolist()
        embed_times.append(time.time() - start)
        
        # ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
        start = time.time()
        collection.query(
            query_embeddings=[query_embedding],
            n_results=10
        )
        search_times.append(time.time() - start)
    
    avg_embed = sum(embed_times) / len(embed_times) * 1000
    avg_search = sum(search_times) / len(search_times) * 1000
    
    print(f"\n   ğŸ“Š í‰ê·  ì„±ëŠ¥:")
    print(f"      - ì„ë² ë”© ìƒì„±: {avg_embed:.2f}ms")
    print(f"      - ë²¡í„° ê²€ìƒ‰: {avg_search:.2f}ms")
    print(f"      - ì´ ì†Œìš” ì‹œê°„: {avg_embed + avg_search:.2f}ms")
    
    # 6. Collection í†µê³„
    print(f"\n" + "=" * 60)
    print("4. Collection í†µê³„")
    print("=" * 60)
    
    # ë©”íƒ€ë°ì´í„°ë³„ ë¬¸ì„œ ìˆ˜ ì§‘ê³„
    all_data = collection.get()
    sources = {}
    types = {}
    
    for meta in all_data['metadatas']:
        source = meta.get('source', 'unknown')
        doc_type = meta.get('type', 'unknown')
        sources[source] = sources.get(source, 0) + 1
        types[doc_type] = types.get(doc_type, 0) + 1
    
    print(f"\n   ğŸ“š ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜:")
    for source, count in sorted(sources.items(), key=lambda x: -x[1]):
        print(f"      - {source}: {count}ê°œ")
    
    print(f"\n   ğŸ“„ íƒ€ì…ë³„ ë¬¸ì„œ ìˆ˜:")
    for doc_type, count in sorted(types.items(), key=lambda x: -x[1]):
        print(f"      - {doc_type}: {count}ê°œ")
    
    print(f"\n" + "=" * 60)
    print("âœ… ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)

if __name__ == "__main__":
    main()

