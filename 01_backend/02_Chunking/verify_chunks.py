import json
import statistics
from pathlib import Path
from collections import Counter, defaultdict

# -----------------------------------------------------------------------------
# ì„¤ì • (Configuration)
# -----------------------------------------------------------------------------
BASE_DIR = Path("/home/pencilfoxs/00_new/History_Docent")
CHUNK_FILE = BASE_DIR / "02_Chunking/output/all_chunks.json"

# ì„ê³„ê°’ ì„¤ì • (Thresholds for Warnings)
MIN_LENGTH_WARNING = 20   # 20ì ë¯¸ë§Œì€ ë…¸ì´ì¦ˆ(í˜ì´ì§€ ë²ˆí˜¸ ë“±)ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
MAX_LENGTH_WARNING = 1500 # 1500ì ì´ˆê³¼ëŠ” ì„ë² ë”© ì‹œ ì˜ë¦´(Truncation) ìœ„í—˜ ìˆìŒ

class ChunkValidator:
    def __init__(self, file_path):
        self.file_path = file_path
        self.data = []
        self.stats = {
            "total_count": 0,
            "by_source": Counter(),
            "by_type": Counter(),
            "empty_text": 0,
            "missing_meta": 0,
            "lengths": []
        }
        self.anomalies = {
            "too_short": [],
            "too_long": []
        }

    def load_data(self):
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(self.data)}ê°œì˜ ì²­í¬")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            exit(1)

    def run_checks(self):
        print("\nğŸ” ê²€ì¦ ì‹œì‘...\n")
        
        for idx, chunk in enumerate(self.data):
            # 1. í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
            if "text" not in chunk or "metadata" not in chunk:
                print(f"âš ï¸ [Index {idx}] í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
                continue
            
            text = chunk.get("text", "")
            meta = chunk.get("metadata", {})
            
            # 2. í…ìŠ¤íŠ¸ ë¬´ê²°ì„±
            if not text.strip():
                self.stats["empty_text"] += 1
            
            # 3. ë©”íƒ€ë°ì´í„° ê²€ì‚¬
            source = meta.get("source", "UNKNOWN")
            c_type = meta.get("type", "unknown")
            
            if source == "UNKNOWN":
                self.stats["missing_meta"] += 1

            # 4. í†µê³„ ìˆ˜ì§‘
            self.stats["by_source"][source] += 1
            self.stats["by_type"][c_type] += 1
            self.stats["lengths"].append(len(text))
            
            # 5. ì´ìƒì¹˜ íƒì§€ (Anomalies)
            if len(text) < MIN_LENGTH_WARNING and c_type == 'text':
                self.anomalies["too_short"].append((chunk["chunk_id"], len(text), text[:20]))
            
            if len(text) > MAX_LENGTH_WARNING:
                self.anomalies["too_long"].append((chunk["chunk_id"], len(text), source))

    def print_report(self):
        print("="*60)
        print("ğŸ“Š ì²­í‚¹ ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ (Chunking Quality Report)")
        print("="*60)
        
        # 1. ê¸°ë³¸ í†µê³„
        print(f"1. ì´ ì²­í¬ ìˆ˜: {len(self.data):,}")
        print(f"2. ë¹ˆ í…ìŠ¤íŠ¸(Empty) ìˆ˜: {self.stats['empty_text']}ê°œ (0ì´ì–´ì•¼ í•¨)")
        print(f"3. ì†ŒìŠ¤(Source) ëˆ„ë½ ìˆ˜: {self.stats['missing_meta']}ê°œ")
        
        # 2. íŒŒì¼ë³„ ë¶„í¬ (Completeness Check)
        print("\n4. íŒŒì¼ë³„ ì²­í¬ ë¶„í¬ (Source Distribution):")
        print(f"   - ì´ {len(self.stats['by_source'])}ê°œì˜ ì†ŒìŠ¤ íŒŒì¼ ê°ì§€ë¨")
        for source, count in self.stats['by_source'].most_common():
            print(f"   - {source:<20}: {count:4,} chunks")
        
        # 3. íƒ€ì… ë¶„í¬
        print("\n5. ë°ì´í„° íƒ€ì… ë¶„í¬:")
        for c_type, count in self.stats['by_type'].items():
            ratio = (count / len(self.data)) * 100
            print(f"   - {c_type:<10}: {count:4,} ({ratio:.1f}%)")

        # 4. ê¸¸ì´ ë¶„ì„ (Length Analysis)
        lengths = self.stats["lengths"]
        if lengths:
            avg_len = statistics.mean(lengths)
            max_len = max(lengths)
            min_len = min(lengths)
            print(f"\n6. í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„ (ê¸€ì ìˆ˜ ê¸°ì¤€):")
            print(f"   - í‰ê· : {avg_len:.1f} ì")
            print(f"   - ìµœëŒ€: {max_len:,} ì")
            print(f"   - ìµœì†Œ: {min_len:,} ì")
            
            # ê¸¸ì´ ë¶„í¬ ì‹œê°í™” (Simple ASCII Histogram)
            print("\n   [ê¸¸ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨]")
            buckets = [0, 200, 500, 1000, 1500, 99999]
            bucket_counts = defaultdict(int)
            for l in lengths:
                for i in range(len(buckets)-1):
                    if buckets[i] <= l < buckets[i+1]:
                        bucket_counts[f"{buckets[i]}~{buckets[i+1]}"] += 1
                        break
            
            for k, v in bucket_counts.items():
                bar = "â–ˆ" * int((v / len(lengths)) * 50)
                print(f"   {k:<10}: {v:4,} |{bar}")

        # 5. ê²½ê³  ë° ì´ìƒì¹˜ (Warnings)
        print("\n7. ê²½ê³  ë° ì´ìƒì¹˜ (Anomalies):")
        
        short_cnt = len(self.anomalies['too_short'])
        if short_cnt > 0:
            print(f"   âš ï¸  ë„ˆë¬´ ì§§ì€ ì²­í¬ (<{MIN_LENGTH_WARNING}ì): {short_cnt}ê°œ (ë…¸ì´ì¦ˆ ê°€ëŠ¥ì„±)")
            print(f"      ì˜ˆì‹œ: {self.anomalies['too_short'][:3]} ...")
        else:
            print(f"   âœ… ë„ˆë¬´ ì§§ì€ ì²­í¬ ì—†ìŒ")
            
        long_cnt = len(self.anomalies['too_long'])
        if long_cnt > 0:
            print(f"   âš ï¸  ë„ˆë¬´ ê¸´ ì²­í¬ (>{MAX_LENGTH_WARNING}ì): {long_cnt}ê°œ (ê²€ìƒ‰ ì •í™•ë„ ì €í•˜ ìœ„í—˜)")
            print(f"      ì˜ˆì‹œ: {self.anomalies['too_long'][:3]} ...")
        else:
            print(f"   âœ… ë„ˆë¬´ ê¸´ ì²­í¬ ì—†ìŒ")

        print("="*60)
        
        # ìµœì¢… íŒì •
        if self.stats['empty_text'] == 0 and self.stats['missing_meta'] == 0:
            print("ğŸ‰ [PASS] ë°ì´í„° êµ¬ì¡° ë¬´ê²°ì„± ê²€ì¦ í†µê³¼!")
        else:
            print("ğŸ”¥ [FAIL] ë°ì´í„°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ìœ„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    validator = ChunkValidator(CHUNK_FILE)
    validator.load_data()
    validator.run_checks()
    validator.print_report()
